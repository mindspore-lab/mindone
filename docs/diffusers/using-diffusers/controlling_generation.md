<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Controlled generation

Controlling outputs generated by diffusion models has been long pursued by the community and is now an active research topic. In many popular diffusion models, subtle changes in inputs, both images and text prompts, can drastically change outputs. In an ideal world we want to be able to control how semantics are preserved and changed.

Most examples of preserving semantics reduce to being able to accurately map a change in input to a change in output. I.e. adding an adjective to a subject in a prompt preserves the entire image, only modifying the changed subject. Or, image variation of a particular subject preserves the subject's pose.

Additionally, there are qualities of generated images that we would like to influence beyond semantic preservation. I.e. in general, we would like our outputs to be of good quality, adhere to a particular style, or be realistic.

We will document some of the techniques `diffusers` supports to control generation of diffusion models. Much is cutting edge research and can be quite nuanced.

We provide a high level explanation of how the generation can be controlled as well as a snippet of the technicals. For more in depth explanations on the technicals, the original papers which are linked from the pipelines are always the best resources.

Depending on the use case, one should choose a technique accordingly. In many cases, these techniques can be combined.

Unless otherwise mentioned, these are techniques that work with existing models and don't require their own weights.

1. [InstructPix2Pix](#instructpix2pix)
2. [Depth2Image](#depth2image)
3. [DreamBooth](#dreambooth)
4. [Textual Inversion](#textual-inversion)
5. [ControlNet](#controlnet)
6. [DiffEdit](#diffedit)
7. [T2I-Adapter](#t2i-adapter)

For convenience, we provide a table to denote which methods are inference-only and which require fine-tuning/training.

|                     **Method**                     | **Inference only** | **Requires training /<br> fine-tuning** |                                          **Comments**                                           |
|:--------------------------------------------------:| :----------------: | :-------------------------------------: | :---------------------------------------------------------------------------------------------: |
|        [InstructPix2Pix](#instructpix2pix)         |         ✅         |                   ❌                    | Can additionally be<br>fine-tuned for better <br>performance on specific <br>edit instructions. |
|            [Depth2Image](#depth2image)             |         ✅         |                   ❌                    |                                                                                                 |
|             [DreamBooth](#dreambooth)              |         ❌         |                   ✅                    |                                                                                                 |
|      [Textual Inversion](#textual-inversion)       |         ❌         |                   ✅                    |                                                                                                 |
|             [ControlNet](#controlnet)              |         ✅         |                   ❌                    |             A ControlNet can be <br>trained/fine-tuned on<br>a custom conditioning.             |
|               [DiffEdit](#diffedit)                |         ✅         |                   ❌                    |                                                                                                 |
|            [T2I-Adapter](#t2i-adapter)             |         ✅         |                   ❌                    |                                                                                                 |

## InstructPix2Pix

[Paper](https://arxiv.org/abs/2211.09800)

[InstructPix2Pix](../api/pipelines/pix2pix.md) is fine-tuned from Stable Diffusion to support editing input images. It takes as inputs an image and a prompt describing an edit, and it outputs the edited image.
InstructPix2Pix has been explicitly trained to work well with [InstructGPT](https://openai.com/blog/instruction-following/)-like prompts.

## Depth2Image

[Project](https://huggingface.co/stabilityai/stable-diffusion-2-depth)

[Depth2Image](../api/pipelines/stable_diffusion/depth2img.md) is fine-tuned from Stable Diffusion to better preserve semantics for text guided image variation.

It conditions on a monocular depth estimate of the original image.

## DreamBooth

[Project](https://dreambooth.github.io/)

[DreamBooth](../training/dreambooth.md) fine-tunes a model to teach it about a new subject. I.e. a few pictures of a person can be used to generate images of that person in different styles.

## Textual Inversion

[Paper](https://arxiv.org/abs/2208.01618)

[Textual Inversion](../training/text_inversion.md) fine-tunes a model to teach it about a new concept. I.e. a few pictures of a style of artwork can be used to generate images in that style.

## ControlNet

[Paper](https://arxiv.org/abs/2302.05543)

[ControlNet](../api/pipelines/controlnet.md) is an auxiliary network which adds an extra condition.
There are 8 canonical pre-trained ControlNets trained on different conditionings such as edge detection, scribbles,
depth maps, and semantic segmentations.

## DiffEdit

[Paper](https://arxiv.org/abs/2210.11427)

[DiffEdit](../api/pipelines/diffedit.md) allows for semantic editing of input images along with
input prompts while preserving the original input images as much as possible.

## T2I-Adapter

[Paper](https://arxiv.org/abs/2302.08453)

[T2I-Adapter](../api/pipelines/stable_diffusion/adapter.md) is an auxiliary network which adds an extra condition.
There are 8 canonical pre-trained adapters trained on different conditionings such as edge detection, sketch,
depth maps, and semantic segmentations.
