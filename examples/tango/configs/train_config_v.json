{
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "optim": "adamw",
    "epochs": 40,
    "betas": [
        0.9,
        0.98
    ],
    "dropout": 0.1,
    "weight_decay": 0.01,
    "warmup_steps": 1000,
    "seed": 3407,
    "start_learning_rate": 3e-5,
    "end_learning_rate": 1e-7,
    "decay_steps": 0,
    "use_ema": true,
    "clip_grad": false,
    "max_grad_norm": 1.0
}
