# Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution
[Paper](https://arxiv.org/abs/2409.12191) | [HF Model Card](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)

# Introduction
> **LRDR;** Qwen2-VL is a multimodal vision-language model series based on Qwen2, which supports inputs of text, arbitrary-resolution image, long video (20min+) and multiple languages.

> **Abstract:** The Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models.

# Get Started

## ğŸ“¦ Requirements
mindspore  |  ascend driver   |cann  |
|:--:|:--:|:--:|
| >=2.6.0    | >=24.1.RC1 |   >=8.1.RC1 |



cd examples/transformers/qwen2-vl
pip install requirements.txt
```

Pretrained weights from huggingface hub: [Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)

## Quick Start


`vqa_test.py` and `video_understanding.py` provides examples of image and video VQA. Here is an usage example of image understanding:

```python
import mindspore
from transformers import AutoProcessor
from mindone.transformers import Qwen2VLForConditionalGeneration
from mindone.transformers.models.qwen2_vl.qwen_vl_utils import process_vision_info

model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen2/Qwen2-VL-7B-Instruct", mindspore_dtype=mindspore.float32)
processor = AutoProcessor.from_pretrained("Qwen2/Qwen2-VL-7B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "demo.jpeg",  # REPLACE with your own image
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="np",
)
# convert input to Tensor
for key, value in inputs.items():
    inputs[key] = mindspore.Tensor(value)
    if inputs[key].dtype == mindspore.int64:
        inputs[key] = inputs[key].to(mindspore.int32)
generated_ids = model.generate(**inputs, max_new_tokens=128)
output_text = processor.batch_decode(
    generated_ids,
    skip_special_tokens=True,
    clean_up_tokenization_spaces=False
)[0]
print(output_text)
```

## **Notice**  
When setting fp32 on 910B4(32GB) machine, inference process may raise OOM error. Becausem the theoretical memory consumption(model weights+activations+memory fragments) may reach to maximum memory on 910B4 machine.  
In this case, some methods could be tried to reduce NPU memory:  
- Method 1. set mindspore_dtype to ms.bfloat16 or ms.float16(model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen2/Qwen2-VL-7B-Instruct", mindspore_dtype=mindspore.bfloat16)). The theoretical memory consumption would be reduced to 14GB.  
- Method 2. Reduce image size  
- Method 3. change the machine to 910B1/910B2/910B3.


# Performance
## Inference

### Inference Speed
Experiments are tested on ascend 910* pynative mode.

Input an image or a list of video frames, and a text prompt, output textual response.

- mindspore 2.6.0

|model name	| precision* | cards	| batch size| resolution | flash attn |	tokens/s| step  | weight |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 1372x2044 (image) | OFF | 4.17 | 128| [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 12x308x476(video) | OFF | 4.35 | 115 | [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 1372x2044 (image) | ON  | 5.26 | 128 | [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 12x308x476(video) | ON  | 4.55 | 63 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct | bf16 | 1 | 1 | 1372x2044 (image) | ON  | 4.76 | 128 | [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct | bf16 | 1 | 1 | 12x308x476(video) | ON  | 4.76 | 63 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)

- mindspore 2.5.0

|model name	| precision* | cards	| batch size| resolution | flash attn |	tokens/s| step | weight |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 1372x2044 (image) | OFF | 2.50 | 128 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 12x308x476(video) | OFF | 2.78 | 128 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 1372x2044 (image) | ON  | 2.94 | 127 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct |  fp16 | 1 | 1 | 12x308x476(video) | ON  | 3.33 | 128 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct | bf16 | 1 | 1 | 1372x2044 (image) | ON  | 3.13| 121 |  [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
| Qwen2-VL-7B-Instruct | bf16 | 1 | 1 | 12x308x476(video) | ON  | 3.33| 128|   [weight](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)

*note: use mixed precision, `LayerNorm` fp32.

### Inference Results

Input an image or a list of video frames, and a text prompt (English, Chinese or other lanugage), output textual response.

#### Image VQA
Input:

*resolution: 1372x2044
<br>
<img src="https://github.com/user-attachments/assets/0520288b-2e21-4b10-b506-1c8b54a1737f" width="512px">

text prompt: `Describe this image.`
<br>
Response: 'This image depicts a serene beach scene at sunset. A woman and her dog are sitting on the sand, enjoying each other's company. The woman is wearing a plaid shirt and dark pants, and she is sitting cross-legged with her dog. The dog which appears to be a large breed, is wearing a harness and is giving a high-five to the woman. The beach is relatively empty, with gentle waves in the background. The lighting is warm and golden, indicating that the photo was taken during the golden hour, just before sunset. The overall atmosphere of the image is peaceful and joyful.'

text prompt: `è¯·æè¿°è¯¥å›¾ç‰‡ã€‚`<br>
Response: "å›¾ä¸­æ˜¯ä¸€ä¸ªå¥³äººå’Œä¸€åªç‹—åœ¨æ²™æ»©ä¸Šç©è€ã€‚å¥³äººç©¿ç€æ ¼å­è¡¬è¡«ï¼Œååœ¨æ²™æ»©ä¸Šï¼Œå¥¹çš„ç‹—æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œç©¿ç€ç‹—ç»³ï¼Œä¼¸å‡ºå‰çˆªå’Œå¥³äººå‡»æŒã€‚å¥³äººå’Œç‹—éƒ½é¢å¸¦å¾®ç¬‘ï¼Œçœ‹èµ·æ¥éå¸¸å¼€å¿ƒã€‚èƒŒæ™¯æ˜¯å¹¿é˜”çš„æµ·æ´‹å’Œå¤©ç©ºï¼Œé˜³å…‰æ´’åœ¨æ²™æ»©ä¸Šï¼Œæ•´ä¸ªåœºæ™¯æ˜¾å¾—éå¸¸æ¸©é¦¨å’Œæ„‰å¿«ã€‚"

#### Video VQA
Input:
*resolution: 12x308x476
<br>
<a href="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/Operate%20a%20Mobile%20Phone.mp4"><img src="https://github.com/user-attachments/assets/0b4afe5d-021d-45be-8877-75b9a39b1ccb" width="512px"></a>

text prompt: `Describe this video.`<br>
Response: 'The video shows a computer screen with a web browser open to a search engine. The search bar at the top of the screen displays the text "What's a good restaurant in San Diego?" The search results are displayed below the search bar, with the top result being a map of San Diego with several restaurant locations marked. The screen also shows a list of search suggestions, including "What's a good restaurant in San Diego?" and "What's a good restaurant in San Diego for families?" The video ends with the search results still displayed on the screen.'

text prompt: `è¯·æè¿°è¯¥è§†é¢‘ã€‚`<br>
Response: "è§†é¢‘ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç”µè„‘å±å¹•ï¼Œä¸Šé¢æœ‰ä¸¤ä¸ªçª—å£ã€‚å·¦è¾¹çš„çª—å£æ˜¾ç¤ºäº†ä¸€ä¸ªç½‘é¡µï¼Œä¸Šé¢æœ‰ä¸€ä¸ªæœç´¢æ¡†å’Œä¸€äº›æœç´¢å»ºè®®ã€‚å³è¾¹çš„çª—å£æ˜¾ç¤ºäº†ä¸€ä¸ªå‘½ä»¤è¡Œç•Œé¢ï¼Œæ˜¾ç¤ºäº†ä¸€äº›æ–‡æœ¬ã€‚"
