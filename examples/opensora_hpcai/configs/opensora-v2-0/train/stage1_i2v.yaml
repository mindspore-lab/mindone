env:
  mode: 0
  jit_level: O1
  max_device_memory: 59GB
  seed: 42
  distributed: False
  debug: False

model:
  from_pretrained:
  guidance_embed: False
  fused_qkv: False
  use_liger_rope: True
  # model architecture
  in_channels: 64
  vec_in_dim: 768
  context_in_dim: 4096
  hidden_size: 3072
  mlp_ratio: 4.0
  num_heads: 24
  depth: 19
  depth_single_blocks: 38
  axes_dim: [ 16, 56, 56 ]
  theta: 10_000
  qkv_bias: True
  cond_embed: True
  recompute_every_nth_block: 1
  dtype: bf16

ae:
  from_pretrained: hpcai-tech/Open-Sora-v2/hunyuan_vae.safetensors
  in_channels: 3
  out_channels: 3
  layers_per_block: 2
  latent_channels: 16
  use_spatial_tiling: True
  use_temporal_tiling: False
  dtype: bf16

dataset:
  v2_pipeline: True
  sample_n_frames: 5
  csv_path: CSV_PATH
  video_folder: VIDEO_FOLDER
  text_emb_folder:
    t5: UL2_FOLDER
    clip: BYT5_FOLDER
  empty_text_emb:
    t5: EMPTY_TEXT_EMB
    clip: EMPTY_TEXT_EMB
  text_drop_prob:
    t5: 0.31622777
    clip: 0.31622777
  vae_scale_factor: 0.476986
  vae_shift_factor: 0
  apply_transforms_dataset: True
  output_columns: [ "video", "video_ids", "t5_caption", "txt_ids", "clip_caption", "shift_alpha" ]

bucket_config:
  init_args:
    bucket_config:
      256px:
        1: [ 1.0, 45 ]
        5: [ 1.0, 12 ]
        9: [ 1.0, 12 ]
        13: [ 1.0, 12 ]
        17: [ 1.0, 12 ]
        21: [ 1.0, 12 ]
        25: [ 1.0, 12 ]
        29: [ 1.0, 12 ]
        33: [ 1.0, 12 ]
        37: [ 1.0, 6 ]
        41: [ 1.0, 6 ]
        45: [ 1.0, 6 ]
        49: [ 1.0, 6 ]
        53: [ 1.0, 6 ]
        57: [ 1.0, 6 ]
        61: [ 1.0, 6 ]
        65: [ 1.0, 6 ]
        69: [ 1.0, 4 ]
        73: [ 1.0, 4 ]
        77: [ 1.0, 4 ]
        81: [ 1.0, 4 ]
        85: [ 1.0, 4 ]
        89: [ 1.0, 4 ]
        93: [ 1.0, 4 ]
        97: [ 1.0, 4 ]
        101: [ 1.0, 3 ]
        105: [ 1.0, 3 ]
        109: [ 1.0, 3 ]
        113: [ 1.0, 3 ]
        117: [ 1.0, 3 ]
        121: [ 1.0, 3 ]
        125: [ 1.0, 3 ]
        129: [ 1.0, 3 ]
      768px:
        1: [ 0.5, 13 ]
      1024px:
        1: [ 0.5, 7 ]

dataloader:
  shuffle: True
  num_workers_dataset: 4

train:
  pipeline:
    is_causal_vae: True
    condition_config:
      t2v: 1
      i2v_head: 5   # train i2v (image as first frame) with weight 5
      i2v_loop: 1   # train image connection with weight 1
      i2v_tail: 1   # train i2v (image as last frame) with weight 1

  sequence_parallel:
    shards: 1   # 1 == no SP

  options:
    steps: 20000

  lr_scheduler:
    name: constant
    lr: 1e-5
    warmup_steps: 0

  optimizer:
    name: adamw_bf16
    eps: 1e-15
    betas: [ 0.9, 0.999 ]
    weight_decay: 0

  loss_scaler:
    class_path: mindspore.nn.FixedLossScaleUpdateCell   # or DynamicLossScaleUpdateCell in FP16
    init_args:
      loss_scale_value: 1

  settings:
    zero_stage: 3
    gradient_accumulation_steps: 1
    clip_grad: True
    clip_norm: 1.0

  save:
    ckpt_save_policy: latest_k
    ckpt_save_interval: 500
    ckpt_max_keep: 10
    log_interval: 1
    save_ema_only: False
    record_lr: False
    ckpt_combine_online: False

save:
  output_path: ../../../output/stage1_i2v  # the path is relative to this config
