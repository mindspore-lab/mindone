# Text2Video-Zero
> [Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://arxiv.org/pdf/2303.13439.pdf)

## Introduction
Text2Video-Zero introduces a new task of zeroshot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g. Stable Diffusion), making them suitable for the video domain. In practice, with a pretrained generative model, it can impaint videos with text prompts and other controls like canny edge. And it doesn't need any further fine-tuning.

<p align="center">
   <img src="https://github.com/Gaohan123/mindone/assets/20148503/c5c27f00-3c20-479c-a540-70a0c8db0d48" width=500 />
</p>
<p align="center">
  <em> Figure 1. Illustration of a ControlNet [<a href="#references">1</a>] </em>
</p>



## Get Started
**MindONE** supports Text2Video-Zero with pretrained Stable Diffusion model based on MindSpore and Ascend platforms.

### Preparation

#### Dependency
- mindspore >= 2.1  [[install](https://www.mindspore.cn/install)]
- python >= 3.7

Install the dependent packages by running:
```shell
pip install -r requirements.txt
```

#### Trained Models
1. Canny edge maps:

   Please download the trained model [SD1.5-canny-ms checkpoint](https://download.mindspore.cn/toolkits/mindone/stable_diffusion/control_canny_sd_v1.5_static-6350d204.ckpt)
   It is the pretrained ControlNet[1] with canny edge control, which is one of basic module for Text2Video-Zero.

2. Others:

   Coming soon.

Put them under `stable_diffusion_v2/models` folder.

#### Testing videos preparation
Please prepare the videos that you want to impaint. There are some examples:

<div align="center">
<img src="https://github.com/Gaohan123/mindone/assets/20148503/24953d5f-dc20-45d4-ba45-ea602466eaa7" width="160" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/f1e21d57-7882-4e4f-a4c0-01568122e43b" width="160" height="240" />
</div>
<p align="center">
  <em> Images prepared to add extra controls </em>
</p>

Put them in an arbitrary directory on your machine. For example, `path/to/test_videos`. We already put some in `stable_diffusion_v2/inference/videos`.

#### Load controls
1. Canny edge maps:
   It is implemented with opencv Canny API directly, you don't need to take more actions.

### Image Generation
1. Canny edge maps:

   Go to the directory `stable_diffusion_v2/inference`.

   Open the config file `./config/text2video-zero.yaml`, set `video_path` with the path of video that you want to impaint. set corresponding text prompt in `prompt`.

   Open the config file `./config/model/v1-inference-text2video-zero.yaml`, set `pretrained_ckpt` with corresponding pretrained model path, e.g. `stable_diffusion_v2/models/control_canny_sd_v1.5_static-6350d204.ckpt`.

   Run the command below to generate images

```shell
python sd_infer.py \
--device_target=Ascend \
--task=text2video_zero \
--model=./config/model/v1-inference-text2video-zero.yaml \
--sampler=./config/schedule/ddim.yaml \
--sampling_steps=20 \
--n_iter=1 \
--n_samples=1 \
--seed=41
```

#### Important arguments in the shell scripts
- `device_target`: Device target, should be in [Ascend, GPU, CPU] (Default is `Ascend`)
- `task`: What kind of inference task you want to do.
- `model`: The path of config file of target model.
- `sampler`: The denosing sampler you want to use.
- `sampling_steps`: How many steps you want the inference to do.
- `seed`: The random seed. Different seeds will bring distinct impainting results.

### Results
Generated images will be saved in `stable_diffusion_v2/inference/output/samples` by default.
Here are samples generated by a beer video and a girl video:

<div align="center">
<img src="https://github.com/Gaohan123/mindone/assets/20148503/6d543d0b-e1c2-447b-805a-19d9253a488b" width="160" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/90835ad9-38aa-4ca2-862a-0344c0760463" width="160" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/bf1bc4e9-c16c-4d37-8b72-cbc83fd8569e" width="160" height="240" />
</div>
<p align="center">
  <em> Generated Images with ControlNet on DeeplabV3Plus segmentation edge maps </em>
</p>


## Reference
[1] [ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)

[2] [Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://arxiv.org/pdf/2303.13439.pdf)
