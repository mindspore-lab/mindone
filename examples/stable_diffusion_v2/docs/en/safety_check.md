# Safety Checker

## Introduction
To validate the safeness of images generated by a model, we implement two safety checkers in `safety_checker_1.py` and `safety_checker_2.py`.

`safety_checker_1.py` applies CLIP to compute the similarity between the given images and a pre-defined list of NSFW concepts (two possible `.yaml` files to choose from). This is consistent with the one used in CompVis's stable diffusion 1.x from `diffusers` implementation. The output is a list of similarity scores, each corresponding to a concept.

`safety_checker_2.py` trains a NSFW classifier with a supervisied approach, taking image features from CLIP as its input. This is consistent with the one used in StabilityAI's stable diffusion 2.x. The output is a number in 0-1, representing the probability of the generated image being NSFW.

## Usage
You can use the checkers in your image generation pipelines. It is by default included in `text_to_image.py` for checking the safety of stable diffusion outputs. In addition, you can test the checkers on saved images. In `examples/stable_diffusion` directory, please run,

- MindSpore backend
```
python tools/safety_checker/safety_checker_1.py --ckpt_path <path-to-model> --image_path_or_dir <path-to-image>
python tools/safety_checker/safety_checker_2.py --ckpt_path <path-to-model> --image_path_or_dir <path-to-image>
```
- PyTorch backend
```
python tools/safety_checker/safety_checker_1.py --backend pt --image_path_or_dir <path-to-image>
python tools/safety_checker/safety_checker_2.py --backend pt --image_path_or_dir <path-to-image>
```
By default, we use MindSpore backend for CLIP score computing. You may swich to use `torch` and `transformers` by setting `--backend=pt`.

For more usage, please run `python tools/safety_checker/safety_checker_{1, 2}.py -h`.

You need to download the checkpoint file for a CLIP model of your choice. Download links for some models are provided below.

- [clip_vit_b_16](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/clip/clip_vit_b_16.ckpt)
- [clip_vit_b_32](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/clip/clip_vit_b_32.ckpt)
- [clip_vit_l_14](https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/clip/clip_vit_l_14.ckpt) (Default)

For other compatible models, e.g., OpenCLIP, you can download `pytorch_model.bin` from HuggingFace (HF) and then convert to `.ckpt` using `eval/clip_score/utils/convert_weight.py`. When using a model other than the default, you should supply the path to your model's config file. Some useful examples are provided in `ldm/models/clip/configs`.

`image_path_or_dir` should lead to an image file or a directory containing images. If it is a directory, then the images are sorted by their filename in an ascending order.

## Reference

[1] https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py
[2] https://github.com/LAION-AI/CLIP-based-NSFW-Detector
