# ControlNet based on Stable Diffusion
> [ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)

## Introduction
ControlNet controls pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small. Large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like canny edge maps, segmentation maps, keypoints, etc.

## Get Started
**MindONE** supports ControlNet generation for Stable Diffusion models based on MindSpore and Ascend platforms.

### Preparation

#### Dependency
- mindspore >= 1.9  [[install](https://www.mindspore.cn/install)] (2.0 is recommended for the best performance.)
- python >= 3.7

Install the dependent packages by running:
```shell
pip install -r requirements.txt
```

#### Trained Models
1. Canny edge maps:

   Please download the trained model [SD1.5-canny-ms checkpoint]

2. Segmentation edge maps:

   Please download the trained model [SD1.5-segmentation-ms checkpoint]

3. Others:

   Coming soon.

put them under `stable_diffusion_v2/models` folder.

#### Testing images preparation
Please prepare the images that you want to add extra conditions. There are some examples:


Put them in an arbitrary directory on your machine. For example, `path/to/test_imgs`.

#### Load controls
1. Canny edge maps:
   It is implemented with opencv Canny API directly, you don't need to take more actions.

2. Segmentation edge maps (DeeplabV3Plus):
   The Segmentation Detector is implemented with DeeplabV3Plus [DeeplabV3Plus](https://arxiv.org/abs/1802.02611). Please download the pretrained model from [DeeplabV3Plus checkpoint](https://download.mindspore.cn/models/r1.9/deeplabv3plus_s16_ascend_v190_voc2012_research_cv_s16acc79.06_s16multiscale79.96_s16multiscaleflip80.12.ckpt)

   Attention: As the DeeplabV3Plus is trained on VOC dataset, currently it only supports prompts related to the objects: 'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train','tvmonitor'. More is coming soon.

### Image Generation
1. Canny edge maps:

   Run the command below to generate images

   ```shell
   sh run_controlnet_canny2image.sh
   ```

2. Segmentation edge maps (DeeplabV3Plus):

   Run the command below to generate images

   ```shell
   sh run_controlnet_seg2image.sh
   ```

#### Important arguments in the shell scripts
- `model_ckpt`: The path to load trained ControlNet model. Please set differently according to different controls. (Default is `stable_diffusion_v2/models` introduced above.)
- `input_image`: The path of image you want to add more control.
- `prompt`: The text prompt corresponds to the input_image.
- `n_samples`: How many samples you want to generate.
- `low_threshold`: A parameter required by Canny edge maps control.
- `high_threshold`: A parameter required by Segmentation edge maps control.
- `segmentation_ckpt_path`: The path to load segmentation detector pretrained model. It is required by Segmentation edge maps.

### Results
Generated images will be saved in `stable_diffusion_v2/output` by default.
Here are samples generated by a bird image with DeeplabV3Plus segmentation edge maps:
