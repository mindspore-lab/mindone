version: SDXL-base-1.0
model:
    target: gm.models.diffusion.DiffusionEngine
    params:
        scale_factor: 0.13025
        disable_first_stage_amp: True

        denoiser_config:
            target: gm.modules.diffusionmodules.denoiser.DiscreteDenoiser
            params:
                num_idx: 1000

                weighting_config:
                    target: gm.modules.diffusionmodules.denoiser_weighting.EpsWeighting
                scaling_config:
                    target: gm.modules.diffusionmodules.denoiser_scaling.EpsScaling
                discretization_config:
                    target: gm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

        network_config:
            target: gm.modules.diffusionmodules.openaimodel.UNetModel_lora
            params:
                lora_dim: 4                 # for LoRA
                lora_alpha: 4               # for LoRA
                lora_dropout: 0.0           # for LoRA
                lora_merge_weights: False   # for LoRA
                only_save_lora: True        # for LoRA

                adm_in_channels: 2816
                num_classes: sequential
                in_channels: 4
                out_channels: 4
                model_channels: 320
                attention_resolutions: [4, 2]
                num_res_blocks: 2
                channel_mult: [1, 2, 4]
                num_head_channels: 64
                use_spatial_transformer: True
                use_linear_in_transformer: True
                transformer_depth: [1, 2, 10]  # note: the first is unused (due to attn_res starting at 2) 32, 16, 8 --> 64, 32, 16
                context_dim: 2048
                spatial_transformer_attn_type: vanilla  # vanilla, flash-attention
                legacy: False
                use_recompute: False

        conditioner_config:
            target: gm.modules.GeneralConditioner
            params:
                emb_models:
                  # crossattn cond
                  - is_trainable: False
                    input_key: txt
                    target: gm.modules.embedders.modules.FrozenCLIPEmbedder
                    params:
                      layer: hidden
                      layer_idx: 11
                      version: openai/clip-vit-large-patch14
                      # pretrained: ''
                  # crossattn and vector cond
                  - is_trainable: False
                    input_key: txt
                    target: gm.modules.embedders.modules.FrozenOpenCLIPEmbedder2
                    params:
                      arch: ViT-bigG-14-Text
                      freeze: True
                      layer: penultimate
                      always_return_pooled: True
                      legacy: False
                      require_pretrained: False
                      # pretrained: ''  # laion2b_s32b_b79k.ckpt
                  # vector cond
                  - is_trainable: False
                    input_key: original_size_as_tuple
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two
                  # vector cond
                  - is_trainable: False
                    input_key: crop_coords_top_left
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two
                  # vector cond
                  - is_trainable: False
                    input_key: target_size_as_tuple
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two

        first_stage_config:
            target: gm.models.autoencoder.AutoencoderKLInferenceWrapper
            params:
                embed_dim: 4
                monitor: val/rec_loss
                ddconfig:
                    attn_type: vanilla
                    double_z: true
                    z_channels: 4
                    resolution: 256
                    in_channels: 3
                    out_ch: 3
                    ch: 128
                    ch_mult: [1, 2, 4, 4]
                    num_res_blocks: 2
                    attn_resolutions: []
                    dropout: 0.0
                lossconfig:
                    target: mindspore.nn.Identity

        # Module for Train
        sigma_sampler_config:
            target: gm.modules.diffusionmodules.sigma_sampling.DiscreteSampling
            params:
                num_idx: 1000
                discretization_config:
                    target: gm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

        loss_fn_config:
            target: gm.modules.diffusionmodules.loss.StandardDiffusionLoss


optim:
    base_learning_rate: 1.0e-4

    optimizer_config:
        target: mindspore.nn.AdamWeightDecay  # mindspore.nn.SGD
        params:
            beta1: 0.9
            beta2: 0.999
            weight_decay: 0.01
            eps: 1.0e-8

#    scheduler_config:
#        target: gm.lr_scheduler.LambdaLinearScheduler
#        params:
#            warm_up_steps: [ 10000 ]
#            cycle_lengths: [ 10000000000000 ]
#            f_start: [ 1.e-6 ]
#            f_max: [ 1. ]
#            f_min: [ 1. ]


data:
    per_batch_size: 1
    total_step: 15000
    num_parallel_workers: 6
    python_multiprocessing: False
    shuffle: True

    dataset_config:
        target: gm.data.dataset.Text2ImageDataset
        params:
            target_size: 1024
            transforms:
                - target: gm.data.mappers.Resize
                  params:
                    size: 1024
                    interpolation: 3
                - target: gm.data.mappers.Rescaler
                  params:
                    isfloat: False
                - target: gm.data.mappers.AddOriginalImageSizeAsTupleAndCropToSquare
                - target: gm.data.mappers.Transpose
                  params:
                    type: hwc2chw
