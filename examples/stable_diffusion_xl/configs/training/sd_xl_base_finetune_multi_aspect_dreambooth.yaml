version: SDXL-base-1.0
model:
    target: gm.models.diffusion.DiffusionEngineDreamBooth
    params:
        scale_factor: 0.13025
        disable_first_stage_amp: True

        denoiser_config:
            target: gm.modules.diffusionmodules.denoiser.DiscreteDenoiser
            params:
                num_idx: 1000

                weighting_config:
                    target: gm.modules.diffusionmodules.denoiser_weighting.EpsWeighting
                scaling_config:
                    target: gm.modules.diffusionmodules.denoiser_scaling.EpsScaling
                discretization_config:
                    target: gm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

        network_config:
            target: gm.modules.diffusionmodules.openaimodel.UNetModel_lora
            params:
                lora_dim: 4                 # for LoRA
                lora_alpha: 4               # for LoRA
                lora_dropout: 0.0           # for LoRA
                lora_merge_weights: False   # for LoRA
                only_save_lora: True        # for LoRA

                adm_in_channels: 2816
                num_classes: sequential
                in_channels: 4
                out_channels: 4
                model_channels: 320
                attention_resolutions: [4, 2]
                num_res_blocks: 2
                channel_mult: [1, 2, 4]
                num_head_channels: 64
                use_spatial_transformer: True
                use_linear_in_transformer: True
                transformer_depth: [1, 2, 10]  # note: the first is unused (due to attn_res starting at 2) 32, 16, 8 --> 64, 32, 16
                context_dim: 2048
                spatial_transformer_attn_type: flash-attention # vanilla, flash-attention
                legacy: False

        conditioner_config:
            target: gm.modules.GeneralConditioner
            params:
                emb_models:
                  # crossattn cond
                  - is_trainable: False
                    input_key: txt
                    target: gm.modules.embedders.modules.FrozenCLIPEmbedder
                    params:
                      layer: hidden
                      layer_idx: 11
                      version: openai/clip-vit-large-patch14
                      # pretrained: ''
                  # crossattn and vector cond
                  - is_trainable: False
                    input_key: txt
                    target: gm.modules.embedders.modules.FrozenOpenCLIPEmbedder2
                    params:
                      arch: ViT-bigG-14-Text
                      freeze: True
                      layer: penultimate
                      always_return_pooled: True
                      legacy: False
                      require_pretrained: False
                      # pretrained: ''  # laion2b_s32b_b79k.ckpt
                  # vector cond
                  - is_trainable: False
                    input_key: original_size_as_tuple
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two
                  # vector cond
                  - is_trainable: False
                    input_key: crop_coords_top_left
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two
                  # vector cond
                  - is_trainable: False
                    input_key: target_size_as_tuple
                    target: gm.modules.embedders.modules.ConcatTimestepEmbedderND
                    params:
                      outdim: 256  # multiplied by two

        first_stage_config:
            target: gm.models.autoencoder.AutoencoderKLInferenceWrapper
            params:
                embed_dim: 4
                monitor: val/rec_loss
                ddconfig:
                    attn_type: vanilla
                    double_z: true
                    z_channels: 4
                    resolution: 256
                    in_channels: 3
                    out_ch: 3
                    ch: 128
                    ch_mult: [1, 2, 4, 4]
                    num_res_blocks: 2
                    attn_resolutions: []
                    dropout: 0.0
                    decoder_attn_dtype: fp16
                lossconfig:
                    target: mindspore.nn.Identity

        # Module for Train
        sigma_sampler_config:
            target: gm.modules.diffusionmodules.sigma_sampling.DiscreteSampling
            params:
                num_idx: 1000
                discretization_config:
                    target: gm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization

        loss_fn_config:
            target: gm.modules.diffusionmodules.loss.StandardDiffusionLoss


optim:
    base_learning_rate: 1.0e-4

    optimizer_config:
        target: mindspore.nn.AdamWeightDecay  # mindspore.nn.SGD
        params:
            beta1: 0.9
            beta2: 0.999
            weight_decay: 0.01
            eps: 1.0e-8


data:
    per_batch_size: 1
    total_step: 4000
    num_parallel_workers: 2
    python_multiprocessing: False
    shuffle: True

    dataset_config:
        target: gm.data.dataset.Text2ImageDataset
        params:
            multi_aspect: [
                [704, 1408],
                [768, 1344],
                [832, 1216],
                [896, 1152],
                [960, 1088],
                [1024, 1024],
                [1088, 960],
                [1152, 896],
                [1216, 832],
                [1344, 768],
                [1472, 704],
                [1600, 640],
                [1728, 576],
                [512, 512],
                [768, 768],
                [704, 1344],
                [768, 1280],
                [832, 1152],
                [896, 1088],
                [960, 1024],
                [1024, 960],
                [1088, 896],
                [1152, 832],
                [1280, 768],
                [1344, 704],
                [1408, 704],
                [1472, 704],
                [1536, 640],
                [1600, 640],
                [1664, 576],
                [1728, 576]
            ]
            batched_transforms:
                - target: gm.data.mappers.BatchedResizedAndRandomCrop
                - target: gm.data.mappers.BatchedRescaler
                  params:
                    isfloat: False
                - target: gm.data.mappers.BatchedTranspose
                  params:
                    type: hwc2chw
