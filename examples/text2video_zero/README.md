# Text2Video-Zero
> [Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://arxiv.org/pdf/2303.13439.pdf)

## Introduction
Text2Video-Zero introduces a new task of zeroshot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g. Stable Diffusion), making them suitable for the video domain. In practice, with a pretrained generative model, it can inpaint videos with text prompts and other controls like canny edge. And it doesn't need any further fine-tuning.

<p align="center">
   <img src="https://github.com/Gaohan123/mindone/assets/20148503/dd002255-3abb-4c3f-b97d-38826787ca9a" width=1000 />
</p>
<p align="center">
  <em> Figure 1. Illustration of Text2Video-Zero overview [<a href="#references">2</a>] </em>
</p>


<p align="center">
   <img src="https://github.com/Gaohan123/mindone/assets/20148503/06b06488-6f8c-40e4-bbce-858cc593a7eb" width=500 />
</p>
<p align="center">
  <em> Figure 2. Illustration of Text2Video-Zero based on ControlNet [<a href="#references">1</a>] </em>
</p>



## Get Started
**MindONE** supports Text2Video-Zero with pretrained Stable Diffusion model based on MindSpore and Ascend platforms.

### Preparation

#### Dependency
- mindspore >= 2.0  [[install](https://www.mindspore.cn/install)]
- python >= 3.7

Install the dependent packages by running:
```shell
pip install -r requirements.txt
```

#### Trained Models
1. Canny edge maps:

   Please download the trained model [SD1.5-canny-ms checkpoint](https://download.mindspore.cn/toolkits/mindone/stable_diffusion/control_canny_sd_v1.5_static-6350d204.ckpt)
   It is the pretrained ControlNet[1] with canny edge control, which is one of basic module for Text2Video-Zero.

2. Segmentation map with SAM model [3]:

   Coming soon.

Put them under `text2video_zero/models` folder.

#### Data preparation
Please prepare the videos that you want to inpaint. There are two examples:

<div align="center">
<img src="https://github.com/Gaohan123/mindone/assets/20148503/2a460733-3f47-4f04-8a12-c215adfb7536" width="240" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/d21ce9a9-7138-49dd-8275-1a259b2ecf58" width="240" height="240" />
</div>
<p align="center">
  <em> Videos prepared to inpaint </em>
</p>

For example, in `text2video_zero/videos`. We already put some in it.

#### Load controls
1. Canny edge maps:
   It is implemented with opencv Canny API directly, you don't need to take more actions. Specifically, it is implemented in `/conditions/canny/canny_detector.py`:
   ```python
   import cv2

   __all__ = ["CannyDetector"]


   class CannyDetector:
      def __call__(self, img, low_threshold, high_threshold):
         return cv2.Canny(img, low_threshold, high_threshold)
   ```
   Where "img" is the input video frame to get edge map.

### Image Generation
1. Canny edge maps:

   Go to the directory `text2video_zero`.

   Open the config file `./config/text2video-zero.yaml`, set `video_path` with the path of video that you want to inpaint. set corresponding text prompt in `prompt`. set pretrained model path in `pretrained_ckpt`, e.g. models/control_canny_sd_v1.5_static-6350d204.ckpt`.

   Run the command below to generate images

```shell
python infer.py \
--device_target=Ascend \
--model=./config/model/v1-inference-text2video-zero.yaml \
--sampler=./config/schedule/ddim.yaml \
--sampling_steps=20 \
--scale=9.0 \
--inputs_config_path=./config/text2video-zero.yaml
```

#### Important arguments in the shell scripts
- `device_target`: Device target, should be in [Ascend, GPU, CPU] (Default is `Ascend`)
- `task`: What kind of inference task you want to do.
- `model`: The path of config file of target model.
- `sampler`: The denosing sampler you want to use.
- `sampling_steps`: How many steps you want the inference to do.
- `inputs_config_path`: The path of config file for necessary input arguments

### Results
Generated images will be saved in `text2video_zero/output/samples` by default.
Here are samples generated by a beer video and a girl video:

<div align="center">
<img src="https://github.com/Gaohan123/mindone/assets/20148503/2a460733-3f47-4f04-8a12-c215adfb7536" width="240" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/711356ca-5478-45cd-a417-50e923e7dad5" width="240" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/db0e6dff-1369-4254-b062-e5f70f7b64a5" width="240" height="240" />
</div>
<p align="center">
  <em> Original video (left), edge map (middle), inpainted video (right) with prompt "oil painting of a deer, a high-quality, detailed, and professional photo" </em>
</p>


<div align="center">
<img src="https://github.com/Gaohan123/mindone/assets/20148503/d21ce9a9-7138-49dd-8275-1a259b2ecf58" width="240" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/6623d478-e701-4270-a595-b425925b3175" width="240" height="240" />
<img src="https://github.com/Gaohan123/mindone/assets/20148503/f26aebb6-2cd3-492e-8a16-cef061717027" width="240" height="240" />
</div>
<p align="center">
  <em> Original video (left), edge map (middle), inpainted video (right) with prompt "A beautiful girl, a high-quality, detailed, and professional photo" </em>
</p>


## Reference
[1] [ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)

[2] [Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators](https://arxiv.org/pdf/2303.13439.pdf)

[3] [Segment Anything](https://scontent-hkg4-1.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=JKPXxtndwo0AX96Eou8&_nc_ht=scontent-hkg4-1.xx&oh=00_AfBrXlvUgk_kS0mjmFTvJUbYKbIwFKKXlBaUhvCEd0aF3g&oe=653FF8A7)
