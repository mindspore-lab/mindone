experiment:
    type: lrm
    parent: lrm-objaverse
    child: large

model:
    camera_embed_dim: 1024
    rendering_samples_per_ray: 96   # w/ larger memory use: 128
    transformer_dim: 768            # w/ larger memory use: 1024
    transformer_layers: 12          # w/ larger net use: 16
    transformer_heads: 12           # w/ larger memory use: 16
    triplane_low_res: 32
    triplane_high_res: 64
    triplane_dim: 80
    encoder_type: dinov2
    encoder_model_name: dinov2_vitb14_reg
    encoder_feat_dim: 768
    encoder_freeze: true

dataset:
    subsets:
        -   name: objaverse
            root_dirs:
                - <REPLACE_WITH_RENDERING_ROOT e.g. datasets/openlrm_448x448>
            meta_path:
                train: <TRAIN_UIDS_IN_JSON e.g. datasets/openlrm_448x448/meta_train.json>
                val: <VAL_UIDS_IN_JSON e.g. datasets/openlrm_448x448/meta_val.json>
            sample_rate: 1.0
    sample_side_views: 3
    source_image_res: 448
    render_image:
        low: 128
        high: 384
        region: 128
    normalize_camera: true
    normed_dist_to_center: auto
    num_train_workers: 8
    num_val_workers: 2
    pin_mem: true

train:
    find_unused_parameters: false
    loss:
        pixel_weight: 1.0
        perceptual_weight: 1.0
        tv_weight: 5e-4
    batch_size: 1

val:
    batch_size: 1
    global_step_period: 1000
    debug_batches: null
