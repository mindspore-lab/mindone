# Captioners for video captioning

Human labeling of videos is expensive and time-consuming.
We adopt powerful image captioning models to generate
captions for videos.

## PLLaVA Captioning
HPC-AI captioned their training videos
with the [PLLaVA](https://github.com/magic-research/PLLaVA) model.
PLLaVA performs highly competitively on multiple
video-based text generation benchmarks including
[MVbench](https://paperswithcode.com/sota/video-question-answering-on-mvbench?p=pllava-parameter-free-llava-extension-from-1).

| Model      | Link                                                     |
| ------------ |----------------------------------------------------------|
| pllava-7b  | [pllava-7b · Hugging Face](https://huggingface.co/ermu2001/pllava-7b)   |
| pllava-13b | [pllava-13b · Hugging Face](https://huggingface.co/ermu2001/pllava-13b) |
| pllava-34b | [pllava-34b · Hugging Face](https://huggingface.co/ermu2001/pllava-34b) |

And put it under `./models` or your designated directory. You can
also use a customized directory by using the option
`--pretrained_model_name_or_path` when running the script.

For captioning, we use the following command:

```bash
python pllavarun.py --video path_to_your_video
```

We support the following arguments for PLLaVA captioning:

- `video:` The path to the video file.
- `pretrained_model_name_or_path`: the PLLaVA model directory.
- `num_frames`: the number of frames to extract from the videos. Default 4.
- `question`: The prompt used to generate captions. Default "Describe the video in detail".
- `max_new_tokens`: Maximum new tokens generated by PLLaVA, default 200.
