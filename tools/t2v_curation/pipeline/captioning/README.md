# Video Captioning

Human labeling of videos is expensive and time-consuming.
We adopt powerful image captioning models to generate
captions for videos. We support LLaVA, PLLaVA, and Qwen2-VL Captioning.

## LLaVA Captioning
LLaVA is utilized in the HunyuanVideo-I2V model. For usage,
 you may download the model [here](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-transformers)
 and put it under `./pretrained_models/llava-llama-3-8b-v1_1-transformers`. You can
 also use a customized directory by using the option
 `--pretrained_model_name_or_path` when running the script.

Currently, we only support captioning on Ascend.

```bash
export PYTHONPATH=$(pwd)
msrun --worker_num=2 --local_worker_num=2 --join=True \
 --log_dir=msrun_log pipeline/captioning/caption_llava.py \
 /path/to/meta.csv
```
Modify `worker_num` and `local_worker_num` based on your resource.

We support the following arguments for LLaVA captioning:

- `pretrained_model_name_or_path`: the LLaVA model directory.
- `question`: The prompt used to generate captions. Default "Describe the video in detail".
- `max_new_tokens`: Maximum new tokens generated by LLaVA, default 200.
- `bs`: Batch size. Default 1.
- `skip_if_existing`: Skip processing if output already exists. Default False.

## PLLaVA Captioning
HPC-AI captioned their training videos
with the [PLLaVA](https://github.com/magic-research/PLLaVA) model.
PLLaVA performs highly competitively on multiple
video-based text generation benchmarks including
[MVbench](https://paperswithcode.com/sota/video-question-answering-on-mvbench?p=pllava-parameter-free-llava-extension-from-1).

| Model      | Link                                                     |
| ------------ |----------------------------------------------------------|
| pllava-7b  | [pllava-7b · Hugging Face](https://huggingface.co/ermu2001/pllava-7b)   |
| pllava-13b | [pllava-13b · Hugging Face](https://huggingface.co/ermu2001/pllava-13b) |
| pllava-34b | [pllava-34b · Hugging Face](https://huggingface.co/ermu2001/pllava-34b) |

And put it under `./pretrained_models/pllava`. You can
also use a customized directory by using the option
`--pretrained_model_name_or_path` when running the script.

Currently, we only support captioning on Ascend.

```bash
export PYTHONPATH=$(pwd)
msrun --worker_num=2 --local_worker_num=2 --join=True \
 --log_dir=msrun_log pipeline/captioning/caption_pllava.py \
 /path/to/meta.csv
```
Modify `worker_num` and `local_worker_num` based on your resource.

We support the following arguments for PLLaVA captioning:

- `pretrained_model_name_or_path`: the PLLaVA model directory.
- `num_frames`: the number of frames to extract from the videos. Default 4.
- `question`: The prompt used to generate captions. Default "Describe the video in detail".
- `max_new_tokens`: Maximum new tokens generated by PLLaVA, default 200.
- `bs`: Batch size. Default 1.
- `skip_if_existing`: Skip processing if output already exists. Default False.

## Qwen2-VL Captioning

We recommend using Qwen2-VL captioning by default. Empirically, we observe that Qwen2-VL
consistently provide high-quality captions for videos.
Qwen2-VL-72B model showcases top-tier performance across
most metrics, often surpassing even closed-source
models like GPT-4o and Claude-3.5-Sonnet.

First, you may download the model on HuggingFace and put it under `./pretrained_models/Qwen2-VL-7B-Instruct`.:

| Model        | Link                                                                         |
|--------------|------------------------------------------------------------------------------|
| Qwen2-VL-2B-Instruct  | [Qwen/Qwen2-VL-2B · Hugging Face](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)   |
| Qwen2-VL-7B-Instruct  | [Qwen/Qwen2-VL-7B · Hugging Face](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)   |
| Qwen2-VL-72B-Instruct | [Qwen/Qwen2-VL-72B · Hugging Face](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct) |


Currently, we only support captioning on Ascend.

```bash
export PYTHONPATH=$(pwd)
msrun --worker_num=2 --local_worker_num=2 --join=True \
 --log_dir=msrun_log pipeline/captioning/caption_qwen2vl.py \
 /path/to/meta.csv
```
Modify `worker_num` and `local_worker_num` based on your resource.

We support the following arguments for Qwen2-VL captioning:

- `pretrained_model_name_or_path`: the Qwen2-VL model directory.
- `question`: The prompt used to generate captions. Default "Describe the video in detail".
- `max_new_tokens`: Maximum new tokens generated by Qwen2-VL, default 200.
- `height`: Resized video height. Default 448.
- `width`: Resized video width. Default 672.
- `fps`: Frame rate of the resized video. Default 4.
- `bs`: Batch size. Default 1.
- `skip_if_existing`: Skip processing if output already exists. Default False.

**NOTE:** When running large-scale parallel inference,
the default `HCCL_CONNECT_TIMEOUT` setting might be
insufficient, potentially causing runtime errors with
AllGather operations. To avoid this issue, consider
setting `export HCCL_CONNECT_TIMEOUT=7200` (corresponds to
7200 seconds) or adjusting it according to your
specific needs.

Additionally, an empty caption will be generated if the
input exceeds the memory usage limit. You may try to
reduce the height, width, or fps to avoid this issue.
