
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindone/0.2/diffusers/api/loaders/lora/">
      
      
        <link rel="prev" href="../ip_adapter/">
      
      
        <link rel="next" href="../peft/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>LoRA - MindOne - One for All</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Source Sans Pro";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lora" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindOne - One for All" class="md-header__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOne - One for All
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LoRA
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../" class="md-tabs__link">
          
  
  Diffusers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../transformers/" class="md-tabs__link">
          
  
  Transformers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../peft/" class="md-tabs__link">
          
  
  PEFT

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindOne - One for All" class="md-nav__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOne - One for All
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Diffusers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Get started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ðŸ§¨ Diffusers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../quicktour/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quicktour
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../stable_diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Effective and efficient diffusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../limitations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Limitations
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/tutorial_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/write_own_pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Understanding pipelines, models and schedulers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/basic_training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Train a diffusion model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/using_peft_for_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load LoRAs for inference
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_3_1" id="__nav_2_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Loaders
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3_1">
            <span class="md-nav__icon md-icon"></span>
            Loaders
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ip_adapter/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IP-Adapter
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LoRA
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.disable_lora_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      disable_lora_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.enable_lora_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.set_adapters_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapters_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../single_file/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Single file
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../textual_inversion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Textual Inversion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNet
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_2" >
        
          
          <label class="md-nav__link" for="__nav_2_3_2" id="__nav_2_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_2">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNet1DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNet2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d-cond/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNet2DConditionModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet3d-cond/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNet3DConditionModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet-motion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UNetMotionModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/uvit2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UViT2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UVQModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AutoEncoderKL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/asymmetricautoencoderkl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AsymmetricAutoEncoderKL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_tiny/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tiny AutoEncoder
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consistency_decoder_vae/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ConsistencyDecoderVae
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pixart_transformer2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PixArtTransformer2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/dit_transformer2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DiTTransformer2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_transformer2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HunyuanDiT2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer_temporal/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TransformerTemporalModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sd3_transformer2d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SD3Transformer2DModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/prior_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PriorTransformer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ControlNetModel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sd3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SD3ControlNetModel
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3_3" id="__nav_2_3_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Pipelines
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_3">
            <span class="md-nav__icon md-icon"></span>
            Pipelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/animatediff/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AnimateDiff
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/blip_diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    BLIP-Diffusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/consistency_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Consistency Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ControlNet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sd3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion 3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ControlNet-XS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs_sdxl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ControlNet-XS with Stable Diffusion XL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dance_diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dance Diffusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDIM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddpm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/deepfloyd_if/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DeepFloyd IF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/diffedit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DiffEdit
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DiT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/hunyuandit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hunyuan-DiT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/i2vgenxl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    I2VGen-XL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pix2pix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    InstructPix2Pix
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kandinsky 2.1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky_v22/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kandinsky 2.2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kandinsky 3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_consistency_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent Consistency Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_diffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Latent Diffusion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/marigold/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Marigold
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PixArt-Î±
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart_sigma/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PixArt-Î£
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/shap_e/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shap-E
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_cascade/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stable Cascade
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/unclip/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    unCLIP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/wuerstchen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Wuerstchen
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3_4" >
        
          
          <label class="md-nav__link" for="__nav_2_3_4" id="__nav_2_3_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Internal Class
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_4">
            <span class="md-nav__icon md-icon"></span>
            Internal Class
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../internal_classes_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attnprocessor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Attention Processor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../activations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom activation functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../normalization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Custom Normalization Layer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utilities/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utilities
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image_processor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VAE Image Processor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../video_processor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Video Processor
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Transformers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Get started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ðŸ¤— Transformers
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PEFT
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Get started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../peft/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ðŸ¤— PEFT
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.disable_lora_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      disable_lora_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.enable_lora_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.set_adapters_for_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapters_for_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/mindspore-lab/mindone/edit/master/docs/diffusers/api/loaders/lora.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindone/raw/master/docs/diffusers/api/loaders/lora.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="lora">LoRA<a class="headerlink" href="#lora" title="Permanent link">&para;</a></h1>
<p>LoRA is a fast and lightweight training method that inserts and trains a significantly smaller number of parameters instead of all the model parameters. This produces a smaller file (~100 MBs) and makes it easier to quickly train a model to learn a new concept. LoRA weights are typically loaded into the UNet, text encoder or both. There are two classes for loading LoRA weights:</p>
<ul>
<li><code>LoraLoaderMixin</code> provides functions for loading and unloading, fusing and unfusing, enabling and disabling, and more functions for managing LoRA weights. This class can be used with any model.</li>
<li><code>StableDiffusionXLLoraLoaderMixin</code> is a <a href="../../api/pipelines/stable_diffusion/stable_diffusion_xl.md">Stable Diffusion (SDXL)</a> version of the <code>LoraLoaderMixin</code> class for loading and saving LoRA weights. It can only be used with the SDXL model.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To learn more about how to load LoRA weights, see the <a href="../../using-diffusers/loading_adapters.md#lora">LoRA</a> loading guide.</p>
</div>


<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora.LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora.LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Load LoRA layers into [<code>UNet2DConditionModel</code>] and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>.</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  87</span>
<span class="normal">  88</span>
<span class="normal">  89</span>
<span class="normal">  90</span>
<span class="normal">  91</span>
<span class="normal">  92</span>
<span class="normal">  93</span>
<span class="normal">  94</span>
<span class="normal">  95</span>
<span class="normal">  96</span>
<span class="normal">  97</span>
<span class="normal">  98</span>
<span class="normal">  99</span>
<span class="normal"> 100</span>
<span class="normal"> 101</span>
<span class="normal"> 102</span>
<span class="normal"> 103</span>
<span class="normal"> 104</span>
<span class="normal"> 105</span>
<span class="normal"> 106</span>
<span class="normal"> 107</span>
<span class="normal"> 108</span>
<span class="normal"> 109</span>
<span class="normal"> 110</span>
<span class="normal"> 111</span>
<span class="normal"> 112</span>
<span class="normal"> 113</span>
<span class="normal"> 114</span>
<span class="normal"> 115</span>
<span class="normal"> 116</span>
<span class="normal"> 117</span>
<span class="normal"> 118</span>
<span class="normal"> 119</span>
<span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LoraLoaderMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`UNet2DConditionModel`] and</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>
    <span class="n">unet_name</span> <span class="o">=</span> <span class="n">UNET_NAME</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>
    <span class="n">num_fused_loras</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into</span>
<span class="sd">        `self.unet`.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded</span>
<span class="sd">        into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span> <span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore statedict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>
<span class="sd">            resume_download:</span>
<span class="sd">                Deprecated and ignored. All downloads are now resumed by default when possible. Will be removed in v1</span>
<span class="sd">                of Diffusers.</span>
<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">                Name of the serialized state dict file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># UNet and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
            <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">model_file</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># Let&#39;s first try to load .safetensors weights</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
                <span class="n">weight_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Here we&#39;re relaxing the loading check to enable more Inference API</span>
                    <span class="c1"># friendliness where sometimes, it&#39;s not at all possible to automatically</span>
                    <span class="c1"># determine `weight_name`.</span>
                    <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">weight_name</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_best_guess_weight_name</span><span class="p">(</span>
                            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                            <span class="n">file_extension</span><span class="o">=</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span>
                            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                        <span class="n">weights_name</span><span class="o">=</span><span class="n">weight_name</span> <span class="ow">or</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span><span class="p">,</span>
                        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                        <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
                <span class="k">except</span> <span class="p">(</span><span class="ne">IOError</span><span class="p">,</span> <span class="n">safetensors</span><span class="o">.</span><span class="n">SafetensorError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_pickle</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">e</span>
                    <span class="c1"># try loading non-safetensors weights</span>
                    <span class="n">model_file</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">pass</span>

            <span class="k">if</span> <span class="n">model_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">weight_name</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_best_guess_weight_name</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">file_extension</span><span class="o">=</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span>
                    <span class="p">)</span>
                <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                    <span class="n">weights_name</span><span class="o">=</span><span class="n">weight_name</span> <span class="ow">or</span> <span class="n">LORA_WEIGHT_NAME</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Only supports deserialization of weights file in safetensors format, but got </span><span class="si">{</span><span class="n">model_file</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span>

        <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Map SDXL blocks correctly.</span>
            <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># use unet config to remap block numbers</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_best_guess_weight_name</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">file_extension</span><span class="o">=</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">local_files_only</span> <span class="ow">or</span> <span class="n">HF_HUB_OFFLINE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;When using the offline mode, you must specify a `weight_name`.&quot;</span><span class="p">)</span>

        <span class="n">targeted_files</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">):</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">):</span>
            <span class="n">targeted_files</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">file_extension</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">files_in_repo</span> <span class="o">=</span> <span class="n">model_info</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">)</span><span class="o">.</span><span class="n">siblings</span>
            <span class="n">targeted_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">rfilename</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">files_in_repo</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">rfilename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">file_extension</span><span class="p">)]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">targeted_files</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># &quot;scheduler&quot; does not correspond to a LoRA checkpoint.</span>
        <span class="c1"># &quot;optimizer&quot; does not correspond to a LoRA checkpoint</span>
        <span class="c1"># only top-level checkpoints are considered and not the other ones, hence &quot;checkpoint&quot;.</span>
        <span class="n">unallowed_substrings</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scheduler&quot;</span><span class="p">,</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">}</span>
        <span class="n">targeted_files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">all</span><span class="p">(</span><span class="n">substring</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">for</span> <span class="n">substring</span> <span class="ow">in</span> <span class="n">unallowed_substrings</span><span class="p">),</span> <span class="n">targeted_files</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">LORA_WEIGHT_NAME</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">targeted_files</span><span class="p">):</span>
            <span class="n">targeted_files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">LORA_WEIGHT_NAME</span><span class="p">),</span> <span class="n">targeted_files</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">LORA_WEIGHT_NAME_SAFE</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">targeted_files</span><span class="p">):</span>
            <span class="n">targeted_files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">LORA_WEIGHT_NAME_SAFE</span><span class="p">),</span> <span class="n">targeted_files</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">targeted_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Provided path contains more than one weights file in the </span><span class="si">{</span><span class="n">file_extension</span><span class="si">}</span><span class="s2"> format. Either specify `weight_name` in `load_lora_weights` or make sure there&#39;s only one  `.safetensors` or `.bin` file in  </span><span class="si">{</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="si">}</span><span class="s2">.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">targeted_files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">weight_name</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
            <span class="c1"># Load the layers corresponding to UNet.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">unet</span><span class="o">.</span><span class="n">load_attn_procs</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                See `LoRALinearLayer` for more details.</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mindone.diffusers._peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span>

        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `self.unet_name` and/or `self.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span> <span class="k">if</span> <span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">prefix</span>

        <span class="c1"># Safe prefix to check with.</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">):</span>
            <span class="c1"># Load the layers corresponding to text encoder and make necessary adjustments.</span>
            <span class="n">text_encoder_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span><span class="p">]</span>
            <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">text_encoder_keys</span>
            <span class="p">}</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                <span class="n">rank</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="n">convert_state_dict_to_diffusers</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span>

                <span class="c1"># convert state dict</span>
                <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="n">convert_state_dict_to_peft</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span>

                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">text_encoder_attn_modules</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
                    <span class="n">rank_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.out_proj.lora_B.weight&quot;</span>
                    <span class="n">rank</span><span class="p">[</span><span class="n">rank_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">patch_mlp</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.mlp.&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">text_encoder_lora_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">patch_mlp</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">text_encoder_mlp_modules</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
                        <span class="n">rank_key_fc1</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.fc1.lora_B.weight&quot;</span>
                        <span class="n">rank_key_fc2</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.fc2.lora_B.weight&quot;</span>

                        <span class="n">rank</span><span class="p">[</span><span class="n">rank_key_fc1</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key_fc1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                        <span class="n">rank</span><span class="p">[</span><span class="n">rank_key_fc2</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key_fc2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">network_alphas</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">alpha_keys</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span>
                    <span class="p">]</span>
                    <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">alpha_keys</span>
                    <span class="p">}</span>

                <span class="n">lora_config_kwargs</span> <span class="o">=</span> <span class="n">get_peft_kwargs</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">,</span> <span class="n">is_unet</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;use_dora&quot;</span> <span class="ow">in</span> <span class="n">lora_config_kwargs</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">lora_config_kwargs</span><span class="p">[</span><span class="s2">&quot;use_dora&quot;</span><span class="p">]:</span>
                        <span class="k">if</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="s2">&quot;You need `peft` 0.9.0 at least to use DoRA-enabled LoRAs. Please upgrade your installation of `peft`.&quot;</span>
                            <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                            <span class="n">lora_config_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_dora&quot;</span><span class="p">)</span>
                <span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">**</span><span class="n">lora_config_kwargs</span><span class="p">)</span>

                <span class="c1"># adapter_name</span>
                <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">get_adapter_name</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">)</span>

                <span class="c1"># inject LoRA layers and load the state dict</span>
                <span class="c1"># in transformers we automatically check whether the adapter name is already in use or not</span>
                <span class="n">text_encoder</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span>
                    <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                    <span class="n">adapter_state_dict</span><span class="o">=</span><span class="n">text_encoder_lora_state_dict</span><span class="p">,</span>
                    <span class="n">peft_config</span><span class="o">=</span><span class="n">lora_config</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># scale LoRA layers with `lora_scale`</span>
                <span class="n">scale_lora_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">)</span>

                <span class="n">text_encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_lora_into_transformer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                See `LoRALinearLayer` for more details.</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mindone.diffusers._peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">inject_adapter_in_model</span><span class="p">,</span> <span class="n">set_peft_model_state_dict</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="n">transformer_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)]</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">transformer_keys</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">network_alphas</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">alpha_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)]</span>
            <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">alpha_keys</span>
            <span class="p">}</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="p">{}):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Adapter name </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> already in use in the transformer - please select a new adapter name.&quot;</span>
                <span class="p">)</span>

            <span class="n">rank</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="s2">&quot;lora_B&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                    <span class="n">rank</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">lora_config_kwargs</span> <span class="o">=</span> <span class="n">get_peft_kwargs</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;use_dora&quot;</span> <span class="ow">in</span> <span class="n">lora_config_kwargs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">lora_config_kwargs</span><span class="p">[</span><span class="s2">&quot;use_dora&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;You need `peft` 0.9.0 at least to use DoRA-enabled LoRAs. Please upgrade your installation of `peft`.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">lora_config_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_dora&quot;</span><span class="p">)</span>
            <span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">**</span><span class="n">lora_config_kwargs</span><span class="p">)</span>

            <span class="c1"># adapter_name</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">get_adapter_name</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

            <span class="n">inject_adapter_in_model</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">)</span>
            <span class="n">incompatible_keys</span> <span class="o">=</span> <span class="n">set_peft_model_state_dict</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">incompatible_keys</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># check only for unexpected keys</span>
                <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">incompatible_keys</span><span class="p">,</span> <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">unexpected_keys</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Loading adapter weights from state_dict led to unexpected keys not found in the model: &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lora_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="c1"># property function that returns the lora scale which can be set at run time by the pipeline.</span>
        <span class="c1"># if _lora_scale has not been set, return 1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_scale</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_lora_scale&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">remove_method</span> <span class="o">=</span> <span class="n">recurse_remove_peft_layers</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="n">remove_method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
            <span class="c1"># In case text encoder have no Lora attached</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">peft_config</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="n">remove_method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">peft_config</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
            <span class="n">layers_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">layers</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">()}</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
            <span class="n">layers_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layers_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">layers_state_dict</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">transformer_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers`, or `transformer_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">))</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">save_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">save_function</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">save_file</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">})</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">save_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
        <span class="n">save_function</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unloads the LoRA parameters.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">        &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>

        <span class="c1"># Safe to call the following regardless of LoRA.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_remove_text_encoder_monkey_patch</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">fuse_unet</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">fuse_text_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            fuse_unet (`bool`, defaults to `True`): Whether to fuse the UNet LoRA parameters.</span>
<span class="sd">            fuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to fuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

        <span class="k">if</span> <span class="n">fuse_unet</span> <span class="ow">or</span> <span class="n">fuse_text_encoder</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The current API is supported for operating with a single LoRA file. You are trying to load and fuse more than one LoRA which is not well-supported.&quot;</span><span class="p">,</span>  <span class="c1"># noqa: E501</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">fuse_unet</span><span class="p">:</span>
            <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
            <span class="n">unet</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">fuse_text_encoder_lora</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">merge_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;safe_merge&quot;</span><span class="p">:</span> <span class="n">safe_fusing</span><span class="p">}</span>

            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">lora_scale</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">scale_layer</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">)</span>

                    <span class="c1"># For BC with previous PEFT versions, we need to check the signature</span>
                    <span class="c1"># of the `merge` method to see if it supports the `adapter_names` argument.</span>
                    <span class="n">supported_merge_kwargs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merge</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;adapter_names&quot;</span> <span class="ow">in</span> <span class="n">supported_merge_kwargs</span><span class="p">:</span>
                        <span class="n">merge_kwargs</span><span class="p">[</span><span class="s2">&quot;adapter_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">adapter_names</span>
                    <span class="k">elif</span> <span class="s2">&quot;adapter_names&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_merge_kwargs</span> <span class="ow">and</span> <span class="n">adapter_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The &#39;adapter_names&#39; argument is not supported with &#39;BaseTunerLayer.merge&#39;&quot;</span><span class="p">)</span>

                    <span class="n">module</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="o">**</span><span class="n">merge_kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">fuse_text_encoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
                <span class="n">fuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
                <span class="n">fuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unfuse_unet</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">unfuse_text_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="k">if</span> <span class="n">unfuse_unet</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">unfuse_text_encoder_lora</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">unfuse_text_encoder</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
                <span class="n">unfuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
                <span class="n">unfuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">-=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">set_adapters_for_text_encoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F821</span>
        <span class="n">text_encoder_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the adapter layers for the text encoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_names (`List[str]` or `str`):</span>
<span class="sd">                The names of the adapters to use.</span>
<span class="sd">            text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">                The text encoder module to set the adapter layers for. If `None`, it will try to get the `text_encoder`</span>
<span class="sd">                attribute.</span>
<span class="sd">            text_encoder_weights (`List[float]`, *optional*):</span>
<span class="sd">                The weights to use for the text encoder. If `None`, the weights are set to `1.0` for all the adapters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">process_weights</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
            <span class="c1"># Expand weights into a list, one entry per adapter</span>
            <span class="c1"># e.g. for 2 adapters:  7 -&gt; [7,7] ; [3, None] -&gt; [3, None]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Set None values to default of 1.0</span>
            <span class="c1"># e.g. [7,7] -&gt; [7,7] ; [3, None] -&gt; [3,1]</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">]</span>

            <span class="k">return</span> <span class="n">weights</span>

        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>
        <span class="n">text_encoder_weights</span> <span class="o">=</span> <span class="n">process_weights</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">text_encoder_weights</span><span class="p">)</span>
        <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The pipeline does not have a default `pipe.text_encoder` class. Please make sure to pass a `text_encoder` instead.&quot;</span>
            <span class="p">)</span>
        <span class="n">set_weights_and_activate_adapters</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">,</span> <span class="n">text_encoder_weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disables the LoRA layers for the text encoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">                The text encoder module to disable the LoRA layers for. If `None`, it will try to get the</span>
<span class="sd">                `text_encoder` attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Text Encoder not found.&quot;</span><span class="p">)</span>
        <span class="n">set_adapter_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">enable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enables the LoRA layers for the text encoder.</span>

<span class="sd">        Args:</span>
<span class="sd">            text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">                The text encoder module to enable the LoRA layers for. If `None`, it will try to get the `text_encoder`</span>
<span class="sd">                attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Text Encoder not found.&quot;</span><span class="p">)</span>
        <span class="n">set_adapter_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_adapters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">adapter_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>

        <span class="n">adapter_weights</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span>

        <span class="c1"># Expand weights into a list, one entry per adapter</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Decompose weights into weights for unet, text_encoder and text_encoder_2</span>
        <span class="n">unet_lora_weights</span><span class="p">,</span> <span class="n">text_encoder_lora_weights</span><span class="p">,</span> <span class="n">text_encoder_2_lora_weights</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="n">list_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span>  <span class="c1"># eg {&quot;unet&quot;: [&quot;adapter1&quot;, &quot;adapter2&quot;], &quot;text_encoder&quot;: [&quot;adapter2&quot;]}</span>
        <span class="n">all_adapters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">adapter</span> <span class="k">for</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span>
        <span class="p">}</span>  <span class="c1"># eg [&quot;adapter1&quot;, &quot;adapter2&quot;]</span>
        <span class="n">invert_list_adapters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">adapter</span><span class="p">:</span> <span class="p">[</span><span class="n">part</span> <span class="k">for</span> <span class="n">part</span><span class="p">,</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">all_adapters</span>
        <span class="p">}</span>  <span class="c1"># eg {&quot;adapter1&quot;: [&quot;unet&quot;], &quot;adapter2&quot;: [&quot;unet&quot;, &quot;text_encoder&quot;]}</span>

        <span class="k">for</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">adapter_weights</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">unet_lora_weight</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="n">text_encoder_lora_weight</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="n">text_encoder_2_lora_weight</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Got invalid key &#39;</span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&#39; in lora weight dict for adapter </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">text_encoder_2_lora_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Lora weight dict contains text_encoder_2 weights but will be ignored because pipeline does not have text_encoder_2.&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># warn if adapter doesn&#39;t have parts specified by adapter_weights</span>
                <span class="k">for</span> <span class="n">part_weight</span><span class="p">,</span> <span class="n">part_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">unet_lora_weight</span><span class="p">,</span> <span class="n">text_encoder_lora_weight</span><span class="p">,</span> <span class="n">text_encoder_2_lora_weight</span><span class="p">],</span>
                    <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
                <span class="p">):</span>
                    <span class="k">if</span> <span class="n">part_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">part_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Lora weight dict for adapter &#39;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2">&#39; contains </span><span class="si">{</span><span class="n">part_name</span><span class="si">}</span><span class="s2">, but this will be ignored because </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not contain weights for </span><span class="si">{</span><span class="n">part_name</span><span class="si">}</span><span class="s2">. Valid parts for </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>  <span class="c1"># noqa E501</span>
                        <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">unet_lora_weight</span> <span class="o">=</span> <span class="n">weights</span>
                <span class="n">text_encoder_lora_weight</span> <span class="o">=</span> <span class="n">weights</span>
                <span class="n">text_encoder_2_lora_weight</span> <span class="o">=</span> <span class="n">weights</span>

            <span class="n">unet_lora_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">unet_lora_weight</span><span class="p">)</span>
            <span class="n">text_encoder_lora_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text_encoder_lora_weight</span><span class="p">)</span>
            <span class="n">text_encoder_2_lora_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text_encoder_2_lora_weight</span><span class="p">)</span>

        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="c1"># Handle the UNET</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">unet_lora_weights</span><span class="p">)</span>

        <span class="c1"># Handle the Text Encoder</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">text_encoder_lora_weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">text_encoder_2_lora_weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">disable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Disable unet adapters</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span>

        <span class="c1"># Disable text encoder adapters</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">enable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Enable unet adapters</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span>

        <span class="c1"># Enable text encoder adapters</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">        Deletes the LoRA layers of `adapter_name` for the unet and text-encoder(s).</span>
<span class="sd">            adapter_names (`Union[List[str], str]`):</span>
<span class="sd">                The names of the adapter to delete. Can be a single string or a list of strings</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

        <span class="c1"># Delete unet adapters</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
            <span class="c1"># Delete text encoder adapters</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
                <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
                <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the list of the current active adapters.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">        pipeline.get_active_adapters()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

        <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">active_adapters</span>

    <span class="k">def</span> <span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
            <span class="n">set_adapters</span><span class="p">[</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
            <span class="n">set_adapters</span><span class="p">[</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">unet</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
            <span class="n">set_adapters</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">set_adapters</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.delete_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.delete_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Deletes the LoRA layers of <code>adapter_name</code> for the unet and text-encoder(s).
    adapter_names (<code>Union[List[str], str]</code>):
        The names of the adapter to delete. Can be a single string or a list of strings</p>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">    Deletes the LoRA layers of `adapter_name` for the unet and text-encoder(s).</span>
<span class="sd">        adapter_names (`Union[List[str], str]`):</span>
<span class="sd">            The names of the adapter to delete. Can be a single string or a list of strings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

    <span class="c1"># Delete unet adapters</span>
    <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
    <span class="n">unet</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
        <span class="c1"># Delete text encoder adapters</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.disable_lora_for_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="n">text_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.disable_lora_for_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Disables the LoRA layers for the text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder module to disable the LoRA layers for. If <code>None</code>, it will try to get the
<code>text_encoder</code> attribute.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.nn.Cell`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">disable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disables the LoRA layers for the text encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">            The text encoder module to disable the LoRA layers for. If `None`, it will try to get the</span>
<span class="sd">            `text_encoder` attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Text Encoder not found.&quot;</span><span class="p">)</span>
    <span class="n">set_adapter_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.enable_lora_for_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="n">text_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.enable_lora_for_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Enables the LoRA layers for the text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder module to enable the LoRA layers for. If <code>None</code>, it will try to get the <code>text_encoder</code>
attribute.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.nn.Cell`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">enable_lora_for_text_encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enables the LoRA layers for the text encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">            The text encoder module to enable the LoRA layers for. If `None`, it will try to get the `text_encoder`</span>
<span class="sd">            attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Text Encoder not found.&quot;</span><span class="p">)</span>
    <span class="n">set_adapter_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">fuse_unet</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fuse_text_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>fuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to fuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>fuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to fuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_fusing</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">mindone.diffusers</span> <span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">fuse_unet</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">fuse_text_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        fuse_unet (`bool`, defaults to `True`): Whether to fuse the UNet LoRA parameters.</span>
<span class="sd">        fuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to fuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

    <span class="k">if</span> <span class="n">fuse_unet</span> <span class="ow">or</span> <span class="n">fuse_text_encoder</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The current API is supported for operating with a single LoRA file. You are trying to load and fuse more than one LoRA which is not well-supported.&quot;</span><span class="p">,</span>  <span class="c1"># noqa: E501</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">fuse_unet</span><span class="p">:</span>
        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fuse_text_encoder_lora</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">merge_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;safe_merge&quot;</span><span class="p">:</span> <span class="n">safe_fusing</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">lora_scale</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">scale_layer</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">)</span>

                <span class="c1"># For BC with previous PEFT versions, we need to check the signature</span>
                <span class="c1"># of the `merge` method to see if it supports the `adapter_names` argument.</span>
                <span class="n">supported_merge_kwargs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merge</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;adapter_names&quot;</span> <span class="ow">in</span> <span class="n">supported_merge_kwargs</span><span class="p">:</span>
                    <span class="n">merge_kwargs</span><span class="p">[</span><span class="s2">&quot;adapter_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">adapter_names</span>
                <span class="k">elif</span> <span class="s2">&quot;adapter_names&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">supported_merge_kwargs</span> <span class="ow">and</span> <span class="n">adapter_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The &#39;adapter_names&#39; argument is not supported with &#39;BaseTunerLayer.merge&#39;&quot;</span><span class="p">)</span>

                <span class="n">module</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="o">**</span><span class="n">merge_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fuse_text_encoder</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="n">fuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="n">fuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.get_active_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_active_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the list of the current active adapters.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">mindone.diffusers</span> <span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;CiroN2022/toy-face&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;toy_face_sdxl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;toy&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the list of the current active adapters.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">    pipeline.get_active_adapters()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

    <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
            <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">active_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.get_list_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.get_list_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the current list of all available adapters in the pipeline.</p>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
        <span class="n">set_adapters</span><span class="p">[</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
        <span class="n">set_adapters</span><span class="p">[</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">unet</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">):</span>
        <span class="n">set_adapters</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">set_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See <code>LoRALinearLayer</code> for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prefix</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            See `LoRALinearLayer` for more details.</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mindone.diffusers._peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span>

    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `self.unet_name` and/or `self.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span> <span class="k">if</span> <span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">prefix</span>

    <span class="c1"># Safe prefix to check with.</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">):</span>
        <span class="c1"># Load the layers corresponding to text encoder and make necessary adjustments.</span>
        <span class="n">text_encoder_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span><span class="p">]</span>
        <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">text_encoder_keys</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="n">convert_state_dict_to_diffusers</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span>

            <span class="c1"># convert state dict</span>
            <span class="n">text_encoder_lora_state_dict</span> <span class="o">=</span> <span class="n">convert_state_dict_to_peft</span><span class="p">(</span><span class="n">text_encoder_lora_state_dict</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">text_encoder_attn_modules</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
                <span class="n">rank_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.out_proj.lora_B.weight&quot;</span>
                <span class="n">rank</span><span class="p">[</span><span class="n">rank_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">patch_mlp</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.mlp.&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">text_encoder_lora_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">patch_mlp</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">text_encoder_mlp_modules</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
                    <span class="n">rank_key_fc1</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.fc1.lora_B.weight&quot;</span>
                    <span class="n">rank_key_fc2</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.fc2.lora_B.weight&quot;</span>

                    <span class="n">rank</span><span class="p">[</span><span class="n">rank_key_fc1</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key_fc1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">rank</span><span class="p">[</span><span class="n">rank_key_fc2</span><span class="p">]</span> <span class="o">=</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">[</span><span class="n">rank_key_fc2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">network_alphas</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">alpha_keys</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="ow">and</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span>
                <span class="p">]</span>
                <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">alpha_keys</span>
                <span class="p">}</span>

            <span class="n">lora_config_kwargs</span> <span class="o">=</span> <span class="n">get_peft_kwargs</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder_lora_state_dict</span><span class="p">,</span> <span class="n">is_unet</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;use_dora&quot;</span> <span class="ow">in</span> <span class="n">lora_config_kwargs</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">lora_config_kwargs</span><span class="p">[</span><span class="s2">&quot;use_dora&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;You need `peft` 0.9.0 at least to use DoRA-enabled LoRAs. Please upgrade your installation of `peft`.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                        <span class="n">lora_config_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_dora&quot;</span><span class="p">)</span>
            <span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">**</span><span class="n">lora_config_kwargs</span><span class="p">)</span>

            <span class="c1"># adapter_name</span>
            <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">get_adapter_name</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">)</span>

            <span class="c1"># inject LoRA layers and load the state dict</span>
            <span class="c1"># in transformers we automatically check whether the adapter name is already in use or not</span>
            <span class="n">text_encoder</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">adapter_state_dict</span><span class="o">=</span><span class="n">text_encoder_lora_state_dict</span><span class="p">,</span>
                <span class="n">peft_config</span><span class="o">=</span><span class="n">lora_config</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># scale LoRA layers with `lora_scale`</span>
            <span class="n">scale_lora_layers</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">)</span>

            <span class="n">text_encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See <code>LoRALinearLayer</code> for more details.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load_lora_into_transformer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            See `LoRALinearLayer` for more details.</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mindone.diffusers._peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">inject_adapter_in_model</span><span class="p">,</span> <span class="n">set_peft_model_state_dict</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">transformer_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)]</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">transformer_keys</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="n">network_alphas</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">alpha_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)]</span>
        <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">network_alphas</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">alpha_keys</span>
        <span class="p">}</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="p">{}):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Adapter name </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> already in use in the transformer - please select a new adapter name.&quot;</span>
            <span class="p">)</span>

        <span class="n">rank</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;lora_B&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">:</span>
                <span class="n">rank</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">lora_config_kwargs</span> <span class="o">=</span> <span class="n">get_peft_kwargs</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;use_dora&quot;</span> <span class="ow">in</span> <span class="n">lora_config_kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">lora_config_kwargs</span><span class="p">[</span><span class="s2">&quot;use_dora&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">is_peft_version</span><span class="p">(</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span> <span class="s2">&quot;0.9.0&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You need `peft` 0.9.0 at least to use DoRA-enabled LoRAs. Please upgrade your installation of `peft`.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lora_config_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_dora&quot;</span><span class="p">)</span>
        <span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span><span class="o">**</span><span class="n">lora_config_kwargs</span><span class="p">)</span>

        <span class="c1"># adapter_name</span>
        <span class="k">if</span> <span class="n">adapter_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">adapter_name</span> <span class="o">=</span> <span class="n">get_adapter_name</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>

        <span class="n">inject_adapter_in_model</span><span class="p">(</span><span class="n">lora_config</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">)</span>
        <span class="n">incompatible_keys</span> <span class="o">=</span> <span class="n">set_peft_model_state_dict</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">incompatible_keys</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># check only for unexpected keys</span>
            <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">incompatible_keys</span><span class="p">,</span> <span class="s2">&quot;unexpected_keys&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">unexpected_keys</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Loading adapter weights from state_dict led to unexpected keys not found in the model: &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_unet" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_into_unet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>unet</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
        <span class="c1"># Load the layers corresponding to UNet.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">load_attn_procs</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.</p>
<p>See [<code>~loaders.LoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is loaded into
<code>self.unet</code>.</p>
<p>See [<code>~loaders.LoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state dict is loaded
into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kwargs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into</span>
<span class="sd">    `self.unet`.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded</span>
<span class="sd">    into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore statedict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>cache_dir</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>force_download</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>resume_download</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Deprecated and ignored. All downloads are now resumed by default when possible. Will be removed in v1
of Diffusers.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>proxies</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>local_files_only</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>token</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>revision</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>subfolder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>weight_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the serialized state dict file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span> <span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore statedict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>
<span class="sd">        resume_download:</span>
<span class="sd">            Deprecated and ignored. All downloads are now resumed by default when possible. Will be removed in v1</span>
<span class="sd">            of Diffusers.</span>
<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">            Name of the serialized state dict file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># UNet and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
        <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">model_file</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="c1"># Let&#39;s first try to load .safetensors weights</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">use_safetensors</span> <span class="ow">and</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">weight_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Here we&#39;re relaxing the loading check to enable more Inference API</span>
                <span class="c1"># friendliness where sometimes, it&#39;s not at all possible to automatically</span>
                <span class="c1"># determine `weight_name`.</span>
                <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">weight_name</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_best_guess_weight_name</span><span class="p">(</span>
                        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                        <span class="n">file_extension</span><span class="o">=</span><span class="s2">&quot;.safetensors&quot;</span><span class="p">,</span>
                        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                    <span class="n">weights_name</span><span class="o">=</span><span class="n">weight_name</span> <span class="ow">or</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">load_file</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">IOError</span><span class="p">,</span> <span class="n">safetensors</span><span class="o">.</span><span class="n">SafetensorError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_pickle</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="n">e</span>
                <span class="c1"># try loading non-safetensors weights</span>
                <span class="n">model_file</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">pass</span>

        <span class="k">if</span> <span class="n">model_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_best_guess_weight_name</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">file_extension</span><span class="o">=</span><span class="s2">&quot;.bin&quot;</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span>
                <span class="p">)</span>
            <span class="n">model_file</span> <span class="o">=</span> <span class="n">_get_model_file</span><span class="p">(</span>
                <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
                <span class="n">weights_name</span><span class="o">=</span><span class="n">weight_name</span> <span class="ow">or</span> <span class="n">LORA_WEIGHT_NAME</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only supports deserialization of weights file in safetensors format, but got </span><span class="si">{</span><span class="n">model_file</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span>

    <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="c1"># Map SDXL blocks correctly.</span>
        <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use unet config to remap block numbers</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>save_directory</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>is_main_process</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>save_function</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_serialization</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional PyTorch way with <code>pickle</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">layers_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">layers</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">()}</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
        <span class="n">layers_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layers_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">return</span> <span class="n">layers_state_dict</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">transformer_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers`, or `transformer_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">))</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.set_adapters_for_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">text_encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.set_adapters_for_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sets the adapter layers for the text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The names of the adapters to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]` or `str`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder module to set the adapter layers for. If <code>None</code>, it will try to get the <code>text_encoder</code>
attribute.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.nn.Cell`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_weights</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The weights to use for the text encoder. If <code>None</code>, the weights are set to <code>1.0</code> for all the adapters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[float]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_adapters_for_text_encoder</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">text_encoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;MSPreTrainedModel&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F821</span>
    <span class="n">text_encoder_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the adapter layers for the text encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        adapter_names (`List[str]` or `str`):</span>
<span class="sd">            The names of the adapters to use.</span>
<span class="sd">        text_encoder (`mindspore.nn.Cell`, *optional*):</span>
<span class="sd">            The text encoder module to set the adapter layers for. If `None`, it will try to get the `text_encoder`</span>
<span class="sd">            attribute.</span>
<span class="sd">        text_encoder_weights (`List[float]`, *optional*):</span>
<span class="sd">            The weights to use for the text encoder. If `None`, the weights are set to `1.0` for all the adapters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">process_weights</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="c1"># Expand weights into a list, one entry per adapter</span>
        <span class="c1"># e.g. for 2 adapters:  7 -&gt; [7,7] ; [3, None] -&gt; [3, None]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Set None values to default of 1.0</span>
        <span class="c1"># e.g. [7,7] -&gt; [7,7] ; [3, None] -&gt; [3,1]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">weights</span>

    <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>
    <span class="n">text_encoder_weights</span> <span class="o">=</span> <span class="n">process_weights</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">text_encoder_weights</span><span class="p">)</span>
    <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">text_encoder</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">text_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The pipeline does not have a default `pipe.text_encoder` class. Please make sure to pass a `text_encoder` instead.&quot;</span>
        <span class="p">)</span>
    <span class="n">set_weights_and_activate_adapters</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">,</span> <span class="n">text_encoder_weights</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">unfuse_unet</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unfuse_text_encoder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>unfuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unfuse_unet</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">unfuse_text_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">mindone.diffusers._peft.tuners.tuners_utils</span> <span class="kn">import</span> <span class="n">BaseTunerLayer</span>

    <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
    <span class="k">if</span> <span class="n">unfuse_unet</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">unet</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unfuse_text_encoder_lora</span><span class="p">(</span><span class="n">text_encoder</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">unfuse_text_encoder</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">):</span>
            <span class="n">unfuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">):</span>
            <span class="n">unfuse_text_encoder_lora</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">-=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.LoraLoaderMixin.unload_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">LoraLoaderMixin</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora.LoraLoaderMixin.unload_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Unloads the LoRA parameters.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="c1"># Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="o">...</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unloads the LoRA parameters.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">    &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">    &gt;&gt;&gt; ...</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span>
    <span class="n">unet</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>

    <span class="c1"># Safe to call the following regardless of LoRA.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_remove_text_encoder_monkey_patch</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora.LoraLoaderMixin" href="#mindone.diffusers.loaders.lora.LoraLoaderMixin">LoraLoaderMixin</a></code></p>


        <p>This class overrides <code>LoraLoaderMixin</code> with LoRA loading/saving code that's specific to SDXL</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">StableDiffusionXLLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraLoaderMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This class overrides `LoraLoaderMixin` with LoRA loading/saving code that&#39;s specific to SDXL&quot;&quot;&quot;</span>

    <span class="c1"># Override to properly handle the loading and unloading of the additional text encoder.</span>
    <span class="k">def</span> <span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into</span>
<span class="sd">        `self.unet`.</span>

<span class="sd">        See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded</span>
<span class="sd">        into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
        <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
        <span class="c1"># pipeline.</span>

        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span>
        <span class="p">)</span>
        <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            text_encoder_2_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
            <span class="n">layers_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">layers</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">()}</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
            <span class="n">layers_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layers_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">layers_state_dict</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers` or `text_encoder_2_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">recurse_remove_peft_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">)</span>
        <span class="c1"># TODO: @younesbelkada handle this in transformers side</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">peft_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">recurse_remove_peft_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">peft_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">_hf_peft_config_loaded</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.</p>
<p>See [<code>~loaders.LoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is loaded into
<code>self.unet</code>.</p>
<p>See [<code>~loaders.LoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state dict is loaded
into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kwargs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.LoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into</span>
<span class="sd">    `self.unet`.</span>

<span class="sd">    See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded</span>
<span class="sd">    into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.LoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
    <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
    <span class="c1"># pipeline.</span>

    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span>
    <span class="p">)</span>
    <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>save_directory</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_2_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder_2</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>is_main_process</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>save_function</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_serialization</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional PyTorch way with <code>pickle</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        text_encoder_2_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">layers_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">layers</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">()}</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
        <span class="n">layers_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layers_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">return</span> <span class="n">layers_state_dict</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers` or `text_encoder_2_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 1, 2024</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 1, 2024</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:77485245+wcrzlh@users.noreply.github.com">Chaoran Wei</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../ip_adapter/" class="md-footer__link md-footer__link--prev" aria-label="Previous: IP-Adapter">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                IP-Adapter
              </div>
            </div>
          </a>
        
        
          
          <a href="../peft/" class="md-footer__link md-footer__link--next" aria-label="Next: PEFT">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                PEFT
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 MindSpore Lab
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindone" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>