
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindone/0.3/diffusers/api/pipelines/audioldm2/">
      
      
        <link rel="prev" href="../audioldm/">
      
      
        <link rel="next" href="../aura_flow/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>AudioLDM 2 - MindOne - One for All</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Source Sans Pro";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#audioldm-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindOne - One for All" class="md-header__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOne - One for All
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              AudioLDM 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../" class="md-tabs__link">
          
  
  
  Diffusers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../transformers/" class="md-tabs__link">
          
  
  
  Transformers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../peft/" class="md-tabs__link">
          
  
  
  PEFT

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindOne - One for All" class="md-nav__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOne - One for All
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Diffusers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ§¨ Diffusers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../quicktour/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quicktour
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Effective and efficient diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../limitations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Limitations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/tutorial_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/write_own_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding pipelines, models and schedulers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/autopipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/basic_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Train a diffusion model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/using_peft_for_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load LoRAs for inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Load pipelines and adapters
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Load pipelines and adapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/schedulers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load schedulers and models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/other-formats/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model files and layouts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading_adapters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load adapters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/push_to_hub/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Push files to the Hub
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Generative tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Generative tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/unconditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/conditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/text-img2vid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Inference techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Inference techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/overview_techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/merge_loras/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Merge LoRAs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/scheduler_features/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler features
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/callback/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pipeline callbacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/reusing_seeds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reproducible pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Advanced inference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Advanced inference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_inference/outpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outpainting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Specific pipeline examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            Specific pipeline examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/t2i_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/textual_inversion_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/shap-e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_tcd_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trajectory Consistency Distillation-LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Video Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/marigold_usage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/create_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create a dataset for training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/adapt_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adapt a model to a new task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_4" >
        
          
          <label class="md-nav__link" for="__nav_2_8_4" id="__nav_2_8_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_4">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/unconditional_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text2image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_5" >
        
          
          <label class="md-nav__link" for="__nav_2_8_5" id="__nav_2_8_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_5">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/dreambooth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DreamBooth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
        
          
          <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Accelerate inference and reduce memory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_9">
            <span class="md-nav__icon md-icon"></span>
            Accelerate inference and reduce memory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/fp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speed up inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reduce memory usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/xformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    xFormers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
        
          
          <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Conceptual Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10">
            <span class="md-nav__icon md-icon"></span>
            Conceptual Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../conceptual/philosophy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Philosophy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlling_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Controlled generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_1" >
        
          
          <label class="md-nav__link" for="__nav_2_11_1" id="__nav_2_11_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Main Classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_1">
            <span class="md-nav__icon md-icon"></span>
            Main Classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../outputs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outputs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" >
        
          
          <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Loaders
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_2">
            <span class="md-nav__icon md-icon"></span>
            Loaders
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/single_file/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single files
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/textual_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/transformer_sd3.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2D
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../loaders/peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/auto_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_3" id="__nav_2_11_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ControlNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_3">
            <span class="md-nav__icon md-icon"></span>
            ControlNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sparsectrl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SparseControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnionModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_4" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_4" id="__nav_2_11_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_4">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/allegro_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AllegroTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/aura_flow_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlowTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogvideox_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consisid_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisIDTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview3plus_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3PlusTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview4_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/dit_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiTTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/easyanimate_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimateTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/flux_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_video_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/latte_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LatteTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina_nextdit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LuminaNextDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina2_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina2Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ltx_video_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mochi_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MochiTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/omnigen_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGenTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pixart_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArtTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/prior_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PriorTransformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sd3_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sana_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SanaTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_audio_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableAudioDiTModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer_temporal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TransformerTemporalModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wan_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WanTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_5" id="__nav_2_11_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    UNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_5">
            <span class="md-nav__icon md-icon"></span>
            UNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_cascade_unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableCascadeUNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet1DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet3d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet3DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet-motion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNetMotionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/uvit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UViT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_6" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_6" id="__nav_2_11_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VAEs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_6">
            <span class="md-nav__icon md-icon"></span>
            VAEs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLAllegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLCogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_kl_hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLHunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLLTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_magvit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMagvit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_kl_wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLWan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/asymmetricautoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AsymmetricAutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_dc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderDC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consistency_decoder_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderVAE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_oobleck/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Oobleck AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_tiny/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tiny AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pipelines
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11_4">
            <span class="md-nav__icon md-icon"></span>
            Pipelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Allegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../amused/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    aMUSEd
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../animatediff/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AnimateDiff
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../attend_and_excite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attend-and-Excite
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audioldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    AudioLDM 2
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    AudioLDM 2
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tips" class="md-nav__link">
    <span class="md-ellipsis">
      Tips
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tips">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#choosing-a-checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing a checkpoint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constructing-a-prompt" class="md-nav__link">
    <span class="md-ellipsis">
      Constructing a prompt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#controlling-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Controlling inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluating-generated-waveforms" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluating generated waveforms:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AudioLDM2Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.disable_vae_slicing" class="md-nav__link">
    <span class="md-ellipsis">
      disable_vae_slicing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.enable_model_cpu_offload" class="md-nav__link">
    <span class="md-ellipsis">
      enable_model_cpu_offload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.enable_vae_slicing" class="md-nav__link">
    <span class="md-ellipsis">
      enable_vae_slicing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.encode_prompt" class="md-nav__link">
    <span class="md-ellipsis">
      encode_prompt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.generate_language_model" class="md-nav__link">
    <span class="md-ellipsis">
      generate_language_model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2ProjectionModel" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2ProjectionModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2UNet2DConditionModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AudioLDM2UNet2DConditionModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.attn_processors" class="md-nav__link">
    <span class="md-ellipsis">
      attn_processors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.construct" class="md-nav__link">
    <span class="md-ellipsis">
      construct
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attention_slice" class="md-nav__link">
    <span class="md-ellipsis">
      set_attention_slice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attn_processor" class="md-nav__link">
    <span class="md-ellipsis">
      set_attn_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_default_attn_processor" class="md-nav__link">
    <span class="md-ellipsis">
      set_default_attn_processor
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.pipelines.AudioPipelineOutput" class="md-nav__link">
    <span class="md-ellipsis">
      AudioPipelineOutput
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../aura_flow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../auto_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../blip_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BLIP-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogview3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cogview4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Flux.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnetxs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnetxs_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dance_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dance Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deepfloyd_if/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepFloyd IF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../easyanimate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimate
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../control_flux_inpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlInpaint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../i2vgenxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I2VGen-XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pix2pix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructPix2Pix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kandinsky_v22/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kandinsky3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kolors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kolors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../latent_consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../latent_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../latte/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latte
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ledits_pp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LEDITS++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lumina2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina 2.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lumina/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina-T2X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marigold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../panorama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MultiDiffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../musicldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MusicLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../paint_by_example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paint by Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Personalized Image Animator (PIA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pixart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î±
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pixart_sigma/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î£
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sana_sprint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana Sprint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../self_attention_guidance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../semantic_stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../shap_e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Audio
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_cascade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Cascade
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4_65" >
        
          
          <label class="md-nav__link" for="__nav_2_11_4_65" id="__nav_2_11_4_65_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_4_65_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_4_65">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/text2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/image_variation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image variation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/stable_diffusion_safe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Safe Stable Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/stable_diffusion_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/stable_diffusion_3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/stable_diffusion_xl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/latent_upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Super-resolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/k_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/ldm3d_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_diffusion/gligen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GLIGEN (Grounded Language-to-Image Generation)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../stable_unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../text_to_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../text_to_video_zero/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text2Video-Zero
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unidiffuser/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniDiffuser
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../value_guided_sampling.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Value-guided sampling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../wuerstchen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wuerstchen
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Schedulers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_5">
            <span class="md-nav__icon md-icon"></span>
            Schedulers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/cm_stochastic_iterative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMStochasticIterativeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/consistency_decoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/cosine_dpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CosineDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMInverseScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/deis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DEISMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepInverse
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_sde/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSDEScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/singlestep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSinglestepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMEulerScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerAncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_euler_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchEulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_heun_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchHeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/heun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ipndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IPNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/stochastic_karras_ve.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KarrasVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2AncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2DiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LCMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lms_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LMSDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/pndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/repaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RePaintScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_ve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_vp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVpScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/tcd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TCDScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/unipc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniPCMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/vq_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQDiffusionScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_6" >
        
          
          <label class="md-nav__link" for="__nav_2_11_6" id="__nav_2_11_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Internal classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_6">
            <span class="md-nav__icon md-icon"></span>
            Internal classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../internal_classes_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attnprocessor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../activations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom activation functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom normalization layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utilities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VAE Image Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../video_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video Processor
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune a pretrained model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune_distribute/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed training and mixed precision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generation with LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tips" class="md-nav__link">
    <span class="md-ellipsis">
      Tips
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tips">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#choosing-a-checkpoint" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing a checkpoint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constructing-a-prompt" class="md-nav__link">
    <span class="md-ellipsis">
      Constructing a prompt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#controlling-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Controlling inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluating-generated-waveforms" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluating generated waveforms:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AudioLDM2Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.disable_vae_slicing" class="md-nav__link">
    <span class="md-ellipsis">
      disable_vae_slicing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.enable_model_cpu_offload" class="md-nav__link">
    <span class="md-ellipsis">
      enable_model_cpu_offload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.enable_vae_slicing" class="md-nav__link">
    <span class="md-ellipsis">
      enable_vae_slicing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.encode_prompt" class="md-nav__link">
    <span class="md-ellipsis">
      encode_prompt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2Pipeline.generate_language_model" class="md-nav__link">
    <span class="md-ellipsis">
      generate_language_model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2ProjectionModel" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2ProjectionModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel" class="md-nav__link">
    <span class="md-ellipsis">
      AudioLDM2UNet2DConditionModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AudioLDM2UNet2DConditionModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.attn_processors" class="md-nav__link">
    <span class="md-ellipsis">
      attn_processors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.construct" class="md-nav__link">
    <span class="md-ellipsis">
      construct
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attention_slice" class="md-nav__link">
    <span class="md-ellipsis">
      set_attention_slice
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attn_processor" class="md-nav__link">
    <span class="md-ellipsis">
      set_attn_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_default_attn_processor" class="md-nav__link">
    <span class="md-ellipsis">
      set_default_attn_processor
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.pipelines.AudioPipelineOutput" class="md-nav__link">
    <span class="md-ellipsis">
      AudioPipelineOutput
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/mindspore-lab/mindone/edit/master/docs/diffusers/api/pipelines/audioldm2.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindone/raw/master/docs/diffusers/api/pipelines/audioldm2.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="audioldm-2">AudioLDM 2<a class="headerlink" href="#audioldm-2" title="Permanent link">&para;</a></h1>
<p>AudioLDM 2 was proposed in <a href="https://arxiv.org/abs/2308.05734">AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</a> by Haohe Liu et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional sound effects, human speech and music.</p>
<p>Inspired by <a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview">Stable Diffusion</a>, AudioLDM 2 is a text-to-audio <em>latent diffusion model (LDM)</em> that learns continuous audio representations from text embeddings. Two text encoder models are used to compute the text embeddings from a prompt input: the text-branch of <a href="https://huggingface.co/docs/transformers/main/en/model_doc/clap">CLAP</a> and the encoder of <a href="https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5">Flan-T5</a>. These text embeddings are then projected to a shared embedding space by an <a href="https://huggingface.co/docs/diffusers/main/api/pipelines/audioldm2#diffusers.AudioLDM2ProjectionModel">AudioLDM2ProjectionModel</a>. A <a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT2</a> <em>language model (LM)</em> is used to auto-regressively predict eight new embedding vectors, conditional on the projected CLAP and Flan-T5 embeddings. The generated embedding vectors and Flan-T5 text embeddings are used as cross-attention conditioning in the LDM. The <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2UNet2DConditionModel">UNet</a> of AudioLDM 2 is unique in the sense that it takes <strong>two</strong> cross-attention embeddings, as opposed to one cross-attention conditioning, as in most other LDMs.</p>
<p>The abstract of the paper is the following:</p>
<p><em>Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called "language of audio" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate state-of-the-art or competitive performance against previous approaches. Our code, pretrained model, and demo are available at <a href="https://audioldm.github.io/audioldm2">this https URL</a>.</em></p>
<p>This pipeline was contributed by <a href="https://huggingface.co/sanchit-gandhi">sanchit-gandhi</a> and <a href="https://github.com/tuanh123789">NguyÃªÌƒn CÃ´ng TuÌ Anh</a>. The original codebase can be
found at <a href="https://github.com/haoheliu/audioldm2">haoheliu/audioldm2</a>.</p>
<h2 id="tips">Tips<a class="headerlink" href="#tips" title="Permanent link">&para;</a></h2>
<h3 id="choosing-a-checkpoint">Choosing a checkpoint<a class="headerlink" href="#choosing-a-checkpoint" title="Permanent link">&para;</a></h3>
<p>AudioLDM2 comes in three variants. Two of these checkpoints are applicable to the general task of text-to-audio generation. The third checkpoint is trained exclusively on text-to-music generation.</p>
<p>All checkpoints share the same model size for the text encoders and VAE. They differ in the size and depth of the UNet.
See table below for details on the three checkpoints:</p>
<table>
<thead>
<tr>
<th>Checkpoint</th>
<th>Task</th>
<th>UNet Model Size</th>
<th>Total Model Size</th>
<th>Training Data / h</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/cvssp/audioldm2">audioldm2</a></td>
<td>Text-to-audio</td>
<td>350M</td>
<td>1.1B</td>
<td>1150k</td>
</tr>
<tr>
<td><a href="https://huggingface.co/cvssp/audioldm2-large">audioldm2-large</a></td>
<td>Text-to-audio</td>
<td>750M</td>
<td>1.5B</td>
<td>1150k</td>
</tr>
<tr>
<td><a href="https://huggingface.co/cvssp/audioldm2-music">audioldm2-music</a></td>
<td>Text-to-music</td>
<td>350M</td>
<td>1.1B</td>
<td>665k</td>
</tr>
<tr>
<td><a href="https://huggingface.co/anhnct/audioldm2_gigaspeech">audioldm2-gigaspeech</a></td>
<td>Text-to-speech</td>
<td>350M</td>
<td>1.1B</td>
<td>10k</td>
</tr>
<tr>
<td><a href="https://huggingface.co/anhnct/audioldm2_ljspeech">audioldm2-ljspeech</a></td>
<td>Text-to-speech</td>
<td>350M</td>
<td>1.1B</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="constructing-a-prompt">Constructing a prompt<a class="headerlink" href="#constructing-a-prompt" title="Permanent link">&para;</a></h3>
<ul>
<li>Descriptive prompt inputs work best: use adjectives to describe the sound (e.g. "high quality" or "clear") and make the prompt context specific (e.g. "water stream in a forest" instead of "stream").</li>
<li>It's best to use general terms like "cat" or "dog" instead of specific names or abstract objects the model may not be familiar with.</li>
<li>Using a <strong>negative prompt</strong> can significantly improve the quality of the generated waveform, by guiding the generation away from terms that correspond to poor quality audio. Try using a negative prompt of "Low quality."</li>
</ul>
<h3 id="controlling-inference">Controlling inference<a class="headerlink" href="#controlling-inference" title="Permanent link">&para;</a></h3>
<ul>
<li>The <em>quality</em> of the predicted audio sample can be controlled by the <code>num_inference_steps</code> argument; higher steps give higher quality audio at the expense of slower inference.</li>
<li>The <em>length</em> of the predicted audio sample can be controlled by varying the <code>audio_length_in_s</code> argument.</li>
</ul>
<h3 id="evaluating-generated-waveforms">Evaluating generated waveforms:<a class="headerlink" href="#evaluating-generated-waveforms" title="Permanent link">&para;</a></h3>
<ul>
<li>The quality of the generated waveforms can vary significantly based on the seed. Try generating with different seeds until you find a satisfactory generation.</li>
<li>Multiple waveforms can be generated in one go: set <code>num_waveforms_per_prompt</code> to a value greater than 1. Automatic scoring will be performed between the generated waveforms and prompt text, and the audios ranked from best to worst accordingly.</li>
</ul>
<p>The following example demonstrates how to construct good music and speech generation using the aforementioned tips: <a href="https://mindspore-lab.github.io/mindone/latest/diffusers/api/pipelines/audioldm2/#mindone.diffusers.pipelines.AudioLDM2Pipeline.__call__">example</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Make sure to check out the Schedulers <a href="../../using-diffusers/schedulers">guide</a> to learn how to explore the tradeoff between scheduler speed and quality, and see the <a href="../../using-diffusers/loading#reuse-a-pipeline">reuse components across pipelines</a> section to learn how to efficiently load the same components into multiple pipelines.</p>
</div>


<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.AudioLDM2Pipeline" class="doc doc-heading">
            <code>mindone.diffusers.AudioLDM2Pipeline</code>


<a href="#mindone.diffusers.AudioLDM2Pipeline" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.DiffusionPipeline (mindone.diffusers.pipelines.pipeline_utils.DiffusionPipeline)" href="../overview/#mindone.diffusers.DiffusionPipeline">DiffusionPipeline</a></code></p>


        <p>Pipeline for text-to-audio generation using AudioLDM2.</p>
<p>This model inherits from [<code>DiffusionPipeline</code>]. Check the superclass documentation for the generic methods
implemented for all pipelines (downloading, saving etc.).</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>vae</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`AutoencoderKL`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>First frozen text-encoder. AudioLDM2 uses the joint audio-text embedding model
<a href="https://huggingface.co/docs/transformers/model_doc/clap#transformers.CLAPTextModelWithProjection">CLAP</a>,
specifically the <a href="https://huggingface.co/laion/clap-htsat-unfused">laion/clap-htsat-unfused</a> variant. The
text branch is used to encode the text prompt to a prompt embedding. The full audio-text model is used to
rank generated waveforms against the text prompt by computing similarity scores.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.ClapModel`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_2</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Second frozen text-encoder. AudioLDM2 uses the encoder of
<a href="https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel">T5</a>, specifically the
<a href="https://huggingface.co/google/flan-t5-large">google/flan-t5-large</a> variant. Second frozen text-encoder use
for TTS. AudioLDM2 uses the encoder of
<a href="https://huggingface.co/docs/transformers/model_doc/vits#transformers.VitsModel">Vits</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.T5EncoderModel`, `~transformers.VitsModel`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>projection_model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A trained model used to linearly project the hidden-states from the first and second text encoder models
and insert learned SOS and EOS token embeddings. The projected hidden-states from the two text encoders are
concatenated to give the input to the language model. A Learned Position Embedding for the Vits
hidden-states</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`AudioLDM2ProjectionModel`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>language_model</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An auto-regressive language model used to generate a sequence of hidden-states conditioned on the projected
outputs from the two text encoders.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.GPT2Model`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tokenizer to tokenize text for the first frozen text-encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.RobertaTokenizer`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer_2</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Tokenizer to tokenize text for the second frozen text-encoder.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.T5Tokenizer`, `~transformers.VitsTokenizer`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>feature_extractor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Feature extractor to pre-process generated audio waveforms to log-mel spectrograms for automatic scoring.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.ClapFeatureExtractor`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A <code>UNet2DConditionModel</code> to denoise the encoded audio latents.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`UNet2DConditionModel`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scheduler</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A scheduler to be used in combination with <code>unet</code> to denoise the encoded audio latents. Can be one of
[<code>DDIMScheduler</code>], [<code>LMSDiscreteScheduler</code>], or [<code>PNDMScheduler</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`SchedulerMixin`]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>vocoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vocoder of class <code>SpeechT5HifiGan</code> to convert the mel-spectrogram latents to the final audio waveform.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>[`~transformers.SpeechT5HifiGan`]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 120</span>
<span class="normal"> 121</span>
<span class="normal"> 122</span>
<span class="normal"> 123</span>
<span class="normal"> 124</span>
<span class="normal"> 125</span>
<span class="normal"> 126</span>
<span class="normal"> 127</span>
<span class="normal"> 128</span>
<span class="normal"> 129</span>
<span class="normal"> 130</span>
<span class="normal"> 131</span>
<span class="normal"> 132</span>
<span class="normal"> 133</span>
<span class="normal"> 134</span>
<span class="normal"> 135</span>
<span class="normal"> 136</span>
<span class="normal"> 137</span>
<span class="normal"> 138</span>
<span class="normal"> 139</span>
<span class="normal"> 140</span>
<span class="normal"> 141</span>
<span class="normal"> 142</span>
<span class="normal"> 143</span>
<span class="normal"> 144</span>
<span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AudioLDM2Pipeline</span><span class="p">(</span><span class="n">DiffusionPipeline</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pipeline for text-to-audio generation using AudioLDM2.</span>

<span class="sd">    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods</span>
<span class="sd">    implemented for all pipelines (downloading, saving etc.).</span>

<span class="sd">    Args:</span>
<span class="sd">        vae ([`AutoencoderKL`]):</span>
<span class="sd">            Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.</span>
<span class="sd">        text_encoder ([`~transformers.ClapModel`]):</span>
<span class="sd">            First frozen text-encoder. AudioLDM2 uses the joint audio-text embedding model</span>
<span class="sd">            [CLAP](https://huggingface.co/docs/transformers/model_doc/clap#transformers.CLAPTextModelWithProjection),</span>
<span class="sd">            specifically the [laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused) variant. The</span>
<span class="sd">            text branch is used to encode the text prompt to a prompt embedding. The full audio-text model is used to</span>
<span class="sd">            rank generated waveforms against the text prompt by computing similarity scores.</span>
<span class="sd">        text_encoder_2 ([`~transformers.T5EncoderModel`, `~transformers.VitsModel`]):</span>
<span class="sd">            Second frozen text-encoder. AudioLDM2 uses the encoder of</span>
<span class="sd">            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the</span>
<span class="sd">            [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) variant. Second frozen text-encoder use</span>
<span class="sd">            for TTS. AudioLDM2 uses the encoder of</span>
<span class="sd">            [Vits](https://huggingface.co/docs/transformers/model_doc/vits#transformers.VitsModel).</span>
<span class="sd">        projection_model ([`AudioLDM2ProjectionModel`]):</span>
<span class="sd">            A trained model used to linearly project the hidden-states from the first and second text encoder models</span>
<span class="sd">            and insert learned SOS and EOS token embeddings. The projected hidden-states from the two text encoders are</span>
<span class="sd">            concatenated to give the input to the language model. A Learned Position Embedding for the Vits</span>
<span class="sd">            hidden-states</span>
<span class="sd">        language_model ([`~transformers.GPT2Model`]):</span>
<span class="sd">            An auto-regressive language model used to generate a sequence of hidden-states conditioned on the projected</span>
<span class="sd">            outputs from the two text encoders.</span>
<span class="sd">        tokenizer ([`~transformers.RobertaTokenizer`]):</span>
<span class="sd">            Tokenizer to tokenize text for the first frozen text-encoder.</span>
<span class="sd">        tokenizer_2 ([`~transformers.T5Tokenizer`, `~transformers.VitsTokenizer`]):</span>
<span class="sd">            Tokenizer to tokenize text for the second frozen text-encoder.</span>
<span class="sd">        feature_extractor ([`~transformers.ClapFeatureExtractor`]):</span>
<span class="sd">            Feature extractor to pre-process generated audio waveforms to log-mel spectrograms for automatic scoring.</span>
<span class="sd">        unet ([`UNet2DConditionModel`]):</span>
<span class="sd">            A `UNet2DConditionModel` to denoise the encoded audio latents.</span>
<span class="sd">        scheduler ([`SchedulerMixin`]):</span>
<span class="sd">            A scheduler to be used in combination with `unet` to denoise the encoded audio latents. Can be one of</span>
<span class="sd">            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].</span>
<span class="sd">        vocoder ([`~transformers.SpeechT5HifiGan`]):</span>
<span class="sd">            Vocoder of class `SpeechT5HifiGan` to convert the mel-spectrogram latents to the final audio waveform.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">vae</span><span class="p">:</span> <span class="n">AutoencoderKL</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">:</span> <span class="n">ClapModel</span><span class="p">,</span>
        <span class="n">text_encoder_2</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">T5EncoderModel</span><span class="p">,</span> <span class="n">VitsModel</span><span class="p">],</span>
        <span class="n">projection_model</span><span class="p">:</span> <span class="n">AudioLDM2ProjectionModel</span><span class="p">,</span>
        <span class="n">language_model</span><span class="p">:</span> <span class="n">GPT2Model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">],</span>
        <span class="n">tokenizer_2</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5TokenizerFast</span><span class="p">,</span> <span class="n">VitsTokenizer</span><span class="p">],</span>
        <span class="n">feature_extractor</span><span class="p">:</span> <span class="n">ClapFeatureExtractor</span><span class="p">,</span>
        <span class="n">unet</span><span class="p">:</span> <span class="n">AudioLDM2UNet2DConditionModel</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">:</span> <span class="n">KarrasDiffusionSchedulers</span><span class="p">,</span>
        <span class="n">vocoder</span><span class="p">:</span> <span class="n">SpeechT5HifiGan</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_modules</span><span class="p">(</span>
            <span class="n">vae</span><span class="o">=</span><span class="n">vae</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">text_encoder_2</span><span class="o">=</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">projection_model</span><span class="o">=</span><span class="n">projection_model</span><span class="p">,</span>
            <span class="n">language_model</span><span class="o">=</span><span class="n">language_model</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">tokenizer_2</span><span class="o">=</span><span class="n">tokenizer_2</span><span class="p">,</span>
            <span class="n">feature_extractor</span><span class="o">=</span><span class="n">feature_extractor</span><span class="p">,</span>
            <span class="n">unet</span><span class="o">=</span><span class="n">unet</span><span class="p">,</span>
            <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
            <span class="n">vocoder</span><span class="o">=</span><span class="n">vocoder</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_out_channels</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Copied from diffusers.pipelines.pipeline_utils.StableDiffusionMixin.enable_vae_slicing</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">enable_vae_slicing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to</span>
<span class="sd">        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">enable_slicing</span><span class="p">()</span>

    <span class="c1"># Copied from diffusers.pipelines.pipeline_utils.StableDiffusionMixin.disable_vae_slicing</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">disable_vae_slicing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to</span>
<span class="sd">        computing decoding in one step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">disable_slicing</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_model_cpu_offload</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared</span>
<span class="sd">        to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`</span>
<span class="sd">        method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with</span>
<span class="sd">        `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hook</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># We&#39;ll offload the last model manually.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_offload_hook</span> <span class="o">=</span> <span class="n">hook</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">generate_language_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Generates a sequence of hidden-states from the language model, conditioned on the embedding inputs.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            inputs_embeds (`mindspore.tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">                The sequence used as a prompt for the generation.</span>
<span class="sd">            max_new_tokens (`int`):</span>
<span class="sd">                Number of new tokens to generate.</span>
<span class="sd">            model_kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">                Ad hoc parametrization of additional model-specific kwargs that will be forwarded to the `forward`</span>
<span class="sd">                function of the model.</span>

<span class="sd">        Return:</span>
<span class="sd">            `inputs_embeds (`mindspore.tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">                The sequence of generated hidden-states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_supports_dynamic_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span> <span class="k">if</span> <span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_new_tokens</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># prepare model inputs</span>
            <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

            <span class="c1"># forward pass to get next hidden states</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">next_hidden_states</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span>

            <span class="c1"># Update the model input</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">next_hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Update generated hidden states, model inputs, and length for next step</span>
            <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="o">-</span><span class="n">max_new_tokens</span><span class="p">:,</span> <span class="p">:]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode_prompt</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
        <span class="n">do_classifier_free_guidance</span><span class="p">,</span>
        <span class="n">transcription</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Encodes the prompt into text encoder hidden states.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">                prompt to be encoded</span>
<span class="sd">            transcription (`str` or `List[str]`):</span>
<span class="sd">                transcription of text to speech</span>
<span class="sd">            num_waveforms_per_prompt (`int`):</span>
<span class="sd">                number of waveforms that should be generated per prompt</span>
<span class="sd">            do_classifier_free_guidance (`bool`):</span>
<span class="sd">                whether to use classifier free guidance or not</span>
<span class="sd">            negative_prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">                The prompt or prompts not to guide the audio generation. If not defined, one has to pass</span>
<span class="sd">                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is</span>
<span class="sd">                less than `1`).</span>
<span class="sd">            prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed text embeddings from the Flan T5 model. Can be used to easily tweak text inputs, *e.g.*</span>
<span class="sd">                prompt weighting. If not provided, text embeddings will be computed from `prompt` input argument.</span>
<span class="sd">            negative_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed negative text embeddings from the Flan T5 model. Can be used to easily tweak text inputs,</span>
<span class="sd">                *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">                `negative_prompt` input argument.</span>
<span class="sd">            generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,</span>
<span class="sd">                 *e.g.* prompt weighting. If not provided, text embeddings will be generated from `prompt` input</span>
<span class="sd">                 argument.</span>
<span class="sd">            negative_generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text</span>
<span class="sd">                inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">                `negative_prompt` input argument.</span>
<span class="sd">            attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed attention mask to be applied to the `prompt_embeds`. If not provided, attention mask will</span>
<span class="sd">                be computed from `prompt` input argument.</span>
<span class="sd">            negative_attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed attention mask to be applied to the `negative_prompt_embeds`. If not provided, attention</span>
<span class="sd">                mask will be computed from `negative_prompt` input argument.</span>
<span class="sd">            max_new_tokens (`int`, *optional*, defaults to None):</span>
<span class="sd">                The number of new tokens to generate with the GPT2 language model.</span>
<span class="sd">        Returns:</span>
<span class="sd">            prompt_embeds (`mindspore.tensor`):</span>
<span class="sd">                Text embeddings from the Flan T5 model.</span>
<span class="sd">            attention_mask (`mindspore.tensor`):</span>
<span class="sd">                Attention mask to be applied to the `prompt_embeds`.</span>
<span class="sd">            generated_prompt_embeds (`mindspore.tensor`):</span>
<span class="sd">                Text embeddings generated from the GPT2 langauge model.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; import scipy</span>
<span class="sd">        &gt;&gt;&gt; from mindone.diffusers import AudioLDM2Pipeline</span>

<span class="sd">        &gt;&gt;&gt; repo_id = &quot;cvssp/audioldm2&quot;</span>
<span class="sd">        &gt;&gt;&gt; pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=mindspore.float16)</span>


<span class="sd">        &gt;&gt;&gt; # Get text embedding vectors</span>
<span class="sd">        &gt;&gt;&gt; prompt_embeds, attention_mask, generated_prompt_embeds = pipe.encode_prompt(</span>
<span class="sd">        ...     prompt=&quot;Techno music with a strong, upbeat tempo and high melodic riffs&quot;,</span>
<span class="sd">        ...     do_classifier_free_guidance=True,</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; # Pass text embeddings to pipeline for text-conditional audio generation</span>
<span class="sd">        &gt;&gt;&gt; audio = pipe(</span>
<span class="sd">        ...     prompt_embeds=prompt_embeds,</span>
<span class="sd">        ...     attention_mask=attention_mask,</span>
<span class="sd">        ...     generated_prompt_embeds=generated_prompt_embeds,</span>
<span class="sd">        ...     num_inference_steps=200,</span>
<span class="sd">        ...     audio_length_in_s=10.0,</span>
<span class="sd">        ... ).audios[0]</span>

<span class="sd">        &gt;&gt;&gt; # save generated audio sample</span>
<span class="sd">        &gt;&gt;&gt; scipy.io.wavfile.write(&quot;techno.wav&quot;, rate=16000, data=audio)</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Define tokenizers and text encoders</span>
        <span class="n">tokenizers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_2</span><span class="p">]</span>
        <span class="n">is_vits_text_encoder</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">VitsModel</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
            <span class="n">text_encoders</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_encoders</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prompt_embeds_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">attention_mask_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">text_encoder</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokenizers</span><span class="p">,</span> <span class="n">text_encoders</span><span class="p">):</span>
                <span class="n">use_prompt</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5TokenizerFast</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">text_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                    <span class="n">prompt</span> <span class="k">if</span> <span class="n">use_prompt</span> <span class="k">else</span> <span class="n">transcription</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">VitsTokenizer</span><span class="p">))</span>
                    <span class="k">else</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                    <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">text_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_inputs</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>
                <span class="n">untruncated_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;longest&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">untruncated_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">text_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">mint</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span>
                    <span class="n">text_input_ids</span><span class="p">,</span> <span class="n">untruncated_ids</span>
                <span class="p">):</span>
                    <span class="n">removed_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">untruncated_ids</span><span class="p">[:,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The following part of your input was truncated because </span><span class="si">{</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span><span class="si">}</span><span class="s2"> can &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;only handle sequences up to </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2"> tokens: </span><span class="si">{</span><span class="n">removed_text</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

                <span class="n">text_input_ids</span> <span class="o">=</span> <span class="n">text_input_ids</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

                <span class="k">if</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;clap&quot;</span><span class="p">:</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
                        <span class="n">text_input_ids</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="c1"># append the seq-len dim: (bs, hidden_size) -&gt; (bs, seq_len, hidden_size)</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="c1"># make sure that we attend to this single hidden-state</span>
                    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
                    <span class="c1"># Add end_token_id and attention mask in the end of sequence phonemes</span>
                    <span class="k">for</span> <span class="n">text_input_id</span><span class="p">,</span> <span class="n">text_attention_mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">text_input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
                        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">phoneme_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">text_input_id</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">phoneme_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                                <span class="n">text_input_id</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">182</span>
                                <span class="n">text_attention_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                                <span class="k">break</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                        <span class="n">text_input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                        <span class="n">text_input_ids</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">prompt_embeds_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prompt_embeds</span><span class="p">)</span>
                <span class="n">attention_mask_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

            <span class="n">projection_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_model</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">prompt_embeds_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states_1</span><span class="o">=</span><span class="n">prompt_embeds_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">attention_mask_1</span><span class="o">=</span><span class="n">attention_mask_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">projected_prompt_embeds</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">hidden_states</span>
            <span class="n">projected_attention_mask</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">attention_mask</span>

            <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_language_model</span><span class="p">(</span>
                <span class="n">projected_prompt_embeds</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">projected_attention_mask</span><span class="p">,</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention_mask</span> <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">bs_embed</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># duplicate text embeddings for each generation per prompt, using mps friendly method</span>
        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># duplicate attention mask for each generation per prompt</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">))</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

        <span class="n">bs_embed</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># duplicate generated embeddings for each generation per prompt, using mps friendly method</span>
        <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span>
        <span class="p">)</span>

        <span class="c1"># get unconditional embeddings for classifier free guidance</span>
        <span class="k">if</span> <span class="n">do_classifier_free_guidance</span> <span class="ow">and</span> <span class="n">negative_prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">uncond_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">negative_prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span>
            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`negative_prompt` should be the same type to `prompt`, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">)</span><span class="si">}</span><span class="s2"> !=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">negative_prompt</span><span class="p">]</span>
            <span class="k">elif</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`negative_prompt`: </span><span class="si">{</span><span class="n">negative_prompt</span><span class="si">}</span><span class="s2"> has batch size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">, but `prompt`:&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2"> has batch size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">. Please make sure that passed `negative_prompt` matches&quot;</span>
                    <span class="s2">&quot; the batch size of `prompt`.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="n">negative_prompt</span>

            <span class="n">negative_prompt_embeds_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">negative_attention_mask_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">text_encoder</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokenizers</span><span class="p">,</span> <span class="n">text_encoders</span><span class="p">):</span>
                <span class="n">uncond_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                    <span class="n">uncond_tokens</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
                    <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">VitsTokenizer</span><span class="p">))</span>
                    <span class="k">else</span> <span class="n">max_length</span><span class="p">,</span>
                    <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">uncond_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">uncond_input</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
                <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">uncond_input</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;clap&quot;</span><span class="p">:</span>
                    <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
                        <span class="n">uncond_input_ids</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="c1"># append the seq-len dim: (bs, hidden_size) -&gt; (bs, seq_len, hidden_size)</span>
                    <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="c1"># make sure that we attend to this single hidden-state</span>
                    <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
                    <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                        <span class="n">batch_size</span><span class="p">,</span>
                        <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                        <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                        <span class="n">uncond_input_ids</span><span class="p">,</span>
                        <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">negative_prompt_embeds_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">)</span>
                <span class="n">negative_attention_mask_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">negative_attention_mask</span><span class="p">)</span>

            <span class="n">projection_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_model</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">negative_prompt_embeds_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">hidden_states_1</span><span class="o">=</span><span class="n">negative_prompt_embeds_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">attention_mask_1</span><span class="o">=</span><span class="n">negative_attention_mask_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">negative_projected_prompt_embeds</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">hidden_states</span>
            <span class="n">negative_projected_attention_mask</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">attention_mask</span>

            <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_language_model</span><span class="p">(</span>
                <span class="n">negative_projected_prompt_embeds</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_projected_attention_mask</span><span class="p">,</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">negative_attention_mask</span>
                <span class="k">if</span> <span class="n">negative_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="k">else</span> <span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="c1"># duplicate unconditional embeddings for each generation per prompt, using mps friendly method</span>
            <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># duplicate unconditional attention mask for each generation per prompt</span>
            <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">))</span>
            <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

            <span class="c1"># duplicate unconditional generated embeddings for each generation per prompt</span>
            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
            <span class="p">)</span>

            <span class="c1"># For classifier free guidance, we need to do two forward passes.</span>
            <span class="c1"># Here we concatenate the unconditional and text embeddings into a single batch</span>
            <span class="c1"># to avoid doing two forward passes</span>
            <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_prompt_embeds</span><span class="p">,</span> <span class="n">prompt_embeds</span><span class="p">])</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>
            <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_generated_prompt_embeds</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span>

    <span class="c1"># Copied from diffusers.pipelines.audioldm.pipeline_audioldm.AudioLDMPipeline.mel_spectrogram_to_waveform</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">mel_spectrogram_to_waveform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mel_spectrogram</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mel_spectrogram</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">mel_spectrogram</span> <span class="o">=</span> <span class="n">mel_spectrogram</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">waveform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="p">(</span><span class="n">mel_spectrogram</span><span class="p">)</span>
        <span class="c1"># we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16</span>
        <span class="n">waveform</span> <span class="o">=</span> <span class="n">waveform</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">waveform</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">score_waveforms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
        <span class="n">resampled_audio</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span>
            <span class="n">audio</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">orig_sr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="n">target_sr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">sampling_rate</span>
        <span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span>
                <span class="nb">list</span><span class="p">(</span><span class="n">resampled_audio</span><span class="p">),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">sampling_rate</span>
            <span class="p">)</span><span class="o">.</span><span class="n">input_features</span>
        <span class="p">)</span>
        <span class="c1"># compute the audio-text similarity score using the CLAP model</span>
        <span class="n">logits_per_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits_per_text</span>
        <span class="c1"># sort by the highest matching generations per prompt</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">logits_per_text</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:,</span> <span class="p">:</span><span class="n">num_waveforms_per_prompt</span><span class="p">]</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">audio</span>

    <span class="c1"># Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_extra_step_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
        <span class="c1"># prepare extra kwargs for the scheduler step, since not all schedulers have the same signature</span>
        <span class="c1"># eta (Î·) is only used with the DDIMScheduler, it will be ignored for other schedulers.</span>
        <span class="c1"># eta corresponds to Î· in DDIM paper: https://arxiv.org/abs/2010.02502</span>
        <span class="c1"># and should be between [0, 1]</span>

        <span class="n">accepts_eta</span> <span class="o">=</span> <span class="s2">&quot;eta&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">extra_step_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">accepts_eta</span><span class="p">:</span>
            <span class="n">extra_step_kwargs</span><span class="p">[</span><span class="s2">&quot;eta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eta</span>

        <span class="c1"># check if the scheduler accepts generator</span>
        <span class="n">accepts_generator</span> <span class="o">=</span> <span class="s2">&quot;generator&quot;</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">accepts_generator</span><span class="p">:</span>
            <span class="n">extra_step_kwargs</span><span class="p">[</span><span class="s2">&quot;generator&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="k">return</span> <span class="n">extra_step_kwargs</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">audio_length_in_s</span><span class="p">,</span>
        <span class="n">vocoder_upsample_factor</span><span class="p">,</span>
        <span class="n">callback_steps</span><span class="p">,</span>
        <span class="n">transcription</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">min_audio_length_in_s</span> <span class="o">=</span> <span class="n">vocoder_upsample_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span>
        <span class="k">if</span> <span class="n">audio_length_in_s</span> <span class="o">&lt;</span> <span class="n">min_audio_length_in_s</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`audio_length_in_s` has to be a positive value greater than or equal to </span><span class="si">{</span><span class="n">min_audio_length_in_s</span><span class="si">}</span><span class="s2">, but &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is </span><span class="si">{</span><span class="n">audio_length_in_s</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_in_dim</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The number of frequency bins in the vocoder&#39;s log-mel spectrogram has to be divisible by the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;VAE scale factor, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_in_dim</span><span class="si">}</span><span class="s2"> bins and a scale factor of &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">callback_steps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">callback_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback_steps</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">callback_steps</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`callback_steps` has to be a positive integer but is </span><span class="si">{</span><span class="n">callback_steps</span><span class="si">}</span><span class="s2"> of type&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">callback_steps</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot forward both `prompt`: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2"> and `prompt_embeds`: </span><span class="si">{</span><span class="n">prompt_embeds</span><span class="si">}</span><span class="s2">. Please make sure to&quot;</span>
                <span class="s2">&quot; only forward one of the two.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">generated_prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Provide either `prompt`, or `prompt_embeds` and `generated_prompt_embeds`. Cannot leave &quot;</span>
                <span class="s2">&quot;`prompt` undefined without specifying both `prompt_embeds` and `generated_prompt_embeds`.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`prompt` has to be of type `str` or `list` but is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">negative_prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">negative_prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot forward both `negative_prompt`: </span><span class="si">{</span><span class="n">negative_prompt</span><span class="si">}</span><span class="s2"> and `negative_prompt_embeds`:&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">negative_prompt_embeds</span><span class="si">}</span><span class="s2">. Please make sure to only forward one of the two.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">negative_prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">negative_generated_prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot forward `negative_prompt_embeds` without `negative_generated_prompt_embeds`. Ensure that&quot;</span>
                <span class="s2">&quot;both arguments are specified&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">negative_prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; got: `prompt_embeds` </span><span class="si">{</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != `negative_prompt_embeds`&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`attention_mask should have the same batch size and sequence length as `prompt_embeds`, but got:&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`attention_mask: </span><span class="si">{</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != `prompt_embeds` </span><span class="si">{</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">transcription</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;vits&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot forward without transcription. Please make sure to&quot;</span> <span class="s2">&quot; have transcription&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">transcription</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transcription</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transcription</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`transcription` has to be of type `str` or `list` but is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">transcription</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">generated_prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">negative_generated_prompt_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`generated_prompt_embeds` and `negative_generated_prompt_embeds` must have the same shape when &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;passed directly, but got: `generated_prompt_embeds` </span><span class="si">{</span><span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`negative_generated_prompt_embeds` </span><span class="si">{</span><span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">negative_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`attention_mask should have the same batch size and sequence length as `prompt_embeds`, but got:&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`attention_mask: </span><span class="si">{</span><span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> != `prompt_embeds` </span><span class="si">{</span><span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="c1"># Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents \</span>
    <span class="c1"># with width-&gt;self.vocoder.config.model_in_dim</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_latents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_channels_latents</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">latents</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_channels_latents</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="n">height</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span><span class="p">,</span>
            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_in_dim</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have passed a list of generators of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span><span class="si">}</span><span class="s2">, but requested an effective batch&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; size of </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">. Make sure the batch size matches the length of the generators.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">latents</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">latents</span> <span class="o">=</span> <span class="n">randn_tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">latents</span> <span class="o">=</span> <span class="n">latents</span>

        <span class="c1"># scale the initial noise by the standard deviation required by the scheduler</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="n">latents</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">init_noise_sigma</span>
        <span class="k">return</span> <span class="n">latents</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transcription</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">audio_length_in_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_inference_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
        <span class="n">guidance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.5</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_waveforms_per_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">latents</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">negative_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">callback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callback_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;np&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The call function to the pipeline for generation.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">                The prompt or prompts to guide audio generation. If not defined, you need to pass `prompt_embeds`.</span>
<span class="sd">            transcription (`str` or `List[str]`, *optional*):\</span>
<span class="sd">                The transcript for text to speech.</span>
<span class="sd">            audio_length_in_s (`int`, *optional*, defaults to 10.24):</span>
<span class="sd">                The length of the generated audio sample in seconds.</span>
<span class="sd">            num_inference_steps (`int`, *optional*, defaults to 200):</span>
<span class="sd">                The number of denoising steps. More denoising steps usually lead to a higher quality audio at the</span>
<span class="sd">                expense of slower inference.</span>
<span class="sd">            guidance_scale (`float`, *optional*, defaults to 3.5):</span>
<span class="sd">                A higher guidance scale value encourages the model to generate audio that is closely linked to the text</span>
<span class="sd">                `prompt` at the expense of lower sound quality. Guidance scale is enabled when `guidance_scale &gt; 1`.</span>
<span class="sd">            negative_prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">                The prompt or prompts to guide what to not include in audio generation. If not defined, you need to</span>
<span class="sd">                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale &lt; 1`).</span>
<span class="sd">            num_waveforms_per_prompt (`int`, *optional*, defaults to 1):</span>
<span class="sd">                The number of waveforms to generate per prompt. If `num_waveforms_per_prompt &gt; 1`, then automatic</span>
<span class="sd">                scoring is performed between the generated outputs and the text prompt. This scoring ranks the</span>
<span class="sd">                generated waveforms based on their cosine similarity with the text input in the joint text-audio</span>
<span class="sd">                embedding space.</span>
<span class="sd">            eta (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">                Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies</span>
<span class="sd">                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.</span>
<span class="sd">            generator (`np.random.Generator` or `List[np.random.Generator]`, *optional*):</span>
<span class="sd">                A [`np.random.Generator`](https://pytorch.org/docs/stable/generated/np.random.Generator.html) to make</span>
<span class="sd">                generation deterministic.</span>
<span class="sd">            latents (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for spectrogram</span>
<span class="sd">                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents</span>
<span class="sd">                tensor is generated by sampling using the supplied random `generator`.</span>
<span class="sd">            prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not</span>
<span class="sd">                provided, text embeddings are generated from the `prompt` input argument.</span>
<span class="sd">            negative_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If</span>
<span class="sd">                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.</span>
<span class="sd">            generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,</span>
<span class="sd">                 *e.g.* prompt weighting. If not provided, text embeddings will be generated from `prompt` input</span>
<span class="sd">                 argument.</span>
<span class="sd">            negative_generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text</span>
<span class="sd">                inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">                `negative_prompt` input argument.</span>
<span class="sd">            attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed attention mask to be applied to the `prompt_embeds`. If not provided, attention mask will</span>
<span class="sd">                be computed from `prompt` input argument.</span>
<span class="sd">            negative_attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">                Pre-computed attention mask to be applied to the `negative_prompt_embeds`. If not provided, attention</span>
<span class="sd">                mask will be computed from `negative_prompt` input argument.</span>
<span class="sd">            max_new_tokens (`int`, *optional*, defaults to None):</span>
<span class="sd">                Number of new tokens to generate with the GPT2 language model. If not provided, number of tokens will</span>
<span class="sd">                be taken from the config of the model.</span>
<span class="sd">            return_dict (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a</span>
<span class="sd">                plain tuple.</span>
<span class="sd">            callback (`Callable`, *optional*):</span>
<span class="sd">                A function that calls every `callback_steps` steps during inference. The function is called with the</span>
<span class="sd">                following arguments: `callback(step: int, timestep: int, latents: mindspore.tensor)`.</span>
<span class="sd">            callback_steps (`int`, *optional*, defaults to 1):</span>
<span class="sd">                The frequency at which the `callback` function is called. If not specified, the callback is called at</span>
<span class="sd">                every step.</span>
<span class="sd">            cross_attention_kwargs (`dict`, *optional*):</span>
<span class="sd">                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in</span>
<span class="sd">                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).</span>
<span class="sd">            output_type (`str`, *optional*, defaults to `&quot;np&quot;`):</span>
<span class="sd">                The output format of the generated audio. Choose between `&quot;np&quot;` to return a NumPy `np.ndarray` or</span>
<span class="sd">                `&quot;pt&quot;` to return a PyTorch `mindspore.tensor` object. Set to `&quot;latent&quot;` to return the latent diffusion</span>
<span class="sd">                model (LDM) output.</span>

<span class="sd">        Examples:</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:</span>
<span class="sd">                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,</span>
<span class="sd">                otherwise a `tuple` is returned where the first element is a list with the generated audio.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 0. Convert audio input length from seconds to spectrogram height</span>
        <span class="n">vocoder_upsample_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sampling_rate</span>

        <span class="k">if</span> <span class="n">audio_length_in_s</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">audio_length_in_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">*</span> <span class="n">vocoder_upsample_factor</span>

        <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">audio_length_in_s</span> <span class="o">/</span> <span class="n">vocoder_upsample_factor</span><span class="p">)</span>

        <span class="n">original_waveform_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">audio_length_in_s</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">height</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">height</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Audio length in seconds </span><span class="si">{</span><span class="n">audio_length_in_s</span><span class="si">}</span><span class="s2"> is increased to </span><span class="si">{</span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vocoder_upsample_factor</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;so that it can be handled by the model. It will be cut to </span><span class="si">{</span><span class="n">audio_length_in_s</span><span class="si">}</span><span class="s2"> after the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;denoising process.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># 1. Check inputs. Raise error if not correct</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_inputs</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span>
            <span class="n">audio_length_in_s</span><span class="p">,</span>
            <span class="n">vocoder_upsample_factor</span><span class="p">,</span>
            <span class="n">callback_steps</span><span class="p">,</span>
            <span class="n">transcription</span><span class="p">,</span>
            <span class="n">negative_prompt</span><span class="p">,</span>
            <span class="n">prompt_embeds</span><span class="p">,</span>
            <span class="n">negative_prompt_embeds</span><span class="p">,</span>
            <span class="n">generated_prompt_embeds</span><span class="p">,</span>
            <span class="n">negative_generated_prompt_embeds</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">negative_attention_mask</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 2. Define call parameters</span>
        <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)</span>
        <span class="c1"># of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`</span>
        <span class="c1"># corresponds to doing no classifier free guidance.</span>
        <span class="n">do_classifier_free_guidance</span> <span class="o">=</span> <span class="n">guidance_scale</span> <span class="o">&gt;</span> <span class="mf">1.0</span>

        <span class="c1"># 3. Encode input prompt</span>
        <span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_prompt</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span>
            <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
            <span class="n">do_classifier_free_guidance</span><span class="p">,</span>
            <span class="n">transcription</span><span class="p">,</span>
            <span class="n">negative_prompt</span><span class="p">,</span>
            <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
            <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
            <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="n">generated_prompt_embeds</span><span class="p">,</span>
            <span class="n">negative_generated_prompt_embeds</span><span class="o">=</span><span class="n">negative_generated_prompt_embeds</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">negative_attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 4. Prepare timesteps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">set_timesteps</span><span class="p">(</span><span class="n">num_inference_steps</span><span class="p">)</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">timesteps</span>

        <span class="c1"># 5. Prepare latent variables</span>
        <span class="n">num_channels_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_latents</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
            <span class="n">num_channels_latents</span><span class="p">,</span>
            <span class="n">height</span><span class="p">,</span>
            <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">generator</span><span class="p">,</span>
            <span class="n">latents</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 6. Prepare extra step kwargs</span>
        <span class="n">extra_step_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_extra_step_kwargs</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

        <span class="c1"># 7. Denoising loop</span>
        <span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_inference_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
                <span class="c1"># expand the latents if we are doing classifier free guidance</span>
                <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">latents</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">do_classifier_free_guidance</span> <span class="k">else</span> <span class="n">latents</span>
                <span class="n">latent_model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">scale_model_input</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

                <span class="c1"># predict the noise residual</span>
                <span class="n">noise_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">(</span>
                    <span class="n">latent_model_input</span><span class="p">,</span>
                    <span class="n">t</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">generated_prompt_embeds</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># perform guidance</span>
                <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
                    <span class="n">noise_pred_uncond</span><span class="p">,</span> <span class="n">noise_pred_text</span> <span class="o">=</span> <span class="n">noise_pred</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                    <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_uncond</span> <span class="o">+</span> <span class="n">guidance_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_pred_text</span> <span class="o">-</span> <span class="n">noise_pred_uncond</span><span class="p">)</span>

                <span class="c1"># compute the previous noisy sample x_t -&gt; x_t-1</span>
                <span class="n">latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_step_kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># call the callback, if provided</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">step_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;order&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">)</span>

        <span class="c1"># 8. Post-processing</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_type</span> <span class="o">==</span> <span class="s2">&quot;latent&quot;</span><span class="p">:</span>
            <span class="n">latents</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">latents</span>
            <span class="n">mel_spectrogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">latents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">AudioPipelineOutput</span><span class="p">(</span><span class="n">audios</span><span class="o">=</span><span class="n">latents</span><span class="p">)</span>

        <span class="n">audio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mel_spectrogram_to_waveform</span><span class="p">(</span><span class="n">mel_spectrogram</span><span class="p">)</span>

        <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[:,</span> <span class="p">:</span><span class="n">original_waveform_length</span><span class="p">]</span>

        <span class="c1"># 9. Automatic scoring</span>
        <span class="k">if</span> <span class="n">num_waveforms_per_prompt</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">audio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_waveforms</span><span class="p">(</span>
                <span class="n">text</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span>
                <span class="n">num_waveforms_per_prompt</span><span class="o">=</span><span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">output_type</span> <span class="o">==</span> <span class="s2">&quot;np&quot;</span><span class="p">:</span>
            <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">audio</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">AudioPipelineOutput</span><span class="p">(</span><span class="n">audios</span><span class="o">=</span><span class="n">audio</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">transcription</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">audio_length_in_s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_inference_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">guidance_scale</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">negative_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">latents</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callback_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="s1">&#39;np&#39;</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.__call__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>The call function to the pipeline for generation.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The prompt or prompts to guide audio generation. If not defined, you need to pass <code>prompt_embeds</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transcription</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>\
The transcript for text to speech.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>audio_length_in_s</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The length of the generated audio sample in seconds.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 10.24</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_inference_steps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of denoising steps. More denoising steps usually lead to a higher quality audio at the
expense of slower inference.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 200</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>200</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guidance_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A higher guidance scale value encourages the model to generate audio that is closely linked to the text
<code>prompt</code> at the expense of lower sound quality. Guidance scale is enabled when <code>guidance_scale &gt; 1</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 3.5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3.5</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The prompt or prompts to guide what to not include in audio generation. If not defined, you need to
pass <code>negative_prompt_embeds</code> instead. Ignored when not using guidance (<code>guidance_scale &lt; 1</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_waveforms_per_prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of waveforms to generate per prompt. If <code>num_waveforms_per_prompt &gt; 1</code>, then automatic
scoring is performed between the generated outputs and the text prompt. This scoring ranks the
generated waveforms based on their cosine similarity with the text input in the joint text-audio
embedding space.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>eta</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Corresponds to parameter eta (Î·) from the <a href="https://arxiv.org/abs/2010.02502">DDIM</a> paper. Only applies
to the [<code>~schedulers.DDIMScheduler</code>], and is ignored in other schedulers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generator</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A <a href="https://pytorch.org/docs/stable/generated/np.random.Generator.html"><code>np.random.Generator</code></a> to make
generation deterministic.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`np.random.Generator` or `List[np.random.Generator]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>latents</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for spectrogram
generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
tensor is generated by sampling using the supplied random <code>generator</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not
provided, text embeddings are generated from the <code>prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
not provided, <code>negative_prompt_embeds</code> are generated from the <code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generated_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,
 <em>e.g.</em> prompt weighting. If not provided, text embeddings will be generated from <code>prompt</code> input
 argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_generated_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text
inputs, <em>e.g.</em> prompt weighting. If not provided, negative_prompt_embeds will be computed from
<code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed attention mask to be applied to the <code>prompt_embeds</code>. If not provided, attention mask will
be computed from <code>prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed attention mask to be applied to the <code>negative_prompt_embeds</code>. If not provided, attention
mask will be computed from <code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of new tokens to generate with the GPT2 language model. If not provided, number of tokens will
be taken from the config of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to None</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code>] instead of a
plain tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>callback</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A function that calls every <code>callback_steps</code> steps during inference. The function is called with the
following arguments: <code>callback(step: int, timestep: int, latents: mindspore.tensor)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>callback_steps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The frequency at which the <code>callback</code> function is called. If not specified, the callback is called at
every step.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A kwargs dictionary that if specified is passed along to the [<code>AttentionProcessor</code>] as defined in
<a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py"><code>self.processor</code></a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The output format of the generated audio. Choose between <code>"np"</code> to return a NumPy <code>np.ndarray</code> or
<code>"pt"</code> to return a PyTorch <code>mindspore.tensor</code> object. Set to <code>"latent"</code> to return the latent diffusion
model (LDM) output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;np&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;np&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>[<code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code>] or <code>tuple</code>:
If <code>return_dict</code> is <code>True</code>, [<code>~pipelines.stable_diffusion.StableDiffusionPipelineOutput</code>] is returned,
otherwise a <code>tuple</code> is returned where the first element is a list with the generated audio.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">transcription</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">audio_length_in_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_inference_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">guidance_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.5</span><span class="p">,</span>
    <span class="n">negative_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_waveforms_per_prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">generator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">latents</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">callback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">],</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callback_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">output_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;np&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The call function to the pipeline for generation.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">            The prompt or prompts to guide audio generation. If not defined, you need to pass `prompt_embeds`.</span>
<span class="sd">        transcription (`str` or `List[str]`, *optional*):\</span>
<span class="sd">            The transcript for text to speech.</span>
<span class="sd">        audio_length_in_s (`int`, *optional*, defaults to 10.24):</span>
<span class="sd">            The length of the generated audio sample in seconds.</span>
<span class="sd">        num_inference_steps (`int`, *optional*, defaults to 200):</span>
<span class="sd">            The number of denoising steps. More denoising steps usually lead to a higher quality audio at the</span>
<span class="sd">            expense of slower inference.</span>
<span class="sd">        guidance_scale (`float`, *optional*, defaults to 3.5):</span>
<span class="sd">            A higher guidance scale value encourages the model to generate audio that is closely linked to the text</span>
<span class="sd">            `prompt` at the expense of lower sound quality. Guidance scale is enabled when `guidance_scale &gt; 1`.</span>
<span class="sd">        negative_prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">            The prompt or prompts to guide what to not include in audio generation. If not defined, you need to</span>
<span class="sd">            pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale &lt; 1`).</span>
<span class="sd">        num_waveforms_per_prompt (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The number of waveforms to generate per prompt. If `num_waveforms_per_prompt &gt; 1`, then automatic</span>
<span class="sd">            scoring is performed between the generated outputs and the text prompt. This scoring ranks the</span>
<span class="sd">            generated waveforms based on their cosine similarity with the text input in the joint text-audio</span>
<span class="sd">            embedding space.</span>
<span class="sd">        eta (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies</span>
<span class="sd">            to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.</span>
<span class="sd">        generator (`np.random.Generator` or `List[np.random.Generator]`, *optional*):</span>
<span class="sd">            A [`np.random.Generator`](https://pytorch.org/docs/stable/generated/np.random.Generator.html) to make</span>
<span class="sd">            generation deterministic.</span>
<span class="sd">        latents (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for spectrogram</span>
<span class="sd">            generation. Can be used to tweak the same generation with different prompts. If not provided, a latents</span>
<span class="sd">            tensor is generated by sampling using the supplied random `generator`.</span>
<span class="sd">        prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not</span>
<span class="sd">            provided, text embeddings are generated from the `prompt` input argument.</span>
<span class="sd">        negative_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If</span>
<span class="sd">            not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.</span>
<span class="sd">        generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,</span>
<span class="sd">             *e.g.* prompt weighting. If not provided, text embeddings will be generated from `prompt` input</span>
<span class="sd">             argument.</span>
<span class="sd">        negative_generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text</span>
<span class="sd">            inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">            `negative_prompt` input argument.</span>
<span class="sd">        attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed attention mask to be applied to the `prompt_embeds`. If not provided, attention mask will</span>
<span class="sd">            be computed from `prompt` input argument.</span>
<span class="sd">        negative_attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed attention mask to be applied to the `negative_prompt_embeds`. If not provided, attention</span>
<span class="sd">            mask will be computed from `negative_prompt` input argument.</span>
<span class="sd">        max_new_tokens (`int`, *optional*, defaults to None):</span>
<span class="sd">            Number of new tokens to generate with the GPT2 language model. If not provided, number of tokens will</span>
<span class="sd">            be taken from the config of the model.</span>
<span class="sd">        return_dict (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a</span>
<span class="sd">            plain tuple.</span>
<span class="sd">        callback (`Callable`, *optional*):</span>
<span class="sd">            A function that calls every `callback_steps` steps during inference. The function is called with the</span>
<span class="sd">            following arguments: `callback(step: int, timestep: int, latents: mindspore.tensor)`.</span>
<span class="sd">        callback_steps (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The frequency at which the `callback` function is called. If not specified, the callback is called at</span>
<span class="sd">            every step.</span>
<span class="sd">        cross_attention_kwargs (`dict`, *optional*):</span>
<span class="sd">            A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in</span>
<span class="sd">            [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).</span>
<span class="sd">        output_type (`str`, *optional*, defaults to `&quot;np&quot;`):</span>
<span class="sd">            The output format of the generated audio. Choose between `&quot;np&quot;` to return a NumPy `np.ndarray` or</span>
<span class="sd">            `&quot;pt&quot;` to return a PyTorch `mindspore.tensor` object. Set to `&quot;latent&quot;` to return the latent diffusion</span>
<span class="sd">            model (LDM) output.</span>

<span class="sd">    Examples:</span>

<span class="sd">    Returns:</span>
<span class="sd">        [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:</span>
<span class="sd">            If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,</span>
<span class="sd">            otherwise a `tuple` is returned where the first element is a list with the generated audio.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 0. Convert audio input length from seconds to spectrogram height</span>
    <span class="n">vocoder_upsample_factor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">upsample_rates</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sampling_rate</span>

    <span class="k">if</span> <span class="n">audio_length_in_s</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">audio_length_in_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">*</span> <span class="n">vocoder_upsample_factor</span>

    <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">audio_length_in_s</span> <span class="o">/</span> <span class="n">vocoder_upsample_factor</span><span class="p">)</span>

    <span class="n">original_waveform_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">audio_length_in_s</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">height</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">height</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">height</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae_scale_factor</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Audio length in seconds </span><span class="si">{</span><span class="n">audio_length_in_s</span><span class="si">}</span><span class="s2"> is increased to </span><span class="si">{</span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vocoder_upsample_factor</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;so that it can be handled by the model. It will be cut to </span><span class="si">{</span><span class="n">audio_length_in_s</span><span class="si">}</span><span class="s2"> after the &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;denoising process.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># 1. Check inputs. Raise error if not correct</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">check_inputs</span><span class="p">(</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">audio_length_in_s</span><span class="p">,</span>
        <span class="n">vocoder_upsample_factor</span><span class="p">,</span>
        <span class="n">callback_steps</span><span class="p">,</span>
        <span class="n">transcription</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="p">,</span>
        <span class="n">negative_prompt_embeds</span><span class="p">,</span>
        <span class="n">generated_prompt_embeds</span><span class="p">,</span>
        <span class="n">negative_generated_prompt_embeds</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">negative_attention_mask</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 2. Define call parameters</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)</span>
    <span class="c1"># of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`</span>
    <span class="c1"># corresponds to doing no classifier free guidance.</span>
    <span class="n">do_classifier_free_guidance</span> <span class="o">=</span> <span class="n">guidance_scale</span> <span class="o">&gt;</span> <span class="mf">1.0</span>

    <span class="c1"># 3. Encode input prompt</span>
    <span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_prompt</span><span class="p">(</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
        <span class="n">do_classifier_free_guidance</span><span class="p">,</span>
        <span class="n">transcription</span><span class="p">,</span>
        <span class="n">negative_prompt</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
        <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
        <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="n">generated_prompt_embeds</span><span class="p">,</span>
        <span class="n">negative_generated_prompt_embeds</span><span class="o">=</span><span class="n">negative_generated_prompt_embeds</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">negative_attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 4. Prepare timesteps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">set_timesteps</span><span class="p">(</span><span class="n">num_inference_steps</span><span class="p">)</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">timesteps</span>

    <span class="c1"># 5. Prepare latent variables</span>
    <span class="n">num_channels_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">in_channels</span>
    <span class="n">latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_latents</span><span class="p">(</span>
        <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
        <span class="n">num_channels_latents</span><span class="p">,</span>
        <span class="n">height</span><span class="p">,</span>
        <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">,</span>
        <span class="n">latents</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 6. Prepare extra step kwargs</span>
    <span class="n">extra_step_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_extra_step_kwargs</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>

    <span class="c1"># 7. Denoising loop</span>
    <span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_inference_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">)</span> <span class="k">as</span> <span class="n">progress_bar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
            <span class="c1"># expand the latents if we are doing classifier free guidance</span>
            <span class="n">latent_model_input</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">latents</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">do_classifier_free_guidance</span> <span class="k">else</span> <span class="n">latents</span>
            <span class="n">latent_model_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">scale_model_input</span><span class="p">(</span><span class="n">latent_model_input</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

            <span class="c1"># predict the noise residual</span>
            <span class="n">noise_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">(</span>
                <span class="n">latent_model_input</span><span class="p">,</span>
                <span class="n">t</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">generated_prompt_embeds</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
                <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># perform guidance</span>
            <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
                <span class="n">noise_pred_uncond</span><span class="p">,</span> <span class="n">noise_pred_text</span> <span class="o">=</span> <span class="n">noise_pred</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">noise_pred_uncond</span> <span class="o">+</span> <span class="n">guidance_scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">noise_pred_text</span> <span class="o">-</span> <span class="n">noise_pred_uncond</span><span class="p">)</span>

            <span class="c1"># compute the previous noisy sample x_t -&gt; x_t-1</span>
            <span class="n">latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_step_kwargs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># call the callback, if provided</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span> <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">step_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;order&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">latents</span><span class="p">)</span>

    <span class="c1"># 8. Post-processing</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_type</span> <span class="o">==</span> <span class="s2">&quot;latent&quot;</span><span class="p">:</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">scaling_factor</span> <span class="o">*</span> <span class="n">latents</span>
        <span class="n">mel_spectrogram</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">latents</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AudioPipelineOutput</span><span class="p">(</span><span class="n">audios</span><span class="o">=</span><span class="n">latents</span><span class="p">)</span>

    <span class="n">audio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mel_spectrogram_to_waveform</span><span class="p">(</span><span class="n">mel_spectrogram</span><span class="p">)</span>

    <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[:,</span> <span class="p">:</span><span class="n">original_waveform_length</span><span class="p">]</span>

    <span class="c1"># 9. Automatic scoring</span>
    <span class="k">if</span> <span class="n">num_waveforms_per_prompt</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_waveforms</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">audio</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span>
            <span class="n">num_waveforms_per_prompt</span><span class="o">=</span><span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">output_type</span> <span class="o">==</span> <span class="s2">&quot;np&quot;</span><span class="p">:</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">audio</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">AudioPipelineOutput</span><span class="p">(</span><span class="n">audios</span><span class="o">=</span><span class="n">audio</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.disable_vae_slicing" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">disable_vae_slicing</span><span class="p">()</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.disable_vae_slicing" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Disable sliced VAE decoding. If <code>enable_vae_slicing</code> was previously enabled, this method will go back to
computing decoding in one step.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">disable_vae_slicing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to</span>
<span class="sd">    computing decoding in one step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">disable_slicing</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.enable_model_cpu_offload" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">enable_model_cpu_offload</span><span class="p">()</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.enable_model_cpu_offload" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared
to <code>enable_sequential_cpu_offload</code>, this method moves one whole model at a time to the GPU when its <code>forward</code>
method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with
<code>enable_sequential_cpu_offload</code>, but performance is much better due to the iterative execution of the <code>unet</code>.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">enable_model_cpu_offload</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared</span>
<span class="sd">    to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`</span>
<span class="sd">    method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with</span>
<span class="sd">    `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hook</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># We&#39;ll offload the last model manually.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">final_offload_hook</span> <span class="o">=</span> <span class="n">hook</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.enable_vae_slicing" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">enable_vae_slicing</span><span class="p">()</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.enable_vae_slicing" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to
compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">enable_vae_slicing</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to</span>
<span class="sd">    compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vae</span><span class="o">.</span><span class="n">enable_slicing</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.encode_prompt" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">encode_prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">do_classifier_free_guidance</span><span class="p">,</span> <span class="n">transcription</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">negative_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.encode_prompt" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Encodes the prompt into text encoder hidden states.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>prompt to be encoded</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transcription</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>transcription of text to speech</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_waveforms_per_prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>number of waveforms that should be generated per prompt</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>do_classifier_free_guidance</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>whether to use classifier free guidance or not</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The prompt or prompts not to guide the audio generation. If not defined, one has to pass
<code>negative_prompt_embeds</code> instead. Ignored when not using guidance (i.e., ignored if <code>guidance_scale</code> is
less than <code>1</code>).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed text embeddings from the Flan T5 model. Can be used to easily tweak text inputs, <em>e.g.</em>
prompt weighting. If not provided, text embeddings will be computed from <code>prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed negative text embeddings from the Flan T5 model. Can be used to easily tweak text inputs,
<em>e.g.</em> prompt weighting. If not provided, negative_prompt_embeds will be computed from
<code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>generated_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,
 <em>e.g.</em> prompt weighting. If not provided, text embeddings will be generated from <code>prompt</code> input
 argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_generated_prompt_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text
inputs, <em>e.g.</em> prompt weighting. If not provided, negative_prompt_embeds will be computed from
<code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed attention mask to be applied to the <code>prompt_embeds</code>. If not provided, attention mask will
be computed from <code>prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>negative_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Pre-computed attention mask to be applied to the <code>negative_prompt_embeds</code>. If not provided, attention
mask will be computed from <code>negative_prompt</code> input argument.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of new tokens to generate with the GPT2 language model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to None</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span><span class="w"> </span><span class="nn">scipy</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AudioLDM2Pipeline</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;cvssp/audioldm2&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pipe</span> <span class="o">=</span> <span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>


<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Get text embedding vectors</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">encode_prompt</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Techno music with a strong, upbeat tempo and high melodic riffs&quot;</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">do_classifier_free_guidance</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Pass text embeddings to pipeline for text-conditional audio generation</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">audio</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span>
<span class="o">...</span>     <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">generated_prompt_embeds</span><span class="o">=</span><span class="n">generated_prompt_embeds</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">num_inference_steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="o">...</span>     <span class="n">audio_length_in_s</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
<span class="o">...</span> <span class="p">)</span><span class="o">.</span><span class="n">audios</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># save generated audio sample</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">scipy</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">wavfile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;techno.wav&quot;</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">audio</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">encode_prompt</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">,</span>
    <span class="n">num_waveforms_per_prompt</span><span class="p">,</span>
    <span class="n">do_classifier_free_guidance</span><span class="p">,</span>
    <span class="n">transcription</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_prompt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_generated_prompt_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">negative_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes the prompt into text encoder hidden states.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">            prompt to be encoded</span>
<span class="sd">        transcription (`str` or `List[str]`):</span>
<span class="sd">            transcription of text to speech</span>
<span class="sd">        num_waveforms_per_prompt (`int`):</span>
<span class="sd">            number of waveforms that should be generated per prompt</span>
<span class="sd">        do_classifier_free_guidance (`bool`):</span>
<span class="sd">            whether to use classifier free guidance or not</span>
<span class="sd">        negative_prompt (`str` or `List[str]`, *optional*):</span>
<span class="sd">            The prompt or prompts not to guide the audio generation. If not defined, one has to pass</span>
<span class="sd">            `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is</span>
<span class="sd">            less than `1`).</span>
<span class="sd">        prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed text embeddings from the Flan T5 model. Can be used to easily tweak text inputs, *e.g.*</span>
<span class="sd">            prompt weighting. If not provided, text embeddings will be computed from `prompt` input argument.</span>
<span class="sd">        negative_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed negative text embeddings from the Flan T5 model. Can be used to easily tweak text inputs,</span>
<span class="sd">            *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">            `negative_prompt` input argument.</span>
<span class="sd">        generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated text embeddings from the GPT2 langauge model. Can be used to easily tweak text inputs,</span>
<span class="sd">             *e.g.* prompt weighting. If not provided, text embeddings will be generated from `prompt` input</span>
<span class="sd">             argument.</span>
<span class="sd">        negative_generated_prompt_embeds (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-generated negative text embeddings from the GPT2 language model. Can be used to easily tweak text</span>
<span class="sd">            inputs, *e.g.* prompt weighting. If not provided, negative_prompt_embeds will be computed from</span>
<span class="sd">            `negative_prompt` input argument.</span>
<span class="sd">        attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed attention mask to be applied to the `prompt_embeds`. If not provided, attention mask will</span>
<span class="sd">            be computed from `prompt` input argument.</span>
<span class="sd">        negative_attention_mask (`mindspore.tensor`, *optional*):</span>
<span class="sd">            Pre-computed attention mask to be applied to the `negative_prompt_embeds`. If not provided, attention</span>
<span class="sd">            mask will be computed from `negative_prompt` input argument.</span>
<span class="sd">        max_new_tokens (`int`, *optional*, defaults to None):</span>
<span class="sd">            The number of new tokens to generate with the GPT2 language model.</span>
<span class="sd">    Returns:</span>
<span class="sd">        prompt_embeds (`mindspore.tensor`):</span>
<span class="sd">            Text embeddings from the Flan T5 model.</span>
<span class="sd">        attention_mask (`mindspore.tensor`):</span>
<span class="sd">            Attention mask to be applied to the `prompt_embeds`.</span>
<span class="sd">        generated_prompt_embeds (`mindspore.tensor`):</span>
<span class="sd">            Text embeddings generated from the GPT2 langauge model.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; import scipy</span>
<span class="sd">    &gt;&gt;&gt; from mindone.diffusers import AudioLDM2Pipeline</span>

<span class="sd">    &gt;&gt;&gt; repo_id = &quot;cvssp/audioldm2&quot;</span>
<span class="sd">    &gt;&gt;&gt; pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=mindspore.float16)</span>


<span class="sd">    &gt;&gt;&gt; # Get text embedding vectors</span>
<span class="sd">    &gt;&gt;&gt; prompt_embeds, attention_mask, generated_prompt_embeds = pipe.encode_prompt(</span>
<span class="sd">    ...     prompt=&quot;Techno music with a strong, upbeat tempo and high melodic riffs&quot;,</span>
<span class="sd">    ...     do_classifier_free_guidance=True,</span>
<span class="sd">    ... )</span>

<span class="sd">    &gt;&gt;&gt; # Pass text embeddings to pipeline for text-conditional audio generation</span>
<span class="sd">    &gt;&gt;&gt; audio = pipe(</span>
<span class="sd">    ...     prompt_embeds=prompt_embeds,</span>
<span class="sd">    ...     attention_mask=attention_mask,</span>
<span class="sd">    ...     generated_prompt_embeds=generated_prompt_embeds,</span>
<span class="sd">    ...     num_inference_steps=200,</span>
<span class="sd">    ...     audio_length_in_s=10.0,</span>
<span class="sd">    ... ).audios[0]</span>

<span class="sd">    &gt;&gt;&gt; # save generated audio sample</span>
<span class="sd">    &gt;&gt;&gt; scipy.io.wavfile.write(&quot;techno.wav&quot;, rate=16000, data=audio)</span>
<span class="sd">    ```&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Define tokenizers and text encoders</span>
    <span class="n">tokenizers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_2</span><span class="p">]</span>
    <span class="n">is_vits_text_encoder</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span> <span class="n">VitsModel</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
        <span class="n">text_encoders</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">text_encoders</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">prompt_embeds_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attention_mask_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">text_encoder</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokenizers</span><span class="p">,</span> <span class="n">text_encoders</span><span class="p">):</span>
            <span class="n">use_prompt</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">T5Tokenizer</span><span class="p">,</span> <span class="n">T5TokenizerFast</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">text_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                <span class="n">prompt</span> <span class="k">if</span> <span class="n">use_prompt</span> <span class="k">else</span> <span class="n">transcription</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">VitsTokenizer</span><span class="p">))</span>
                <span class="k">else</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">text_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">text_inputs</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">untruncated_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;longest&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">untruncated_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">text_input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">mint</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span>
                <span class="n">text_input_ids</span><span class="p">,</span> <span class="n">untruncated_ids</span>
            <span class="p">):</span>
                <span class="n">removed_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">untruncated_ids</span><span class="p">[:,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The following part of your input was truncated because </span><span class="si">{</span><span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span><span class="si">}</span><span class="s2"> can &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;only handle sequences up to </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2"> tokens: </span><span class="si">{</span><span class="n">removed_text</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="n">text_input_ids</span> <span class="o">=</span> <span class="n">text_input_ids</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>

            <span class="k">if</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;clap&quot;</span><span class="p">:</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
                    <span class="n">text_input_ids</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># append the seq-len dim: (bs, hidden_size) -&gt; (bs, seq_len, hidden_size)</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                <span class="c1"># make sure that we attend to this single hidden-state</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
                <span class="c1"># Add end_token_id and attention mask in the end of sequence phonemes</span>
                <span class="k">for</span> <span class="n">text_input_id</span><span class="p">,</span> <span class="n">text_attention_mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">text_input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">phoneme_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">text_input_id</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">phoneme_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">text_input_id</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">182</span>
                            <span class="n">text_attention_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                            <span class="k">break</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                    <span class="n">text_input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">padding_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                    <span class="n">text_input_ids</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">prompt_embeds_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prompt_embeds</span><span class="p">)</span>
            <span class="n">attention_mask_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

        <span class="n">projection_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_model</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">prompt_embeds_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">hidden_states_1</span><span class="o">=</span><span class="n">prompt_embeds_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">attention_mask_1</span><span class="o">=</span><span class="n">attention_mask_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">projected_prompt_embeds</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">hidden_states</span>
        <span class="n">projected_attention_mask</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">attention_mask</span>

        <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_language_model</span><span class="p">(</span>
            <span class="n">projected_prompt_embeds</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">projected_attention_mask</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">attention_mask</span> <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">bs_embed</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># duplicate text embeddings for each generation per prompt, using mps friendly method</span>
    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># duplicate attention mask for each generation per prompt</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">))</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

    <span class="n">bs_embed</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># duplicate generated embeddings for each generation per prompt, using mps friendly method</span>
    <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">generated_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">bs_embed</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span>
    <span class="p">)</span>

    <span class="c1"># get unconditional embeddings for classifier free guidance</span>
    <span class="k">if</span> <span class="n">do_classifier_free_guidance</span> <span class="ow">and</span> <span class="n">negative_prompt_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uncond_tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">negative_prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`negative_prompt` should be the same type to `prompt`, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">)</span><span class="si">}</span><span class="s2"> !=&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">negative_prompt</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`negative_prompt`: </span><span class="si">{</span><span class="n">negative_prompt</span><span class="si">}</span><span class="s2"> has batch size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">negative_prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">, but `prompt`:&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2"> has batch size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">. Please make sure that passed `negative_prompt` matches&quot;</span>
                <span class="s2">&quot; the batch size of `prompt`.&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">uncond_tokens</span> <span class="o">=</span> <span class="n">negative_prompt</span>

        <span class="n">negative_prompt_embeds_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">negative_attention_mask_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">text_encoder</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">tokenizers</span><span class="p">,</span> <span class="n">text_encoders</span><span class="p">):</span>
            <span class="n">uncond_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                <span class="n">uncond_tokens</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="p">(</span><span class="n">RobertaTokenizer</span><span class="p">,</span> <span class="n">RobertaTokenizerFast</span><span class="p">,</span> <span class="n">VitsTokenizer</span><span class="p">))</span>
                <span class="k">else</span> <span class="n">max_length</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">uncond_input_ids</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">uncond_input</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
            <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">uncond_input</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;clap&quot;</span><span class="p">:</span>
                <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="o">.</span><span class="n">get_text_features</span><span class="p">(</span>
                    <span class="n">uncond_input_ids</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="c1"># append the seq-len dim: (bs, hidden_size) -&gt; (bs, seq_len, hidden_size)</span>
                <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
                <span class="c1"># make sure that we attend to this single hidden-state</span>
                <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">is_vits_text_encoder</span><span class="p">:</span>
                <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                    <span class="n">text_encoder</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span>
                    <span class="n">uncond_input_ids</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">negative_prompt_embeds_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">)</span>
            <span class="n">negative_attention_mask_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">negative_attention_mask</span><span class="p">)</span>

        <span class="n">projection_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_model</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">negative_prompt_embeds_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">hidden_states_1</span><span class="o">=</span><span class="n">negative_prompt_embeds_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_attention_mask_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">attention_mask_1</span><span class="o">=</span><span class="n">negative_attention_mask_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">negative_projected_prompt_embeds</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">hidden_states</span>
        <span class="n">negative_projected_attention_mask</span> <span class="o">=</span> <span class="n">projection_output</span><span class="o">.</span><span class="n">attention_mask</span>

        <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_language_model</span><span class="p">(</span>
            <span class="n">negative_projected_prompt_embeds</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">negative_projected_attention_mask</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">negative_attention_mask</span>
            <span class="k">if</span> <span class="n">negative_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># duplicate unconditional embeddings for each generation per prompt, using mps friendly method</span>
        <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">negative_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># duplicate unconditional attention mask for each generation per prompt</span>
        <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">))</span>
        <span class="n">negative_attention_mask</span> <span class="o">=</span> <span class="n">negative_attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

        <span class="c1"># duplicate unconditional generated embeddings for each generation per prompt</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">negative_generated_prompt_embeds</span> <span class="o">=</span> <span class="n">negative_generated_prompt_embeds</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_waveforms_per_prompt</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="c1"># For classifier free guidance, we need to do two forward passes.</span>
        <span class="c1"># Here we concatenate the unconditional and text embeddings into a single batch</span>
        <span class="c1"># to avoid doing two forward passes</span>
        <span class="n">prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_prompt_embeds</span><span class="p">,</span> <span class="n">prompt_embeds</span><span class="p">])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>
        <span class="n">generated_prompt_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">negative_generated_prompt_embeds</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">prompt_embeds</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">generated_prompt_embeds</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2Pipeline.generate_language_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2Pipeline</span><span class="o">.</span><span class="n">generate_language_model</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2Pipeline.generate_language_model" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Generates a sequence of hidden-states from the language model, conditioned on the embedding inputs.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The sequence used as a prompt for the generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor` of shape `(batch_size, sequence_length, hidden_size)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of new tokens to generate.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Ad hoc parametrization of additional model-specific kwargs that will be forwarded to the <code>forward</code>
function of the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, Any]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="return" open>
  <summary>Return</summary>
  <p><code>inputs_embeds (</code>mindspore.tensor<code>of shape</code>(batch_size, sequence_length, hidden_size)`):
    The sequence of generated hidden-states.</p>
</details>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/pipeline_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">generate_language_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Generates a sequence of hidden-states from the language model, conditioned on the embedding inputs.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        inputs_embeds (`mindspore.tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            The sequence used as a prompt for the generation.</span>
<span class="sd">        max_new_tokens (`int`):</span>
<span class="sd">            Number of new tokens to generate.</span>
<span class="sd">        model_kwargs (`Dict[str, Any]`, *optional*):</span>
<span class="sd">            Ad hoc parametrization of additional model-specific kwargs that will be forwarded to the `forward`</span>
<span class="sd">            function of the model.</span>

<span class="sd">    Return:</span>
<span class="sd">        `inputs_embeds (`mindspore.tensor` of shape `(batch_size, sequence_length, hidden_size)`):</span>
<span class="sd">            The sequence of generated hidden-states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_supports_dynamic_input</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="n">max_new_tokens</span> <span class="k">if</span> <span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_new_tokens</span>
    <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_get_initial_cache_position</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="c1"># prepare model inputs</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>

        <span class="c1"># forward pass to get next hidden states</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">next_hidden_states</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span>

        <span class="c1"># Update the model input</span>
        <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">inputs_embeds</span><span class="p">,</span> <span class="n">next_hidden_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Update generated hidden states, model inputs, and length for next step</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">_update_model_kwargs_for_generation</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_embeds</span><span class="p">[:,</span> <span class="o">-</span><span class="n">max_new_tokens</span><span class="p">:,</span> <span class="p">:]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.AudioLDM2ProjectionModel" class="doc doc-heading">
            <code>mindone.diffusers.AudioLDM2ProjectionModel</code>


<a href="#mindone.diffusers.AudioLDM2ProjectionModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.ModelMixin (mindone.diffusers.models.modeling_utils.ModelMixin)" href="../../models/overview/#mindone.diffusers.ModelMixin">ModelMixin</a></code>, <code><a class="autorefs autorefs-internal" title="mindone.diffusers.configuration_utils.ConfigMixin" href="../../configuration/#mindone.diffusers.configuration_utils.ConfigMixin">ConfigMixin</a></code></p>


        <p>A simple linear projection model to map two text embeddings to a shared latent space. It also inserts learned
embedding vectors at the start and end of each text embedding sequence respectively. Each variable appended with
<code>_1</code> refers to that corresponding to the second text encoder. Otherwise, it is from the first.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the text embeddings from the first text encoder (CLAP).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_1_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the text embeddings from the second text encoder (T5 or VITS).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>langauge_model_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Dimensionality of the text embeddings from the language model (GPT2).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AudioLDM2ProjectionModel</span><span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">ConfigMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple linear projection model to map two text embeddings to a shared latent space. It also inserts learned</span>
<span class="sd">    embedding vectors at the start and end of each text embedding sequence respectively. Each variable appended with</span>
<span class="sd">    `_1` refers to that corresponding to the second text encoder. Otherwise, it is from the first.</span>

<span class="sd">    Args:</span>
<span class="sd">        text_encoder_dim (`int`):</span>
<span class="sd">            Dimensionality of the text embeddings from the first text encoder (CLAP).</span>
<span class="sd">        text_encoder_1_dim (`int`):</span>
<span class="sd">            Dimensionality of the text embeddings from the second text encoder (T5 or VITS).</span>
<span class="sd">        langauge_model_dim (`int`):</span>
<span class="sd">            Dimensionality of the text embeddings from the language model (GPT2).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@register_to_config</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text_encoder_dim</span><span class="p">,</span>
        <span class="n">text_encoder_1_dim</span><span class="p">,</span>
        <span class="n">langauge_model_dim</span><span class="p">,</span>
        <span class="n">use_learned_position_embedding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">max_seq_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># additional projection layers for each text encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">text_encoder_dim</span><span class="p">,</span> <span class="n">langauge_model_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection_1</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">text_encoder_1_dim</span><span class="p">,</span> <span class="n">langauge_model_dim</span><span class="p">)</span>

        <span class="c1"># learnable SOS / EOS token embeddings for each text encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sos_embed</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">langauge_model_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_embed</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">langauge_model_dim</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sos_embed_1</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">langauge_model_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eos_embed_1</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">langauge_model_dim</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_learned_position_embedding</span> <span class="o">=</span> <span class="n">use_learned_position_embedding</span>

        <span class="c1"># learable positional embedding for vits encoder</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_learned_position_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learnable_positional_embedding</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">text_encoder_1_dim</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">)))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hidden_states_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">add_special_tokens</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">sos_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sos_embed</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_embed</span>
        <span class="p">)</span>

        <span class="c1"># Add positional embedding for Vits hidden state</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_learned_position_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states_1</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">learnable_positional_embedding</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">hidden_states_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_1</span><span class="p">(</span><span class="n">hidden_states_1</span><span class="p">)</span>
        <span class="n">hidden_states_1</span><span class="p">,</span> <span class="n">attention_mask_1</span> <span class="o">=</span> <span class="n">add_special_tokens</span><span class="p">(</span>
            <span class="n">hidden_states_1</span><span class="p">,</span> <span class="n">attention_mask_1</span><span class="p">,</span> <span class="n">sos_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sos_embed_1</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_embed_1</span>
        <span class="p">)</span>

        <span class="c1"># concatenate clap and t5 text encoding</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_states_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># concatenate attention masks</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attention_mask_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask_1</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">hidden_states</span><span class="p">[:</span><span class="mi">2</span><span class="p">]))</span>
        <span class="k">elif</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attention_mask_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask_1</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="n">hidden_states_1</span><span class="p">[:</span><span class="mi">2</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">attention_mask_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">attention_mask_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">AudioLDM2ProjectionModelOutput</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.AudioLDM2UNet2DConditionModel" class="doc doc-heading">
            <code>mindone.diffusers.AudioLDM2UNet2DConditionModel</code>


<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.ModelMixin (mindone.diffusers.models.modeling_utils.ModelMixin)" href="../../models/overview/#mindone.diffusers.ModelMixin">ModelMixin</a></code>, <code><a class="autorefs autorefs-internal" title="mindone.diffusers.configuration_utils.ConfigMixin" href="../../configuration/#mindone.diffusers.configuration_utils.ConfigMixin">ConfigMixin</a></code>, <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.unet.UNet2DConditionLoadersMixin (mindone.diffusers.loaders.UNet2DConditionLoadersMixin)" href="../../loaders/unet/#mindone.diffusers.loaders.unet.UNet2DConditionLoadersMixin">UNet2DConditionLoadersMixin</a></code></p>


        <p>A conditional 2D UNet model that takes a noisy sample, conditional state, and a timestep and returns a sample
shaped output. Compared to the vanilla [<code>UNet2DConditionModel</code>], this variant optionally includes an additional
self-attention layer in each Transformer block, as well as multiple cross-attention layers. It also allows for up
to two cross-attention embeddings, <code>encoder_hidden_states</code> and <code>encoder_hidden_states_1</code>.</p>
<p>This model inherits from [<code>ModelMixin</code>]. Check the superclass documentation for it's generic methods implemented
for all models (such as downloading or saving).</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Height and width of input/output sample.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int` or `Tuple[int, int]`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of channels in the input sample.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of channels in the output.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 4</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>4</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>flip_sin_to_cos</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to flip the sin to cos in the time embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>freq_shift</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The frequency shift to apply to the time embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>down_block_types</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tuple of downsample blocks to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[str]`, *optional*, defaults to `(&#34;CrossAttnDownBlock2D&#34;, &#34;CrossAttnDownBlock2D&#34;, &#34;CrossAttnDownBlock2D&#34;, &#34;DownBlock2D&#34;)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(&#39;CrossAttnDownBlock2D&#39;, &#39;CrossAttnDownBlock2D&#39;, &#39;CrossAttnDownBlock2D&#39;, &#39;DownBlock2D&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mid_block_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Block type for middle of UNet, it can only be <code>UNetMidBlock2DCrossAttn</code> for AudioLDM2.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;UNetMidBlock2DCrossAttn&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;UNetMidBlock2DCrossAttn&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>up_block_types</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tuple of upsample blocks to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[str]`, *optional*, defaults to `(&#34;UpBlock2D&#34;, &#34;CrossAttnUpBlock2D&#34;, &#34;CrossAttnUpBlock2D&#34;, &#34;CrossAttnUpBlock2D&#34;)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(&#39;UpBlock2D&#39;, &#39;CrossAttnUpBlock2D&#39;, &#39;CrossAttnUpBlock2D&#39;, &#39;CrossAttnUpBlock2D&#39;)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>only_cross_attention</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to include self-attention in the basic transformer blocks, see
[<code>~models.attention.BasicTransformerBlock</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool` or `Tuple[bool]`, *optional*, default to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>block_out_channels</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The tuple of output channels for each block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(320, 640, 1280, 1280)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layers_per_block</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of layers per block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 2</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>downsample_padding</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The padding to use for the downsampling convolution.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>mid_block_scale_factor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The scale factor to use for the mid block.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>act_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The activation function to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;silu&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;silu&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>norm_num_groups</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of groups to use for the normalization.
If <code>None</code>, normalization and activation layers is skipped in post-processing.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 32</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>32</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>norm_eps</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The epsilon to use for the normalization.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 1e-5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1e-05</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the cross attention features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int` or `Tuple[int]`, *optional*, defaults to 1280</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_layers_per_block</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of transformer blocks of type [<code>~models.attention.BasicTransformerBlock</code>]. Only relevant for
[<code>~models.unet_2d_blocks.CrossAttnDownBlock2D</code>], [<code>~models.unet_2d_blocks.CrossAttnUpBlock2D</code>],
[<code>~models.unet_2d_blocks.UNetMidBlock2DCrossAttn</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int` or `Tuple[int]`, *optional*, defaults to 1</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_head_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the attention heads.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to 8</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>8</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_attention_heads</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of attention heads. If not defined, defaults to <code>attention_head_dim</code></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>resnet_time_scale_shift</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Time scale shift config
for ResNet blocks (see [<code>~models.resnet.ResnetBlock2D</code>]). Choose from <code>default</code> or <code>scale_shift</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;default&#34;`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;default&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>class_embed_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The type of class embedding to use which is ultimately summed with the time embeddings. Choose from <code>None</code>,
<code>"timestep"</code>, <code>"identity"</code>, <code>"projection"</code>, or <code>"simple_projection"</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_class_embeds</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input dimension of the learnable embedding matrix to be projected to <code>time_embed_dim</code>, when performing
class conditioning with <code>class_embed_type</code> equal to <code>None</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_embedding_type</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The type of position embedding to use for timesteps. Choose from <code>positional</code> or <code>fourier</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `positional`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>&#39;positional&#39;</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_embedding_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>An optional override for the dimension of the projected time embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_embedding_act_fn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional activation function to use only once on the time embeddings before they are passed to the rest of
the UNet. Choose from <code>silu</code>, <code>mish</code>, <code>gelu</code>, and <code>swish</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>timestep_post_act</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The second activation function to use in timestep embedding. Choose from <code>silu</code>, <code>mish</code> and <code>gelu</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>time_cond_proj_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of <code>cond_proj</code> layer in the timestep embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_in_kernel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size of <code>conv_in</code> layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, default to `3`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>conv_out_kernel</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The kernel size of <code>conv_out</code> layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, default to `3`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>3</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>projection_class_embeddings_input_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dimension of the <code>class_labels</code> input when
<code>class_embed_type="projection"</code>. Required when <code>class_embed_type="projection"</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>class_embeddings_concat</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to concatenate the time
embeddings with the class embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AudioLDM2UNet2DConditionModel</span><span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">ConfigMixin</span><span class="p">,</span> <span class="n">UNet2DConditionLoadersMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A conditional 2D UNet model that takes a noisy sample, conditional state, and a timestep and returns a sample</span>
<span class="sd">    shaped output. Compared to the vanilla [`UNet2DConditionModel`], this variant optionally includes an additional</span>
<span class="sd">    self-attention layer in each Transformer block, as well as multiple cross-attention layers. It also allows for up</span>
<span class="sd">    to two cross-attention embeddings, `encoder_hidden_states` and `encoder_hidden_states_1`.</span>

<span class="sd">    This model inherits from [`ModelMixin`]. Check the superclass documentation for it&#39;s generic methods implemented</span>
<span class="sd">    for all models (such as downloading or saving).</span>

<span class="sd">    Parameters:</span>
<span class="sd">        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):</span>
<span class="sd">            Height and width of input/output sample.</span>
<span class="sd">        in_channels (`int`, *optional*, defaults to 4): Number of channels in the input sample.</span>
<span class="sd">        out_channels (`int`, *optional*, defaults to 4): Number of channels in the output.</span>
<span class="sd">        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to flip the sin to cos in the time embedding.</span>
<span class="sd">        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.</span>
<span class="sd">        down_block_types (`Tuple[str]`, *optional*, defaults to `(&quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;DownBlock2D&quot;)`):</span>
<span class="sd">            The tuple of downsample blocks to use.</span>
<span class="sd">        mid_block_type (`str`, *optional*, defaults to `&quot;UNetMidBlock2DCrossAttn&quot;`):</span>
<span class="sd">            Block type for middle of UNet, it can only be `UNetMidBlock2DCrossAttn` for AudioLDM2.</span>
<span class="sd">        up_block_types (`Tuple[str]`, *optional*, defaults to `(&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;)`):</span>
<span class="sd">            The tuple of upsample blocks to use.</span>
<span class="sd">        only_cross_attention (`bool` or `Tuple[bool]`, *optional*, default to `False`):</span>
<span class="sd">            Whether to include self-attention in the basic transformer blocks, see</span>
<span class="sd">            [`~models.attention.BasicTransformerBlock`].</span>
<span class="sd">        block_out_channels (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`):</span>
<span class="sd">            The tuple of output channels for each block.</span>
<span class="sd">        layers_per_block (`int`, *optional*, defaults to 2): The number of layers per block.</span>
<span class="sd">        downsample_padding (`int`, *optional*, defaults to 1): The padding to use for the downsampling convolution.</span>
<span class="sd">        mid_block_scale_factor (`float`, *optional*, defaults to 1.0): The scale factor to use for the mid block.</span>
<span class="sd">        act_fn (`str`, *optional*, defaults to `&quot;silu&quot;`): The activation function to use.</span>
<span class="sd">        norm_num_groups (`int`, *optional*, defaults to 32): The number of groups to use for the normalization.</span>
<span class="sd">            If `None`, normalization and activation layers is skipped in post-processing.</span>
<span class="sd">        norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon to use for the normalization.</span>
<span class="sd">        cross_attention_dim (`int` or `Tuple[int]`, *optional*, defaults to 1280):</span>
<span class="sd">            The dimension of the cross attention features.</span>
<span class="sd">        transformer_layers_per_block (`int` or `Tuple[int]`, *optional*, defaults to 1):</span>
<span class="sd">            The number of transformer blocks of type [`~models.attention.BasicTransformerBlock`]. Only relevant for</span>
<span class="sd">            [`~models.unet_2d_blocks.CrossAttnDownBlock2D`], [`~models.unet_2d_blocks.CrossAttnUpBlock2D`],</span>
<span class="sd">            [`~models.unet_2d_blocks.UNetMidBlock2DCrossAttn`].</span>
<span class="sd">        attention_head_dim (`int`, *optional*, defaults to 8): The dimension of the attention heads.</span>
<span class="sd">        num_attention_heads (`int`, *optional*):</span>
<span class="sd">            The number of attention heads. If not defined, defaults to `attention_head_dim`</span>
<span class="sd">        resnet_time_scale_shift (`str`, *optional*, defaults to `&quot;default&quot;`): Time scale shift config</span>
<span class="sd">            for ResNet blocks (see [`~models.resnet.ResnetBlock2D`]). Choose from `default` or `scale_shift`.</span>
<span class="sd">        class_embed_type (`str`, *optional*, defaults to `None`):</span>
<span class="sd">            The type of class embedding to use which is ultimately summed with the time embeddings. Choose from `None`,</span>
<span class="sd">            `&quot;timestep&quot;`, `&quot;identity&quot;`, `&quot;projection&quot;`, or `&quot;simple_projection&quot;`.</span>
<span class="sd">        num_class_embeds (`int`, *optional*, defaults to `None`):</span>
<span class="sd">            Input dimension of the learnable embedding matrix to be projected to `time_embed_dim`, when performing</span>
<span class="sd">            class conditioning with `class_embed_type` equal to `None`.</span>
<span class="sd">        time_embedding_type (`str`, *optional*, defaults to `positional`):</span>
<span class="sd">            The type of position embedding to use for timesteps. Choose from `positional` or `fourier`.</span>
<span class="sd">        time_embedding_dim (`int`, *optional*, defaults to `None`):</span>
<span class="sd">            An optional override for the dimension of the projected time embedding.</span>
<span class="sd">        time_embedding_act_fn (`str`, *optional*, defaults to `None`):</span>
<span class="sd">            Optional activation function to use only once on the time embeddings before they are passed to the rest of</span>
<span class="sd">            the UNet. Choose from `silu`, `mish`, `gelu`, and `swish`.</span>
<span class="sd">        timestep_post_act (`str`, *optional*, defaults to `None`):</span>
<span class="sd">            The second activation function to use in timestep embedding. Choose from `silu`, `mish` and `gelu`.</span>
<span class="sd">        time_cond_proj_dim (`int`, *optional*, defaults to `None`):</span>
<span class="sd">            The dimension of `cond_proj` layer in the timestep embedding.</span>
<span class="sd">        conv_in_kernel (`int`, *optional*, default to `3`): The kernel size of `conv_in` layer.</span>
<span class="sd">        conv_out_kernel (`int`, *optional*, default to `3`): The kernel size of `conv_out` layer.</span>
<span class="sd">        projection_class_embeddings_input_dim (`int`, *optional*): The dimension of the `class_labels` input when</span>
<span class="sd">            `class_embed_type=&quot;projection&quot;`. Required when `class_embed_type=&quot;projection&quot;`.</span>
<span class="sd">        class_embeddings_concat (`bool`, *optional*, defaults to `False`): Whether to concatenate the time</span>
<span class="sd">            embeddings with the class embeddings.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_supports_gradient_checkpointing</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@register_to_config</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">flip_sin_to_cos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">freq_shift</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">down_block_types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;CrossAttnDownBlock2D&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CrossAttnDownBlock2D&quot;</span><span class="p">,</span>
            <span class="s2">&quot;CrossAttnDownBlock2D&quot;</span><span class="p">,</span>
            <span class="s2">&quot;DownBlock2D&quot;</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">mid_block_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;UNetMidBlock2DCrossAttn&quot;</span><span class="p">,</span>
        <span class="n">up_block_types</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;UpBlock2D&quot;</span><span class="p">,</span> <span class="s2">&quot;CrossAttnUpBlock2D&quot;</span><span class="p">,</span> <span class="s2">&quot;CrossAttnUpBlock2D&quot;</span><span class="p">,</span> <span class="s2">&quot;CrossAttnUpBlock2D&quot;</span><span class="p">),</span>
        <span class="n">only_cross_attention</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">block_out_channels</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">320</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">1280</span><span class="p">,</span> <span class="mi">1280</span><span class="p">),</span>
        <span class="n">layers_per_block</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">downsample_padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">mid_block_scale_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">act_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;silu&quot;</span><span class="p">,</span>
        <span class="n">norm_num_groups</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">cross_attention_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1280</span><span class="p">,</span>
        <span class="n">transformer_layers_per_block</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">attention_head_dim</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_linear_projection</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">class_embed_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_class_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">upcast_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">resnet_time_scale_shift</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">time_embedding_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;positional&quot;</span><span class="p">,</span>
        <span class="n">time_embedding_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">time_embedding_act_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep_post_act</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">time_cond_proj_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">conv_in_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">conv_out_kernel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">projection_class_embeddings_input_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">class_embeddings_concat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sample_size</span> <span class="o">=</span> <span class="n">sample_size</span>

        <span class="k">if</span> <span class="n">num_attention_heads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;At the moment it is not possible to define the number of attention heads via `num_attention_heads` because of a naming issue </span><span class="se">\</span>
<span class="s2">                as described in https://github.com/huggingface/diffusers/issues/2011#issuecomment-1547958131. </span><span class="se">\</span>
<span class="s2">                Passing `num_attention_heads` will only be supported in diffusers v0.19.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># If `num_attention_heads` is not defined (which is the case for most models)</span>
        <span class="c1"># it will default to `attention_head_dim`. This looks weird upon first reading it and it is.</span>
        <span class="c1"># The reason for this behavior is to correct for incorrectly named variables that were introduced</span>
        <span class="c1"># Changing `attention_head_dim` to `num_attention_heads` for 40,000+ configurations is too backwards breaking</span>
        <span class="c1"># which is why we correct for the naming here.</span>
        <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span> <span class="ow">or</span> <span class="n">attention_head_dim</span>

        <span class="c1"># Check inputs</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">up_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `down_block_types` as `up_block_types`. `down_block_types`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">. `up_block_types`: </span><span class="si">{</span><span class="n">up_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`:</span><span class="se">\</span>
<span class="s2">                      </span><span class="si">{</span><span class="n">block_out_channels</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">only_cross_attention</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">only_cross_attention</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `only_cross_attention` as `down_block_types`. `only_cross_attention`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">only_cross_attention</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `num_attention_heads` as `down_block_types`. `num_attention_heads`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_head_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">attention_head_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `attention_head_dim` as `down_block_types`. `attention_head_dim`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">attention_head_dim</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `cross_attention_dim` as `down_block_types`. `cross_attention_dim`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">cross_attention_dim</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers_per_block</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_per_block</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Must provide the same number of `layers_per_block` as `down_block_types`. `layers_per_block`: </span><span class="se">\</span>
<span class="s2">                    </span><span class="si">{</span><span class="n">layers_per_block</span><span class="si">}</span><span class="s2">. `down_block_types`: </span><span class="si">{</span><span class="n">down_block_types</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># input</span>
        <span class="n">conv_in_padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_in_kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_in_kernel</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">conv_in_padding</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># time</span>
        <span class="k">if</span> <span class="n">time_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;positional&quot;</span><span class="p">:</span>
            <span class="n">time_embed_dim</span> <span class="o">=</span> <span class="n">time_embedding_dim</span> <span class="ow">or</span> <span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span> <span class="o">=</span> <span class="n">Timesteps</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">flip_sin_to_cos</span><span class="p">,</span> <span class="n">freq_shift</span><span class="p">)</span>
            <span class="n">timestep_input_dim</span> <span class="o">=</span> <span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">time_embedding_type</span><span class="si">}</span><span class="s2"> does not exist. Please make sure to use `positional`.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">time_embedding</span> <span class="o">=</span> <span class="n">TimestepEmbedding</span><span class="p">(</span>
            <span class="n">timestep_input_dim</span><span class="p">,</span>
            <span class="n">time_embed_dim</span><span class="p">,</span>
            <span class="n">act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">,</span>
            <span class="n">post_act_fn</span><span class="o">=</span><span class="n">timestep_post_act</span><span class="p">,</span>
            <span class="n">cond_proj_dim</span><span class="o">=</span><span class="n">time_cond_proj_dim</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># class embedding</span>
        <span class="k">if</span> <span class="n">class_embed_type</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_class_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_class_embeds</span><span class="p">,</span> <span class="n">time_embed_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;timestep&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">TimestepEmbedding</span><span class="p">(</span><span class="n">timestep_input_dim</span><span class="p">,</span> <span class="n">time_embed_dim</span><span class="p">,</span> <span class="n">act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;identity&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(</span><span class="n">time_embed_dim</span><span class="p">,</span> <span class="n">time_embed_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;projection&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">projection_class_embeddings_input_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`class_embed_type`: &#39;projection&#39; requires `projection_class_embeddings_input_dim` be set&quot;</span>
                <span class="p">)</span>
            <span class="c1"># The projection `class_embed_type` is the same as the timestep `class_embed_type` except</span>
            <span class="c1"># 1. the `class_labels` inputs are not first converted to sinusoidal embeddings</span>
            <span class="c1"># 2. it projects from an arbitrary input dimension.</span>
            <span class="c1">#</span>
            <span class="c1"># Note that `TimestepEmbedding` is quite general, being mainly linear layers and activations.</span>
            <span class="c1"># When used for embedding actual timesteps, the timesteps are first converted to sinusoidal embeddings.</span>
            <span class="c1"># As a result, `TimestepEmbedding` can be passed arbitrary vectors.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">TimestepEmbedding</span><span class="p">(</span><span class="n">projection_class_embeddings_input_dim</span><span class="p">,</span> <span class="n">time_embed_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;simple_projection&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">projection_class_embeddings_input_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`class_embed_type`: &#39;simple_projection&#39; requires `projection_class_embeddings_input_dim` be set&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">projection_class_embeddings_input_dim</span><span class="p">,</span> <span class="n">time_embed_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">time_embedding_act_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">time_embedding_act_fn</span><span class="p">)</span>

        <span class="n">down_blocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">up_blocks</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">only_cross_attention</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="n">only_cross_attention</span> <span class="o">=</span> <span class="p">[</span><span class="n">only_cross_attention</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_attention_heads</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">cross_attention_dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers_per_block</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">layers_per_block</span> <span class="o">=</span> <span class="p">[</span><span class="n">layers_per_block</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformer_layers_per_block</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">transformer_layers_per_block</span> <span class="o">=</span> <span class="p">[</span><span class="n">transformer_layers_per_block</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">class_embeddings_concat</span><span class="p">:</span>
            <span class="c1"># The time embeddings are concatenated with the class embeddings. The dimension of the</span>
            <span class="c1"># time embeddings passed to the down, middle, and up blocks is twice the dimension of the</span>
            <span class="c1"># regular time embeddings</span>
            <span class="n">blocks_time_embed_dim</span> <span class="o">=</span> <span class="n">time_embed_dim</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">blocks_time_embed_dim</span> <span class="o">=</span> <span class="n">time_embed_dim</span>

        <span class="c1"># down</span>
        <span class="n">output_channel</span> <span class="o">=</span> <span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">down_block_type</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">down_block_types</span><span class="p">):</span>
            <span class="n">input_channel</span> <span class="o">=</span> <span class="n">output_channel</span>
            <span class="n">output_channel</span> <span class="o">=</span> <span class="n">block_out_channels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">is_final_block</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="n">down_block</span> <span class="o">=</span> <span class="n">get_down_block</span><span class="p">(</span>
                <span class="n">down_block_type</span><span class="p">,</span>
                <span class="n">num_layers</span><span class="o">=</span><span class="n">layers_per_block</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">transformer_layers_per_block</span><span class="o">=</span><span class="n">transformer_layers_per_block</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channel</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channel</span><span class="p">,</span>
                <span class="n">temb_channels</span><span class="o">=</span><span class="n">blocks_time_embed_dim</span><span class="p">,</span>
                <span class="n">add_downsample</span><span class="o">=</span><span class="ow">not</span> <span class="n">is_final_block</span><span class="p">,</span>
                <span class="n">resnet_eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span>
                <span class="n">resnet_act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">,</span>
                <span class="n">resnet_groups</span><span class="o">=</span><span class="n">norm_num_groups</span><span class="p">,</span>
                <span class="n">cross_attention_dim</span><span class="o">=</span><span class="n">cross_attention_dim</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_attention_heads</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">downsample_padding</span><span class="o">=</span><span class="n">downsample_padding</span><span class="p">,</span>
                <span class="n">use_linear_projection</span><span class="o">=</span><span class="n">use_linear_projection</span><span class="p">,</span>
                <span class="n">only_cross_attention</span><span class="o">=</span><span class="n">only_cross_attention</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">upcast_attention</span><span class="o">=</span><span class="n">upcast_attention</span><span class="p">,</span>
                <span class="n">resnet_time_scale_shift</span><span class="o">=</span><span class="n">resnet_time_scale_shift</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">down_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">down_block</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span><span class="n">down_blocks</span><span class="p">)</span>

        <span class="c1"># mid</span>
        <span class="k">if</span> <span class="n">mid_block_type</span> <span class="o">==</span> <span class="s2">&quot;UNetMidBlock2DCrossAttn&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span> <span class="o">=</span> <span class="n">UNetMidBlock2DCrossAttn</span><span class="p">(</span>
                <span class="n">transformer_layers_per_block</span><span class="o">=</span><span class="n">transformer_layers_per_block</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">block_out_channels</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">temb_channels</span><span class="o">=</span><span class="n">blocks_time_embed_dim</span><span class="p">,</span>
                <span class="n">resnet_eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span>
                <span class="n">resnet_act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">,</span>
                <span class="n">output_scale_factor</span><span class="o">=</span><span class="n">mid_block_scale_factor</span><span class="p">,</span>
                <span class="n">resnet_time_scale_shift</span><span class="o">=</span><span class="n">resnet_time_scale_shift</span><span class="p">,</span>
                <span class="n">cross_attention_dim</span><span class="o">=</span><span class="n">cross_attention_dim</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_attention_heads</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">resnet_groups</span><span class="o">=</span><span class="n">norm_num_groups</span><span class="p">,</span>
                <span class="n">use_linear_projection</span><span class="o">=</span><span class="n">use_linear_projection</span><span class="p">,</span>
                <span class="n">upcast_attention</span><span class="o">=</span><span class="n">upcast_attention</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;unknown mid_block_type : </span><span class="si">{</span><span class="n">mid_block_type</span><span class="si">}</span><span class="s2">. Should be `UNetMidBlock2DCrossAttn` for AudioLDM2.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># count how many layers upsample the images</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_upsamplers</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># up</span>
        <span class="n">reversed_block_out_channels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">))</span>
        <span class="n">reversed_num_attention_heads</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">))</span>
        <span class="n">reversed_layers_per_block</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">layers_per_block</span><span class="p">))</span>
        <span class="n">reversed_cross_attention_dim</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">))</span>
        <span class="n">reversed_transformer_layers_per_block</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">transformer_layers_per_block</span><span class="p">))</span>
        <span class="n">only_cross_attention</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">only_cross_attention</span><span class="p">))</span>

        <span class="n">output_channel</span> <span class="o">=</span> <span class="n">reversed_block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">up_block_type</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">up_block_types</span><span class="p">):</span>
            <span class="n">is_final_block</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="n">prev_output_channel</span> <span class="o">=</span> <span class="n">output_channel</span>
            <span class="n">output_channel</span> <span class="o">=</span> <span class="n">reversed_block_out_channels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">input_channel</span> <span class="o">=</span> <span class="n">reversed_block_out_channels</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_out_channels</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="c1"># add upsample block for all BUT final layer</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_final_block</span><span class="p">:</span>
                <span class="n">add_upsample</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_upsamplers</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">add_upsample</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="n">up_block</span> <span class="o">=</span> <span class="n">get_up_block</span><span class="p">(</span>
                <span class="n">up_block_type</span><span class="p">,</span>
                <span class="n">num_layers</span><span class="o">=</span><span class="n">reversed_layers_per_block</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">transformer_layers_per_block</span><span class="o">=</span><span class="n">reversed_transformer_layers_per_block</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">in_channels</span><span class="o">=</span><span class="n">input_channel</span><span class="p">,</span>
                <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channel</span><span class="p">,</span>
                <span class="n">prev_output_channel</span><span class="o">=</span><span class="n">prev_output_channel</span><span class="p">,</span>
                <span class="n">temb_channels</span><span class="o">=</span><span class="n">blocks_time_embed_dim</span><span class="p">,</span>
                <span class="n">add_upsample</span><span class="o">=</span><span class="n">add_upsample</span><span class="p">,</span>
                <span class="n">resnet_eps</span><span class="o">=</span><span class="n">norm_eps</span><span class="p">,</span>
                <span class="n">resnet_act_fn</span><span class="o">=</span><span class="n">act_fn</span><span class="p">,</span>
                <span class="n">resnet_groups</span><span class="o">=</span><span class="n">norm_num_groups</span><span class="p">,</span>
                <span class="n">cross_attention_dim</span><span class="o">=</span><span class="n">reversed_cross_attention_dim</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">reversed_num_attention_heads</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">use_linear_projection</span><span class="o">=</span><span class="n">use_linear_projection</span><span class="p">,</span>
                <span class="n">only_cross_attention</span><span class="o">=</span><span class="n">only_cross_attention</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">upcast_attention</span><span class="o">=</span><span class="n">upcast_attention</span><span class="p">,</span>
                <span class="n">resnet_time_scale_shift</span><span class="o">=</span><span class="n">resnet_time_scale_shift</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">up_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">up_block</span><span class="p">)</span>
            <span class="n">prev_output_channel</span> <span class="o">=</span> <span class="n">output_channel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span><span class="n">up_blocks</span><span class="p">)</span>

        <span class="c1"># out</span>
        <span class="k">if</span> <span class="n">norm_num_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span>
                <span class="n">num_channels</span><span class="o">=</span><span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_groups</span><span class="o">=</span><span class="n">norm_num_groups</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">norm_eps</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">conv_act</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">act_fn</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_act</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">conv_out_padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_out_kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">block_out_channels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">conv_out_kernel</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">conv_out_padding</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="c1"># Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">attn_processors</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AttentionProcessor</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            `dict` of attention processors: A dictionary containing all attention processors used in the model with</span>
<span class="sd">            indexed by its weight name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># set recursively</span>
        <span class="n">processors</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_add_processors</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">processors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AttentionProcessor</span><span class="p">]):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;get_processor&quot;</span><span class="p">):</span>
                <span class="n">processors</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.processor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_processor</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">sub_name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
                <span class="n">fn_recursive_add_processors</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">sub_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">processors</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">processors</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">fn_recursive_add_processors</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">processors</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">processors</span>

    <span class="c1"># Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_attn_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">processor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AttentionProcessor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AttentionProcessor</span><span class="p">]]):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the attention processor to use to compute attention.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):</span>
<span class="sd">                The instantiated processor class or a dictionary of processor classes that will be set as the processor</span>
<span class="sd">                for **all** `Attention` layers.</span>

<span class="sd">                If `processor` is a dict, the key needs to define the path to the corresponding cross attention</span>
<span class="sd">                processor. This is strongly recommended when setting trainable attention processors.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">count</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;A dict of processors was passed, but the number of processors </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not match the&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; number of attention layers: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">. Please make sure to pass </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2"> processor classes.&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_attn_processor</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">processor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_processor&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">set_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">module</span><span class="o">.</span><span class="n">set_processor</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.processor&quot;</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">sub_name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
                <span class="n">fn_recursive_attn_processor</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">sub_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">fn_recursive_attn_processor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>

    <span class="c1"># Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_default_attn_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disables custom attention processors and sets the default attention implementation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">proc</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="n">ADDED_KV_ATTENTION_PROCESSORS</span> <span class="k">for</span> <span class="n">proc</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">processor</span> <span class="o">=</span> <span class="n">AttnAddedKVProcessor</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="n">proc</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="n">CROSS_ATTENTION_PROCESSORS</span> <span class="k">for</span> <span class="n">proc</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="n">processor</span> <span class="o">=</span> <span class="n">AttnProcessor</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot call `set_default_attn_processor` when attention processors are of type </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_attn_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>

    <span class="c1"># Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attention_slice</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_attention_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enable sliced attention computation.</span>

<span class="sd">        When this option is enabled, the attention module splits the input tensor in slices to compute attention in</span>
<span class="sd">        several steps. This is useful for saving some memory in exchange for a small decrease in speed.</span>

<span class="sd">        Args:</span>
<span class="sd">            slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `&quot;auto&quot;`):</span>
<span class="sd">                When `&quot;auto&quot;`, input to the attention heads is halved, so attention is computed in two steps. If</span>
<span class="sd">                `&quot;max&quot;`, maximum amount of memory is saved by running only one slice at a time. If a number is</span>
<span class="sd">                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`</span>
<span class="sd">                must be a multiple of `slice_size`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sliceable_head_dims</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_attention_slice&quot;</span><span class="p">):</span>
                <span class="n">sliceable_head_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">sliceable_head_dim</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
                <span class="n">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

        <span class="c1"># retrieve number of attention layers</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

        <span class="n">num_sliceable_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">slice_size</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="c1"># half the attention head size is usually a good trade-off between</span>
            <span class="c1"># speed and memory</span>
            <span class="n">slice_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">sliceable_head_dims</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">slice_size</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="c1"># make smallest slice possible</span>
            <span class="n">slice_size</span> <span class="o">=</span> <span class="n">num_sliceable_layers</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">slice_size</span> <span class="o">=</span> <span class="n">num_sliceable_layers</span> <span class="o">*</span> <span class="p">[</span><span class="n">slice_size</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">slice_size</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You have provided </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span><span class="si">}</span><span class="s2">, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span><span class="si">}</span><span class="s2"> different&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; attention layers. Make sure to match `len(slice_size)` to be </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)):</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">slice_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="n">sliceable_head_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">dim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;size </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2"> has to be smaller or equal to </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="c1"># Recursively walk through all the children.</span>
        <span class="c1"># Any children which exposes the set_attention_slice method</span>
        <span class="c1"># gets the message</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_attention_slice&quot;</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">set_attention_slice</span><span class="p">(</span><span class="n">slice_size</span><span class="o">.</span><span class="n">pop</span><span class="p">())</span>

            <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
                <span class="n">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>

        <span class="n">reversed_slice_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">slice_size</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">reversed_slice_size</span><span class="p">)</span>

    <span class="c1"># Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel._set_gradient_checkpointing</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_set_gradient_checkpointing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sample</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
        <span class="n">class_labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep_cond</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">encoder_hidden_states_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">UNet2DConditionOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The [`AudioLDM2UNet2DConditionModel`] forward method.</span>

<span class="sd">        Args:</span>
<span class="sd">            sample (`mindspore.tensor`):</span>
<span class="sd">                The noisy input tensor with the following shape `(batch, channel, height, width)`.</span>
<span class="sd">            timestep (`mindspore.tensor` or `float` or `int`): The number of timesteps to denoise an input.</span>
<span class="sd">            encoder_hidden_states (`mindspore.tensor`):</span>
<span class="sd">                The encoder hidden states with shape `(batch, sequence_length, feature_dim)`.</span>
<span class="sd">            encoder_attention_mask (`mindspore.tensor`):</span>
<span class="sd">                A cross-attention mask of shape `(batch, sequence_length)` is applied to `encoder_hidden_states`. If</span>
<span class="sd">                `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,</span>
<span class="sd">                which adds large negative values to the attention scores corresponding to &quot;discard&quot; tokens.</span>
<span class="sd">            return_dict (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to return a [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] instead of a plain</span>
<span class="sd">                tuple.</span>
<span class="sd">            cross_attention_kwargs (`dict`, *optional*):</span>
<span class="sd">                A kwargs dictionary that if specified is passed along to the [`AttnProcessor`].</span>
<span class="sd">            encoder_hidden_states_1 (`mindspore.tensor`, *optional*):</span>
<span class="sd">                A second set of encoder hidden states with shape `(batch, sequence_length_2, feature_dim_2)`. Can be</span>
<span class="sd">                used to condition the model on a different set of embeddings to `encoder_hidden_states`.</span>
<span class="sd">            encoder_attention_mask_1 (`mindspore.tensor`, *optional*):</span>
<span class="sd">                A cross-attention mask of shape `(batch, sequence_length_2)` is applied to `encoder_hidden_states_1`.</span>
<span class="sd">                If `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,</span>
<span class="sd">                which adds large negative values to the attention scores corresponding to &quot;discard&quot; tokens.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:</span>
<span class="sd">                If `return_dict` is True, an [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] is returned,</span>
<span class="sd">                otherwise a `tuple` is returned where the first element is the sample tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># By default samples have to be AT least a multiple of the overall upsampling factor.</span>
        <span class="c1"># The overall upsampling factor is equal to 2 ** (# num of upsampling layers).</span>
        <span class="c1"># However, the upsampling interpolation output size can be forced to fit any upsampling size</span>
        <span class="c1"># on the fly if necessary.</span>
        <span class="c1"># default_overall_up_factor = 2**self.num_upsamplers</span>

        <span class="c1"># upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`</span>
        <span class="n">upsample_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward upsample size to force interpolation output size.&quot;</span><span class="p">)</span>
        <span class="n">forward_upsample_size</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># ensure attention_mask is a bias, and give it a singleton query_tokens dimension</span>
        <span class="c1"># expects mask of shape:</span>
        <span class="c1">#   [batch, key_tokens]</span>
        <span class="c1"># adds singleton query_tokens dimension:</span>
        <span class="c1">#   [batch,                    1, key_tokens]</span>
        <span class="c1"># this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:</span>
        <span class="c1">#   [batch,  heads, query_tokens, key_tokens] (e.g. mint sdp attn)</span>
        <span class="c1">#   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># assume that mask is expressed as:</span>
            <span class="c1">#   (1 = keep,      0 = discard)</span>
            <span class="c1"># convert mask into a bias that can be added to attention scores:</span>
            <span class="c1">#       (keep = +0,     discard = -10000.0)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># convert encoder_attention_mask to a bias the same way we do for attention_mask</span>
        <span class="k">if</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_attention_mask_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_attention_mask_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">encoder_attention_mask_1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
            <span class="n">encoder_attention_mask_1</span> <span class="o">=</span> <span class="n">encoder_attention_mask_1</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 1. time</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timestep</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
            <span class="c1"># TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can</span>
            <span class="c1"># This would be a good case for the `match` statement (Python 3.10+)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float64</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">timesteps</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>

        <span class="c1"># broadcast to batch dimension in a way that&#39;s compatible with ONNX/Core ML</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>

        <span class="n">t_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>

        <span class="c1"># `Timesteps` does not contain any weights and will always return f32 tensors</span>
        <span class="c1"># but time_embedding might actually be running in fp16. so we need to cast here.</span>
        <span class="c1"># there might be better ways to encapsulate this.</span>
        <span class="n">t_emb</span> <span class="o">=</span> <span class="n">t_emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embedding</span><span class="p">(</span><span class="n">t_emb</span><span class="p">,</span> <span class="n">timestep_cond</span><span class="p">)</span>
        <span class="n">aug_emb</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">class_labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;class_labels should be provided when num_class_embeds &gt; 0&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;timestep&quot;</span><span class="p">:</span>
                <span class="n">class_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>

                <span class="c1"># `Timesteps` does not contain any weights and will always return f32 tensors</span>
                <span class="c1"># there might be better ways to encapsulate this.</span>
                <span class="n">class_labels</span> <span class="o">=</span> <span class="n">class_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="n">class_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">class_embeddings_concat</span><span class="p">:</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">class_emb</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">+</span> <span class="n">class_emb</span>

        <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">+</span> <span class="n">aug_emb</span> <span class="k">if</span> <span class="n">aug_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">emb</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>

        <span class="c1"># 2. pre-process</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

        <span class="c1"># 3. down</span>
        <span class="n">down_block_res_samples</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">downsample_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_blocks</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">downsample_block</span><span class="p">,</span> <span class="s2">&quot;has_cross_attention&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">downsample_block</span><span class="o">.</span><span class="n">has_cross_attention</span><span class="p">:</span>
                <span class="n">sample</span><span class="p">,</span> <span class="n">res_samples</span> <span class="o">=</span> <span class="n">downsample_block</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                    <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample</span><span class="p">,</span> <span class="n">res_samples</span> <span class="o">=</span> <span class="n">downsample_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">)</span>

            <span class="n">down_block_res_samples</span> <span class="o">+=</span> <span class="n">res_samples</span>

        <span class="c1"># 4. mid</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span><span class="p">(</span>
                <span class="n">sample</span><span class="p">,</span>
                <span class="n">emb</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
                <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># 5. up</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">upsample_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="p">):</span>
            <span class="n">is_final_block</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="n">res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">upsample_block</span><span class="o">.</span><span class="n">resnets</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">down_block_res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">upsample_block</span><span class="o">.</span><span class="n">resnets</span><span class="p">)]</span>

            <span class="c1"># if we have not reached the final block and need to forward the</span>
            <span class="c1"># upsample size, we do it here</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_final_block</span> <span class="ow">and</span> <span class="n">forward_upsample_size</span><span class="p">:</span>
                <span class="n">upsample_size</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">upsample_block</span><span class="p">,</span> <span class="s2">&quot;has_cross_attention&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">upsample_block</span><span class="o">.</span><span class="n">has_cross_attention</span><span class="p">:</span>
                <span class="n">sample</span> <span class="o">=</span> <span class="n">upsample_block</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                    <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                    <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                    <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample</span> <span class="o">=</span> <span class="n">upsample_block</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span> <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span> <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span>
                <span class="p">)</span>

        <span class="c1"># 6. post-process</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_act</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">sample</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">UNet2DConditionOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="mindone.diffusers.AudioLDM2UNet2DConditionModel.attn_processors" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2UNet2DConditionModel</span><span class="o">.</span><span class="n">attn_processors</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.attn_processors" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="mindone.diffusers.models.attention_processor.AttentionProcessor">AttentionProcessor</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>dict</code> of attention processors: A dictionary containing all attention processors used in the model with</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="mindone.diffusers.models.attention_processor.AttentionProcessor">AttentionProcessor</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>indexed by its weight name.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2UNet2DConditionModel.construct" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2UNet2DConditionModel</span><span class="o">.</span><span class="n">construct</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">timestep</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">class_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">timestep_cond</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.construct" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>The [<code>AudioLDM2UNet2DConditionModel</code>] forward method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>sample</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The noisy input tensor with the following shape <code>(batch, channel, height, width)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>timestep</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of timesteps to denoise an input.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor` or `float` or `int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The encoder hidden states with shape <code>(batch, sequence_length, feature_dim)</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A cross-attention mask of shape <code>(batch, sequence_length)</code> is applied to <code>encoder_hidden_states</code>. If
<code>True</code> the mask is kept, otherwise if <code>False</code> it is discarded. Mask will be converted into a bias,
which adds large negative values to the attention scores corresponding to "discard" tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to return a [<code>~models.unets.unet_2d_condition.UNet2DConditionOutput</code>] instead of a plain
tuple.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A kwargs dictionary that if specified is passed along to the [<code>AttnProcessor</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A second set of encoder hidden states with shape <code>(batch, sequence_length_2, feature_dim_2)</code>. Can be
used to condition the model on a different set of embeddings to <code>encoder_hidden_states</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_attention_mask_1</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A cross-attention mask of shape <code>(batch, sequence_length_2)</code> is applied to <code>encoder_hidden_states_1</code>.
If <code>True</code> the mask is kept, otherwise if <code>False</code> it is discarded. Mask will be converted into a bias,
which adds large negative values to the attention scores corresponding to "discard" tokens.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`mindspore.tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="mindone.diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput


  
      dataclass
  " href="../../models/unet2d-cond/#mindone.diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput">UNet2DConditionOutput</a>, <span title="typing.Tuple">Tuple</span>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>[<code>~models.unets.unet_2d_condition.UNet2DConditionOutput</code>] or <code>tuple</code>:
If <code>return_dict</code> is True, an [<code>~models.unets.unet_2d_condition.UNet2DConditionOutput</code>] is returned,
otherwise a <code>tuple</code> is returned where the first element is the sample tensor.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sample</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
    <span class="n">timestep</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
    <span class="n">class_labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timestep_cond</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">encoder_hidden_states_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">encoder_attention_mask_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">UNet2DConditionOutput</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The [`AudioLDM2UNet2DConditionModel`] forward method.</span>

<span class="sd">    Args:</span>
<span class="sd">        sample (`mindspore.tensor`):</span>
<span class="sd">            The noisy input tensor with the following shape `(batch, channel, height, width)`.</span>
<span class="sd">        timestep (`mindspore.tensor` or `float` or `int`): The number of timesteps to denoise an input.</span>
<span class="sd">        encoder_hidden_states (`mindspore.tensor`):</span>
<span class="sd">            The encoder hidden states with shape `(batch, sequence_length, feature_dim)`.</span>
<span class="sd">        encoder_attention_mask (`mindspore.tensor`):</span>
<span class="sd">            A cross-attention mask of shape `(batch, sequence_length)` is applied to `encoder_hidden_states`. If</span>
<span class="sd">            `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,</span>
<span class="sd">            which adds large negative values to the attention scores corresponding to &quot;discard&quot; tokens.</span>
<span class="sd">        return_dict (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether or not to return a [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] instead of a plain</span>
<span class="sd">            tuple.</span>
<span class="sd">        cross_attention_kwargs (`dict`, *optional*):</span>
<span class="sd">            A kwargs dictionary that if specified is passed along to the [`AttnProcessor`].</span>
<span class="sd">        encoder_hidden_states_1 (`mindspore.tensor`, *optional*):</span>
<span class="sd">            A second set of encoder hidden states with shape `(batch, sequence_length_2, feature_dim_2)`. Can be</span>
<span class="sd">            used to condition the model on a different set of embeddings to `encoder_hidden_states`.</span>
<span class="sd">        encoder_attention_mask_1 (`mindspore.tensor`, *optional*):</span>
<span class="sd">            A cross-attention mask of shape `(batch, sequence_length_2)` is applied to `encoder_hidden_states_1`.</span>
<span class="sd">            If `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,</span>
<span class="sd">            which adds large negative values to the attention scores corresponding to &quot;discard&quot; tokens.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:</span>
<span class="sd">            If `return_dict` is True, an [`~models.unets.unet_2d_condition.UNet2DConditionOutput`] is returned,</span>
<span class="sd">            otherwise a `tuple` is returned where the first element is the sample tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># By default samples have to be AT least a multiple of the overall upsampling factor.</span>
    <span class="c1"># The overall upsampling factor is equal to 2 ** (# num of upsampling layers).</span>
    <span class="c1"># However, the upsampling interpolation output size can be forced to fit any upsampling size</span>
    <span class="c1"># on the fly if necessary.</span>
    <span class="c1"># default_overall_up_factor = 2**self.num_upsamplers</span>

    <span class="c1"># upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`</span>
    <span class="n">upsample_size</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward upsample size to force interpolation output size.&quot;</span><span class="p">)</span>
    <span class="n">forward_upsample_size</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># ensure attention_mask is a bias, and give it a singleton query_tokens dimension</span>
    <span class="c1"># expects mask of shape:</span>
    <span class="c1">#   [batch, key_tokens]</span>
    <span class="c1"># adds singleton query_tokens dimension:</span>
    <span class="c1">#   [batch,                    1, key_tokens]</span>
    <span class="c1"># this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:</span>
    <span class="c1">#   [batch,  heads, query_tokens, key_tokens] (e.g. mint sdp attn)</span>
    <span class="c1">#   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># assume that mask is expressed as:</span>
        <span class="c1">#   (1 = keep,      0 = discard)</span>
        <span class="c1"># convert mask into a bias that can be added to attention scores:</span>
        <span class="c1">#       (keep = +0,     discard = -10000.0)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># convert encoder_attention_mask to a bias the same way we do for attention_mask</span>
    <span class="k">if</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">encoder_attention_mask_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoder_attention_mask_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">encoder_attention_mask_1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span>
        <span class="n">encoder_attention_mask_1</span> <span class="o">=</span> <span class="n">encoder_attention_mask_1</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. time</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timestep</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
        <span class="c1"># TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can</span>
        <span class="c1"># This would be a good case for the `match` statement (Python 3.10+)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">timestep</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float64</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">timesteps</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span>

    <span class="c1"># broadcast to batch dimension in a way that&#39;s compatible with ONNX/Core ML</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="n">timesteps</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>

    <span class="n">t_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>

    <span class="c1"># `Timesteps` does not contain any weights and will always return f32 tensors</span>
    <span class="c1"># but time_embedding might actually be running in fp16. so we need to cast here.</span>
    <span class="c1"># there might be better ways to encapsulate this.</span>
    <span class="n">t_emb</span> <span class="o">=</span> <span class="n">t_emb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embedding</span><span class="p">(</span><span class="n">t_emb</span><span class="p">,</span> <span class="n">timestep_cond</span><span class="p">)</span>
    <span class="n">aug_emb</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">class_labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;class_labels should be provided when num_class_embeds &gt; 0&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">class_embed_type</span> <span class="o">==</span> <span class="s2">&quot;timestep&quot;</span><span class="p">:</span>
            <span class="n">class_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_proj</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>

            <span class="c1"># `Timesteps` does not contain any weights and will always return f32 tensors</span>
            <span class="c1"># there might be better ways to encapsulate this.</span>
            <span class="n">class_labels</span> <span class="o">=</span> <span class="n">class_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">class_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">sample</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">class_embeddings_concat</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">emb</span><span class="p">,</span> <span class="n">class_emb</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">+</span> <span class="n">class_emb</span>

    <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">+</span> <span class="n">aug_emb</span> <span class="k">if</span> <span class="n">aug_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">emb</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_embed_act</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>

    <span class="c1"># 2. pre-process</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_in</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="c1"># 3. down</span>
    <span class="n">down_block_res_samples</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample</span><span class="p">,)</span>
    <span class="k">for</span> <span class="n">downsample_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_blocks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">downsample_block</span><span class="p">,</span> <span class="s2">&quot;has_cross_attention&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">downsample_block</span><span class="o">.</span><span class="n">has_cross_attention</span><span class="p">:</span>
            <span class="n">sample</span><span class="p">,</span> <span class="n">res_samples</span> <span class="o">=</span> <span class="n">downsample_block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
                <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample</span><span class="p">,</span> <span class="n">res_samples</span> <span class="o">=</span> <span class="n">downsample_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">)</span>

        <span class="n">down_block_res_samples</span> <span class="o">+=</span> <span class="n">res_samples</span>

    <span class="c1"># 4. mid</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mid_block</span><span class="p">(</span>
            <span class="n">sample</span><span class="p">,</span>
            <span class="n">emb</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
            <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># 5. up</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">upsample_block</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="p">):</span>
        <span class="n">is_final_block</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">up_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="n">res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">upsample_block</span><span class="o">.</span><span class="n">resnets</span><span class="p">)</span> <span class="p">:]</span>
        <span class="n">down_block_res_samples</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">upsample_block</span><span class="o">.</span><span class="n">resnets</span><span class="p">)]</span>

        <span class="c1"># if we have not reached the final block and need to forward the</span>
        <span class="c1"># upsample size, we do it here</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_final_block</span> <span class="ow">and</span> <span class="n">forward_upsample_size</span><span class="p">:</span>
            <span class="n">upsample_size</span> <span class="o">=</span> <span class="n">down_block_res_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">upsample_block</span><span class="p">,</span> <span class="s2">&quot;has_cross_attention&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">upsample_block</span><span class="o">.</span><span class="n">has_cross_attention</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">upsample_block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span>
                <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span>
                <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">cross_attention_kwargs</span><span class="o">=</span><span class="n">cross_attention_kwargs</span><span class="p">,</span>
                <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_1</span><span class="o">=</span><span class="n">encoder_hidden_states_1</span><span class="p">,</span>
                <span class="n">encoder_attention_mask_1</span><span class="o">=</span><span class="n">encoder_attention_mask_1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">upsample_block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">sample</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">emb</span><span class="p">,</span> <span class="n">res_hidden_states_tuple</span><span class="o">=</span><span class="n">res_samples</span><span class="p">,</span> <span class="n">upsample_size</span><span class="o">=</span><span class="n">upsample_size</span>
            <span class="p">)</span>

    <span class="c1"># 6. post-process</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span><span class="p">:</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_norm_out</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_act</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_out</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">sample</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">UNet2DConditionOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attention_slice" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2UNet2DConditionModel</span><span class="o">.</span><span class="n">set_attention_slice</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attention_slice" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Enable sliced attention computation.</p>
<p>When this option is enabled, the attention module splits the input tensor in slices to compute attention in
several steps. This is useful for saving some memory in exchange for a small decrease in speed.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>slice_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When <code>"auto"</code>, input to the attention heads is halved, so attention is computed in two steps. If
<code>"max"</code>, maximum amount of memory is saved by running only one slice at a time. If a number is
provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case, <code>attention_head_dim</code>
must be a multiple of <code>slice_size</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `int` or `list(int)`, *optional*, defaults to `&#34;auto&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_attention_slice</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enable sliced attention computation.</span>

<span class="sd">    When this option is enabled, the attention module splits the input tensor in slices to compute attention in</span>
<span class="sd">    several steps. This is useful for saving some memory in exchange for a small decrease in speed.</span>

<span class="sd">    Args:</span>
<span class="sd">        slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `&quot;auto&quot;`):</span>
<span class="sd">            When `&quot;auto&quot;`, input to the attention heads is halved, so attention is computed in two steps. If</span>
<span class="sd">            `&quot;max&quot;`, maximum amount of memory is saved by running only one slice at a time. If a number is</span>
<span class="sd">            provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`</span>
<span class="sd">            must be a multiple of `slice_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sliceable_head_dims</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_attention_slice&quot;</span><span class="p">):</span>
            <span class="n">sliceable_head_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">sliceable_head_dim</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>

    <span class="c1"># retrieve number of attention layers</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">fn_recursive_retrieve_sliceable_dims</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

    <span class="n">num_sliceable_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">slice_size</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
        <span class="c1"># half the attention head size is usually a good trade-off between</span>
        <span class="c1"># speed and memory</span>
        <span class="n">slice_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">sliceable_head_dims</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">slice_size</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
        <span class="c1"># make smallest slice possible</span>
        <span class="n">slice_size</span> <span class="o">=</span> <span class="n">num_sliceable_layers</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">slice_size</span> <span class="o">=</span> <span class="n">num_sliceable_layers</span> <span class="o">*</span> <span class="p">[</span><span class="n">slice_size</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">slice_size</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;You have provided </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)</span><span class="si">}</span><span class="s2">, but </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s2"> has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span><span class="si">}</span><span class="s2"> different&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; attention layers. Make sure to match `len(slice_size)` to be </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sliceable_head_dims</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">slice_size</span><span class="p">)):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">slice_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">sliceable_head_dims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;size </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2"> has to be smaller or equal to </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># Recursively walk through all the children.</span>
    <span class="c1"># Any children which exposes the set_attention_slice method</span>
    <span class="c1"># gets the message</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_attention_slice&quot;</span><span class="p">):</span>
            <span class="n">module</span><span class="o">.</span><span class="n">set_attention_slice</span><span class="p">(</span><span class="n">slice_size</span><span class="o">.</span><span class="n">pop</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>

    <span class="n">reversed_slice_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">slice_size</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">children</span><span class="p">():</span>
        <span class="n">fn_recursive_set_attention_slice</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">reversed_slice_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attn_processor" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2UNet2DConditionModel</span><span class="o">.</span><span class="n">set_attn_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span></code>

<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_attn_processor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Sets the attention processor to use to compute attention.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>processor</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The instantiated processor class or a dictionary of processor classes that will be set as the processor
for <strong>all</strong> <code>Attention</code> layers.</p>
<p>If <code>processor</code> is a dict, the key needs to define the path to the corresponding cross attention
processor. This is strongly recommended when setting trainable attention processors.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict` of `AttentionProcessor` or only `AttentionProcessor`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_attn_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">processor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AttentionProcessor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AttentionProcessor</span><span class="p">]]):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the attention processor to use to compute attention.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):</span>
<span class="sd">            The instantiated processor class or a dictionary of processor classes that will be set as the processor</span>
<span class="sd">            for **all** `Attention` layers.</span>

<span class="sd">            If `processor` is a dict, the key needs to define the path to the corresponding cross attention</span>
<span class="sd">            processor. This is strongly recommended when setting trainable attention processors.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">count</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;A dict of processors was passed, but the number of processors </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not match the&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; number of attention layers: </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">. Please make sure to pass </span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2"> processor classes.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fn_recursive_attn_processor</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">processor</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;set_processor&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">set_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">module</span><span class="o">.</span><span class="n">set_processor</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.processor&quot;</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">sub_name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">fn_recursive_attn_processor</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">sub_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">fn_recursive_attn_processor</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">processor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.AudioLDM2UNet2DConditionModel.set_default_attn_processor" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">AudioLDM2UNet2DConditionModel</span><span class="o">.</span><span class="n">set_default_attn_processor</span><span class="p">()</span></code>

<a href="#mindone.diffusers.AudioLDM2UNet2DConditionModel.set_default_attn_processor" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Disables custom attention processors and sets the default attention implementation.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/pipelines/audioldm2/modeling_audioldm2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_default_attn_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disables custom attention processors and sets the default attention implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">proc</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="n">ADDED_KV_ATTENTION_PROCESSORS</span> <span class="k">for</span> <span class="n">proc</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AttnAddedKVProcessor</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">all</span><span class="p">(</span><span class="n">proc</span><span class="o">.</span><span class="vm">__class__</span> <span class="ow">in</span> <span class="n">CROSS_ATTENTION_PROCESSORS</span> <span class="k">for</span> <span class="n">proc</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">AttnProcessor</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Cannot call `set_default_attn_processor` when attention processors are of type </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_processors</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_attn_processor</span><span class="p">(</span><span class="n">processor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.pipelines.AudioPipelineOutput" class="doc doc-heading">
            <code>mindone.diffusers.pipelines.AudioPipelineOutput</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

<a href="#mindone.diffusers.pipelines.AudioPipelineOutput" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.utils.BaseOutput" href="../../outputs/#mindone.diffusers.utils.BaseOutput">BaseOutput</a></code></p>


        <p>Output class for audio pipelines.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/pipelines/pipeline_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AudioPipelineOutput</span><span class="p">(</span><span class="n">BaseOutput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Output class for audio pipelines.</span>

<span class="sd">    Args:</span>
<span class="sd">        audios (`np.ndarray`)</span>
<span class="sd">            List of denoised audio samples of a NumPy array of shape `(batch_size, num_channels, sample_rate)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">audios</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="July 1, 2025 08:00:24 UTC">July 1, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="July 1, 2025 08:00:24 UTC">July 1, 2025</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:73014084+cui-yshoho@users.noreply.github.com">Cui-yshoho</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../audioldm/" class="md-footer__link md-footer__link--prev" aria-label="Previous: AudioLDM">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                AudioLDM
              </div>
            </div>
          </a>
        
        
          
          <a href="../aura_flow/" class="md-footer__link md-footer__link--next" aria-label="Next: AuraFlow">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                AuraFlow
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 MindSpore Lab
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindone" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>