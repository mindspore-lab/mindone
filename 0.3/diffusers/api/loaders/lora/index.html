
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindone/0.3/diffusers/api/loaders/lora/">
      
      
        <link rel="prev" href="../ip_adapter/">
      
      
        <link rel="next" href="../single_file/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>LoRA - MindOne - One for All</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Source Sans Pro";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lora" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindOne - One for All" class="md-header__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOne - One for All
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LoRA
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../" class="md-tabs__link">
          
  
  
  Diffusers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../transformers/" class="md-tabs__link">
          
  
  
  Transformers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../peft/" class="md-tabs__link">
          
  
  
  PEFT

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindOne - One for All" class="md-nav__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOne - One for All
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Diffusers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ§¨ Diffusers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../quicktour/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quicktour
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Effective and efficient diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../limitations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Limitations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/tutorial_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/write_own_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding pipelines, models and schedulers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/autopipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/basic_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Train a diffusion model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/using_peft_for_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load LoRAs for inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Load pipelines and adapters
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Load pipelines and adapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/schedulers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load schedulers and models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/other-formats/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model files and layouts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading_adapters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load adapters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/push_to_hub/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Push files to the Hub
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Generative tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Generative tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/unconditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/conditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/text-img2vid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Inference techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Inference techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/overview_techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/merge_loras/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Merge LoRAs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/scheduler_features/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler features
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/callback/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pipeline callbacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/reusing_seeds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reproducible pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Advanced inference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Advanced inference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_inference/outpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outpainting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Specific pipeline examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            Specific pipeline examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/t2i_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/textual_inversion_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/shap-e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_tcd_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trajectory Consistency Distillation-LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Video Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/marigold_usage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/create_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create a dataset for training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/adapt_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adapt a model to a new task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_4" >
        
          
          <label class="md-nav__link" for="__nav_2_8_4" id="__nav_2_8_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_4">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/unconditional_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text2image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_5" >
        
          
          <label class="md-nav__link" for="__nav_2_8_5" id="__nav_2_8_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_5">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/dreambooth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DreamBooth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
        
          
          <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Accelerate inference and reduce memory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_9">
            <span class="md-nav__icon md-icon"></span>
            Accelerate inference and reduce memory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/fp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speed up inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reduce memory usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/xformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    xFormers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
        
          
          <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Conceptual Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10">
            <span class="md-nav__icon md-icon"></span>
            Conceptual Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../conceptual/philosophy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Philosophy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlling_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Controlled generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_1" >
        
          
          <label class="md-nav__link" for="__nav_2_11_1" id="__nav_2_11_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Main Classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_1">
            <span class="md-nav__icon md-icon"></span>
            Main Classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../outputs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outputs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Loaders
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11_2">
            <span class="md-nav__icon md-icon"></span>
            Loaders
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SD3LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      FluxLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FluxLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      CogVideoXLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CogVideoXLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      Mochi1LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mochi1LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LTXVideoLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LTXVideoLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SanaLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SanaLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      HunyuanVideoLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HunyuanVideoLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      Lumina2LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lumina2LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      WanLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="WanLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SkyReelsV2LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SkyReelsV2LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      AmusedLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AmusedLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraBaseMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraBaseMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fused_loras" class="md-nav__link">
    <span class="md-ellipsis">
      fused_loras
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.lora_scale" class="md-nav__link">
    <span class="md-ellipsis">
      lora_scale
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.num_fused_loras" class="md-nav__link">
    <span class="md-ellipsis">
      num_fused_loras
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.disable_lora" class="md-nav__link">
    <span class="md-ellipsis">
      disable_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora_hotswap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.set_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.write_lora_layers" class="md-nav__link">
    <span class="md-ellipsis">
      write_lora_layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../single_file/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single files
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../textual_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer_sd3.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2D
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/auto_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_3" id="__nav_2_11_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ControlNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_3">
            <span class="md-nav__icon md-icon"></span>
            ControlNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SanaControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sparsectrl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SparseControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnionModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_4" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_4" id="__nav_2_11_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_4">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/allegro_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AllegroTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/aura_flow_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlowTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/chroma_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ChromaTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogvideox_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consisid_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisIDTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview3plus_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3PlusTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview4_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cosmos_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CosmosTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/dit_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiTTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/easyanimate_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimateTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/flux_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_video_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/latte_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LatteTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina_nextdit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LuminaNextDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina2_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina2Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ltx_video_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mochi_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MochiTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/omnigen_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGenTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pixart_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArtTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/prior_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PriorTransformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sd3_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/skyreels_v2_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SkyReelsV2Transformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sana_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SanaTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_audio_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableAudioDiTModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer_temporal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TransformerTemporalModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/wan_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WanTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_5" id="__nav_2_11_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    UNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_5">
            <span class="md-nav__icon md-icon"></span>
            UNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_cascade_unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableCascadeUNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet1DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet3d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet3DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet-motion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNetMotionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/uvit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UViT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_6" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_6" id="__nav_2_11_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VAEs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_6">
            <span class="md-nav__icon md-icon"></span>
            VAEs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLAllegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLCogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_cosmos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLCosmos
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_kl_hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLHunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLLTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_magvit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMagvit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_kl_wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLWan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/asymmetricautoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AsymmetricAutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_dc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderDC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consistency_decoder_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderVAE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_oobleck/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Oobleck AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_tiny/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tiny AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" >
        
          
          <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pipelines
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_4">
            <span class="md-nav__icon md-icon"></span>
            Pipelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Allegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/amused/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    aMUSEd
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/animatediff/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AnimateDiff
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/attend_and_excite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attend-and-Excite
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/audioldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/audioldm2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/aura_flow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/auto_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/blip_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BLIP-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/chroma/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chroma
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogview3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogview4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Flux.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-Sana
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cosmos/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cosmos
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dance_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dance Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/deepfloyd_if/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepFloyd IF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/easyanimate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimate
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/control_flux_inpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlInpaint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/framepack/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Framepack
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/i2vgenxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I2VGen-XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pix2pix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructPix2Pix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky_v22/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kolors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kolors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latte/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latte
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ledits_pp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LEDITS++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/lumina2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina 2.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/lumina/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina-T2X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/marigold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/panorama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MultiDiffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/musicldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MusicLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/paint_by_example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paint by Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Personalized Image Animator (PIA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î±
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart_sigma/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î£
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/sana_sprint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana Sprint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/self_attention_guidance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/semantic_stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/shap_e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/skyreels_v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SkyReels-V2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Audio
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_cascade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Cascade
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4_70" >
        
          
          <label class="md-nav__link" for="__nav_2_11_4_70" id="__nav_2_11_4_70_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_4_70_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_4_70">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/text2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/image_variation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image variation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_safe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Safe Stable Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_xl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/latent_upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Super-resolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/k_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/ldm3d_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/gligen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GLIGEN (Grounded Language-to-Image Generation)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/text_to_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/text_to_video_zero/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text2Video-Zero
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/unidiffuser/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniDiffuser
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/value_guided_sampling.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Value-guided sampling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/visualcloze/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VisualCloze
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/wuerstchen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wuerstchen
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Schedulers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_5">
            <span class="md-nav__icon md-icon"></span>
            Schedulers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/cm_stochastic_iterative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMStochasticIterativeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/consistency_decoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/cosine_dpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CosineDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMInverseScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/deis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DEISMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepInverse
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_sde/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSDEScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/singlestep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSinglestepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMEulerScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerAncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_euler_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchEulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_heun_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchHeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/heun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ipndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IPNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/stochastic_karras_ve.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KarrasVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2AncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2DiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LCMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lms_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LMSDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/pndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/repaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RePaintScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_ve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_vp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVpScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/tcd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TCDScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/unipc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniPCMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/vq_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQDiffusionScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_6" >
        
          
          <label class="md-nav__link" for="__nav_2_11_6" id="__nav_2_11_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Internal classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_6">
            <span class="md-nav__icon md-icon"></span>
            Internal classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../internal_classes_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attnprocessor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../activations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom activation functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom normalization layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utilities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VAE Image Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../video_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video Processor
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune a pretrained model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune_distribute/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed training and mixed precision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generation with LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SD3LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      FluxLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FluxLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      CogVideoXLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CogVideoXLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      Mochi1LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mochi1LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LTXVideoLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LTXVideoLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SanaLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SanaLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      HunyuanVideoLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HunyuanVideoLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      Lumina2LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lumina2LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      WanLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="WanLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SkyReelsV2LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SkyReelsV2LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      AmusedLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AmusedLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraBaseMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraBaseMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fused_loras" class="md-nav__link">
    <span class="md-ellipsis">
      fused_loras
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.lora_scale" class="md-nav__link">
    <span class="md-ellipsis">
      lora_scale
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.num_fused_loras" class="md-nav__link">
    <span class="md-ellipsis">
      num_fused_loras
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.disable_lora" class="md-nav__link">
    <span class="md-ellipsis">
      disable_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap" class="md-nav__link">
    <span class="md-ellipsis">
      enable_lora_hotswap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.set_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      set_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.write_lora_layers" class="md-nav__link">
    <span class="md-ellipsis">
      write_lora_layers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/mindspore-lab/mindone/edit/master/docs/diffusers/api/loaders/lora.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindone/raw/master/docs/diffusers/api/loaders/lora.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="lora">LoRA<a class="headerlink" href="#lora" title="Permanent link">&para;</a></h1>
<p>LoRA is a fast and lightweight training method that inserts and trains a significantly smaller number of parameters instead of all the model parameters. This produces a smaller file (~100 MBs) and makes it easier to quickly train a model to learn a new concept. LoRA weights are typically loaded into the denoiser, text encoder or both. The denoiser usually corresponds to a UNet (<code>UNet2DConditionModel</code>, for example) or a Transformer (<code>SD3Transformer2DModel</code>, for example). There are several classes for loading LoRA weights:</p>
<ul>
<li><code>StableDiffusionLoraLoaderMixin</code> provides functions for loading and unloading, fusing and unfusing, enabling and disabling, and more functions for managing LoRA weights. This class can be used with any model.</li>
<li><code>StableDiffusionXLLoraLoaderMixin</code> is a <a href="../../pipelines/stable_diffusion/stable_diffusion_xl/">Stable Diffusion (SDXL)</a> version of the <code>StableDiffusionLoraLoaderMixin</code> class for loading and saving LoRA weights. It can only be used with the SDXL model.</li>
<li><code>SD3LoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/stable_diffusion/stable_diffusion_3/">Stable Diffusion 3</a>.</li>
<li><code>FluxLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/flux/">Flux</a>.</li>
<li><code>CogVideoXLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/cogvideox/">CogVideoX</a>.</li>
<li><code>Mochi1LoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/mochi/">Mochi</a>.</li>
<li><code>LTXVideoLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/ltx_video/">LTX-Video</a>.</li>
<li><code>SanaLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/sana/">Sana</a>.</li>
<li><code>HunyuanVideoLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/hunyuan_video/">HunyuanVideo</a>.</li>
<li><code>Lumina2LoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/lumina2/">Lumina2</a>.</li>
<li><code>WanLoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/wan/">Wan</a>.</li>
<li><code>SkyReelsV2LoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/skyreels_v2/">SkyReels-V2</a>.</li>
<li><code>AmusedLoraLoaderMixin</code> is for the <a href="../../pipelines/amused/">AmusedPipeline</a>.</li>
<li><code>LoraBaseMixin</code> provides a base class with several utility methods to fuse, unfuse, unload, LoRAs and more.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To learn more about how to load LoRA weights, see the <a href="../../../using-diffusers/loading_adapters/#lora">LoRA</a> loading guide.</p>
</div>


<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into Stable Diffusion [<code>UNet2DConditionModel</code>] and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>.</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StableDiffusionLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into Stable Diffusion [`UNet2DConditionModel`] and</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">unet_name</span> <span class="o">=</span> <span class="n">UNET_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">        loaded into `self.unet`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                Defaults to `False`. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter</span>
<span class="sd">                in-place. This means that, instead of loading an additional adapter, this will take the existing</span>
<span class="sd">                adapter weights and replace them with the weights of the new adapter. This can be faster and more</span>
<span class="sd">                memory efficient. However, the main advantage of hotswapping is that when the model is compiled with</span>
<span class="sd">                torch.compile, loading the new adapter does not require recompilation of the model. When using</span>
<span class="sd">                hotswapping, the passed `adapter_name` should be the name of an already loaded adapter.</span>

<span class="sd">                If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need</span>
<span class="sd">                to call an additional method before loading the adapter:</span>

<span class="sd">                ```py</span>
<span class="sd">                pipeline = ...  # load diffusers pipeline</span>
<span class="sd">                max_rank = ...  # the highest rank among all LoRAs that you want to load</span>
<span class="sd">                # call *before* compiling and loading the LoRA adapter</span>
<span class="sd">                pipeline.enable_lora_hotswap(target_rank=max_rank)</span>
<span class="sd">                pipeline.load_lora_weights(file_name)</span>
<span class="sd">                # optionally compile the model now</span>
<span class="sd">                ```</span>

<span class="sd">                Note that hotswapping adapters of the text encoder is not yet supported. There are some further</span>
<span class="sd">                limitations to this technique, which are documented here:</span>
<span class="sd">                https://huggingface.co/docs/peft/main/en/package_reference/hotswap</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">                Name of the serialized state dict file.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># UNet and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Map SDXL blocks correctly.</span>
            <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># use unet config to remap block numbers</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            unet_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the unet to be serialized with the state dict.</span>
<span class="sd">            text_encoder_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `unet_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">unet_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">unet_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>unet</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">unet</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is
loaded into <code>self.unet</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state
dict is loaded into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Defaults to <code>False</code>. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter
in-place. This means that, instead of loading an additional adapter, this will take the existing
adapter weights and replace them with the weights of the new adapter. This can be faster and more
memory efficient. However, the main advantage of hotswapping is that when the model is compiled with
torch.compile, loading the new adapter does not require recompilation of the model. When using
hotswapping, the passed <code>adapter_name</code> should be the name of an already loaded adapter.</p>
<p>If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need
to call an additional method before loading the adapter:</p>
<div class="highlight"><pre><span></span><code><span class="n">pipeline</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># load diffusers pipeline</span>
<span class="n">max_rank</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># the highest rank among all LoRAs that you want to load</span>
<span class="c1"># call *before* compiling and loading the LoRA adapter</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">enable_lora_hotswap</span><span class="p">(</span><span class="n">target_rank</span><span class="o">=</span><span class="n">max_rank</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
<span class="c1"># optionally compile the model now</span>
</code></pre></div>
<p>Note that hotswapping adapters of the text encoder is not yet supported. There are some further
limitations to this technique, which are documented here:
https://huggingface.co/docs/peft/main/en/package_reference/hotswap</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">    loaded into `self.unet`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            Defaults to `False`. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter</span>
<span class="sd">            in-place. This means that, instead of loading an additional adapter, this will take the existing</span>
<span class="sd">            adapter weights and replace them with the weights of the new adapter. This can be faster and more</span>
<span class="sd">            memory efficient. However, the main advantage of hotswapping is that when the model is compiled with</span>
<span class="sd">            torch.compile, loading the new adapter does not require recompilation of the model. When using</span>
<span class="sd">            hotswapping, the passed `adapter_name` should be the name of an already loaded adapter.</span>

<span class="sd">            If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need</span>
<span class="sd">            to call an additional method before loading the adapter:</span>

<span class="sd">            ```py</span>
<span class="sd">            pipeline = ...  # load diffusers pipeline</span>
<span class="sd">            max_rank = ...  # the highest rank among all LoRAs that you want to load</span>
<span class="sd">            # call *before* compiling and loading the LoRA adapter</span>
<span class="sd">            pipeline.enable_lora_hotswap(target_rank=max_rank)</span>
<span class="sd">            pipeline.load_lora_weights(file_name)</span>
<span class="sd">            # optionally compile the model now</span>
<span class="sd">            ```</span>

<span class="sd">            Note that hotswapping adapters of the text encoder is not yet supported. There are some further</span>
<span class="sd">            limitations to this technique, which are documented here:</span>
<span class="sd">            https://huggingface.co/docs/peft/main/en/package_reference/hotswap</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>weight_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the serialized state dict file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">            Name of the serialized state dict file.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># UNet and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="c1"># Map SDXL blocks correctly.</span>
        <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use unet config to remap block numbers</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the unet to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        unet_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the unet to be serialized with the state dict.</span>
<span class="sd">        text_encoder_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `unet_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">unet_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">unet_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into Stable Diffusion XL [<code>UNet2DConditionModel</code>],
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>, and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection"><code>CLIPTextModelWithProjection</code></a>.</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StableDiffusionXLLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into Stable Diffusion XL [`UNet2DConditionModel`],</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), and</span>
<span class="sd">    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span>
    <span class="n">unet_name</span> <span class="o">=</span> <span class="n">UNET_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">        loaded into `self.unet`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
        <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
        <span class="c1"># pipeline.</span>

        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="si">}</span><span class="s2">_2&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">                Name of the serialized state dict file.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># UNet and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Map SDXL blocks correctly.</span>
            <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># use unet config to remap block numbers</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            unet_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the unet to be serialized with the state dict.</span>
<span class="sd">            text_encoder_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">            text_encoder_2_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">unet_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">unet_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>unet</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">unet</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is
loaded into <code>self.unet</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state
dict is loaded into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">    loaded into `self.unet`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
    <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
    <span class="c1"># pipeline.</span>

    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="si">}</span><span class="s2">_2&quot;</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>weight_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the serialized state dict file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">            Name of the serialized state dict file.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># UNet and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="c1"># Map SDXL blocks correctly.</span>
        <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use unet config to remap block numbers</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_2_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder_2</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the unet to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_2_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">unet_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        unet_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the unet to be serialized with the state dict.</span>
<span class="sd">        text_encoder_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">        text_encoder_2_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">unet_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">unet_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span>
<span class="normal">967</span>
<span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span>
<span class="normal">973</span>
<span class="normal">974</span>
<span class="normal">975</span>
<span class="normal">976</span>
<span class="normal">977</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>SD3Transformer2DModel</code>],
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>, and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection"><code>CLIPTextModelWithProjection</code></a>.</p>
<p>Specific to [<code>StableDiffusion3Pipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SD3LoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`SD3Transformer2DModel`],</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), and</span>
<span class="sd">    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection).</span>

<span class="sd">    Specific to [`StableDiffusion3Pipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="si">}</span><span class="s2">_2&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`SD3Transformer2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights with unet-&gt;transformer</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">            text_encoder_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">            text_encoder_2_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `transformer_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora with unet-&gt;transformer</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora with unet-&gt;transformer</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`SD3Transformer2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`SD3Transformer2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="si">}</span><span class="s2">_2&quot;</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_2_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder_2</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_2_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights with unet-&gt;transformer</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        text_encoder_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">        text_encoder_2_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the second text encoder to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `transformer_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_2_lora_adapter_metadata</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>FluxTransformer2DModel</code>],
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>.</p>
<p>Specific to [<code>StableDiffusion3Pipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span>
<span class="normal">2109</span>
<span class="normal">2110</span>
<span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">FluxLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`FluxTransformer2DModel`],</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).</span>

<span class="sd">    Specific to [`StableDiffusion3Pipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>
    <span class="n">_control_lora_supported_norm_keys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;norm_q&quot;</span><span class="p">,</span> <span class="s2">&quot;norm_k&quot;</span><span class="p">,</span> <span class="s2">&quot;norm_added_q&quot;</span><span class="p">,</span> <span class="s2">&quot;norm_added_k&quot;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">return_alphas</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="c1"># TODO (sayakpaul): to a follow-up to clean and try to unify the conditions.</span>
        <span class="n">is_kohya</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.lora_down.weight&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_kohya</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_kohya_flux_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="c1"># Kohya already takes care of scaling the LoRA parameters with alpha.</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
                <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">is_xlabs</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;processor&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_xlabs</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_xlabs_flux_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="c1"># xlabs doesn&#39;t use `alpha`.</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
                <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">is_bfl_control</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;query_norm.scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_bfl_control</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_bfl_flux_control_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
                <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># For state dicts like</span>
        <span class="c1"># https://huggingface.co/TheLastBen/Jon_Snow_Flux_LoRA</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;alpha&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                <span class="n">alpha_value</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                <span class="c1"># todo: unavailable mint interface</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">alpha_value</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">alpha_value</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">alpha_value</span><span class="p">,</span> <span class="nb">float</span>
                <span class="p">):</span>
                    <span class="n">network_alphas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The alpha key (</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">) seems to be incorrect. If you think this error is unexpected, please open as issue.&quot;</span>
                    <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_alphas</span> <span class="ow">or</span> <span class="n">return_lora_metadata</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
                <span class="n">alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
                <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">return_alphas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="n">has_lora_keys</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="c1"># Flux Control LoRAs also have norm keys</span>
        <span class="n">has_norm_keys</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">norm_key</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">norm_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_lora_supported_norm_keys</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">has_lora_keys</span> <span class="ow">or</span> <span class="n">has_norm_keys</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="n">transformer_lora_state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">k</span>
        <span class="p">}</span>
        <span class="n">transformer_norm_state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">norm_key</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">norm_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_lora_supported_norm_keys</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
        <span class="n">has_param_with_expanded_shape</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">has_param_with_expanded_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_transformer_param_shape_or_error_</span><span class="p">(</span>
                <span class="n">transformer</span><span class="p">,</span> <span class="n">transformer_lora_state_dict</span><span class="p">,</span> <span class="n">transformer_norm_state_dict</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">has_param_with_expanded_shape</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;The LoRA weights contain parameters that have different shapes that expected by the transformer. &quot;</span>
                <span class="s2">&quot;As a result, the state_dict of the transformer has been expanded to match the LoRA parameter shapes. &quot;</span>
                <span class="s2">&quot;To get a comprehensive list of parameter names that were modified, enable debug logging.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">transformer_lora_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_lora_state_dict</span><span class="p">(</span>
                <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span> <span class="n">lora_state_dict</span><span class="o">=</span><span class="n">transformer_lora_state_dict</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">transformer_lora_state_dict</span><span class="p">:</span>
                <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">transformer_lora_state_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]})</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_norm_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_norm_into_transformer</span><span class="p">(</span>
                <span class="n">transformer_norm_state_dict</span><span class="p">,</span>
                <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
                <span class="n">discard_original_layers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            transformer (`FluxTransformer2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_load_norm_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">discard_original_layers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="c1"># Remove prefix if present</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="ow">or</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span><span class="p">:</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="o">.</span><span class="n">removeprefix</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Find invalid keys</span>
        <span class="n">transformer_state_dict</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">transformer_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">transformer_state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">state_dict_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">extra_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict_keys</span> <span class="o">-</span> <span class="n">transformer_keys</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">extra_keys</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unsupported keys found in state dict when trying to load normalization layers into the transformer. The following keys will be ignored:</span><span class="se">\n</span><span class="si">{</span><span class="n">extra_keys</span><span class="si">}</span><span class="s2">.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">extra_keys</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Save the layers that are going to be overwritten so that unload_lora_weights can work as expected</span>
        <span class="n">overwritten_layers_state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">discard_original_layers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">overwritten_layers_state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">transformer_state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;The provided state dict contains normalization layers in addition to LoRA layers. The normalization layers will directly update the state_dict of the transformer &quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="s1">&#39;as opposed to the LoRA layers that will co-exist separately until the &quot;fuse_lora()&quot; method is called. That is to say, the normalization layers will always be directly &#39;</span>  <span class="c1"># noqa: E501</span>
            <span class="s2">&quot;fused into the transformer and can only be unfused if `discard_original_layers=True` is passed. This might also have implications when dealing with multiple LoRAs. &quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="s2">&quot;If you notice something unexpected, please open an issue: https://github.com/huggingface/diffusers/issues.&quot;</span>
        <span class="p">)</span>

        <span class="c1"># We can&#39;t load with strict=True because the current state_dict does not contain all the transformer keys</span>
        <span class="n">param_not_load</span><span class="p">,</span> <span class="n">unexpected_keys</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># We shouldn&#39;t expect to see the supported norm keys here being present in the unexpected keys.</span>
        <span class="k">if</span> <span class="n">unexpected_keys</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">norm_key</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_keys</span> <span class="k">for</span> <span class="n">norm_key</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_control_lora_supported_norm_keys</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">unexpected_keys</span><span class="si">}</span><span class="s2"> as unexpected keys while trying to load norm layers into the transformer.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">overwritten_layers_state_dict</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights with unet-&gt;transformer</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">            text_encoder_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `transformer_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;The provided state dict contains normalization layers in addition to LoRA layers. The normalization layers will be directly updated the state_dict of the transformer &quot;</span>  <span class="c1"># noqa: E501</span>
                <span class="s2">&quot;as opposed to the LoRA layers that will co-exist separately until the &#39;fuse_lora()&#39; method is called. That is to say, the normalization layers will always be directly &quot;</span>  <span class="c1"># noqa: E501</span>
                <span class="s2">&quot;fused into the transformer and can only be unfused if `discard_original_layers=True` is passed.&quot;</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">:</span>
            <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># We override this here account for `_transformer_norm_layers` and `_overwritten_params`.</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reset_to_overwritten_params</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unloads the LoRA parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            reset_to_overwritten_params (`bool`, defaults to `False`): Whether to reset the LoRA-loaded modules</span>
<span class="sd">                to their original params. Refer to the [Flux</span>
<span class="sd">                documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) to learn more.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">        &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>

        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">:</span>
            <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">reset_to_overwritten_params</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_overwritten_params&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">overwritten_params</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_overwritten_params</span>
            <span class="n">module_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">overwritten_params</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">param_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">):</span>
                    <span class="n">module_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">transformer</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">))</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
                    <span class="n">module_weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                    <span class="n">module_bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                    <span class="n">bias</span> <span class="o">=</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

                    <span class="n">parent_module_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">current_module_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                    <span class="n">parent_module</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">parent_module_name</span><span class="p">)</span>

                    <span class="n">current_param_weight</span> <span class="o">=</span> <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">]</span>
                    <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">original_module</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">module_weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                    <span class="n">tmp_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">current_param_weight</span><span class="p">}</span>
                    <span class="k">if</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">tmp_state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.bias&quot;</span><span class="p">]})</span>
                    <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">original_module</span><span class="p">,</span> <span class="n">tmp_state_dict</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">current_module_name</span><span class="p">,</span> <span class="n">original_module</span><span class="p">)</span>

                    <span class="k">del</span> <span class="n">tmp_state_dict</span>

                    <span class="k">if</span> <span class="n">current_module_name</span> <span class="ow">in</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">:</span>
                        <span class="n">attribute_name</span> <span class="o">=</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">[</span><span class="n">current_module_name</span><span class="p">]</span>
                        <span class="n">new_value</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                        <span class="n">old_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">)</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Set the </span><span class="si">{</span><span class="n">attribute_name</span><span class="si">}</span><span class="s2"> attribute of the model to </span><span class="si">{</span><span class="n">new_value</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">old_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_expand_transformer_param_shape_or_error_</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span>
        <span class="n">lora_state_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_state_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Control LoRA expands the shape of the input layer from (3072, 64) to (3072, 128). This method handles that and</span>
<span class="sd">        generalizes things a bit so that any parameter that needs expansion receives appropriate treatment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">lora_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">lora_state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">norm_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">norm_state_dict</span><span class="p">)</span>

        <span class="c1"># Remove prefix if present</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefix</span> <span class="ow">or</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">prefix</span><span class="p">:</span>
                <span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="o">.</span><span class="n">removeprefix</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Expand transformer parameter shapes if they don&#39;t match lora</span>
        <span class="n">has_param_with_shape_update</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">overwritten_params</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">is_peft_loaded</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">transformer</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                <span class="n">module_weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                <span class="n">module_bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

                <span class="n">lora_base_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.base_layer&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_peft_loaded</span> <span class="k">else</span> <span class="n">name</span>
                <span class="n">lora_A_weight_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lora_base_name</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span>
                <span class="n">lora_B_weight_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lora_base_name</span><span class="si">}</span><span class="s2">.lora_B.weight&quot;</span>
                <span class="k">if</span> <span class="n">lora_A_weight_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">in_features</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">lora_A_weight_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">out_features</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">lora_B_weight_name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># Model maybe loaded with different quantization schemes which may flatten the params.</span>
                <span class="c1"># `bitsandbytes`, for example, flatten the weights when using 4bit. 8bit bnb models</span>
                <span class="c1"># preserve weight shape.</span>
                <span class="n">module_weight_shape</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_calculate_module_shape</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span> <span class="n">base_module</span><span class="o">=</span><span class="n">module</span><span class="p">)</span>

                <span class="c1"># This means there&#39;s no need for an expansion in the params, so we simply skip.</span>
                <span class="k">if</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">module_weight_shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">):</span>
                    <span class="k">continue</span>

                <span class="n">module_out_features</span><span class="p">,</span> <span class="n">module_in_features</span> <span class="o">=</span> <span class="n">module_weight_shape</span>
                <span class="n">debug_message</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="k">if</span> <span class="n">in_features</span> <span class="o">&gt;</span> <span class="n">module_in_features</span><span class="p">:</span>
                    <span class="n">debug_message</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="sa">f</span><span class="s1">&#39;Expanding the nn.Linear input/output features for module=&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">&quot; because the provided LoRA &#39;</span>
                        <span class="sa">f</span><span class="s2">&quot;checkpoint contains higher number of features than expected. The number of input_features will be &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;expanded from </span><span class="si">{</span><span class="n">module_in_features</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">out_features</span> <span class="o">&gt;</span> <span class="n">module_out_features</span><span class="p">:</span>
                    <span class="n">debug_message</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="s2">&quot;, and the number of output features will be &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;expanded from </span><span class="si">{</span><span class="n">module_out_features</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">out_features</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">debug_message</span> <span class="o">+=</span> <span class="s2">&quot;.&quot;</span>
                <span class="k">if</span> <span class="n">debug_message</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">debug_message</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">out_features</span> <span class="o">&gt;</span> <span class="n">module_out_features</span> <span class="ow">or</span> <span class="n">in_features</span> <span class="o">&gt;</span> <span class="n">module_in_features</span><span class="p">:</span>
                    <span class="n">has_param_with_shape_update</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">parent_module_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">current_module_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                    <span class="n">parent_module</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">parent_module_name</span><span class="p">)</span>

                    <span class="n">expanded_module</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">module_weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="c1"># Only weights are expanded and biases are not. This is because only the input dimensions</span>
                    <span class="c1"># are changed while the output dimensions remain the same. The shape of the weight tensor</span>
                    <span class="c1"># is (out_features, in_features), while the shape of bias tensor is (out_features,), which</span>
                    <span class="c1"># explains the reason why only weights are expanded.</span>
                    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">expanded_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">module_weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">slices</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">module_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                    <span class="n">new_weight</span><span class="p">[</span><span class="n">slices</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_weight</span>
                    <span class="n">tmp_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">new_weight</span><span class="p">)}</span>
                    <span class="k">if</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">tmp_state_dict</span><span class="p">[</span><span class="s2">&quot;bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_bias</span>
                    <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">expanded_module</span><span class="p">,</span> <span class="n">tmp_state_dict</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                    <span class="nb">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">current_module_name</span><span class="p">,</span> <span class="n">expanded_module</span><span class="p">)</span>

                    <span class="k">del</span> <span class="n">tmp_state_dict</span>

                    <span class="k">if</span> <span class="n">current_module_name</span> <span class="ow">in</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">:</span>
                        <span class="n">attribute_name</span> <span class="o">=</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">[</span><span class="n">current_module_name</span><span class="p">]</span>
                        <span class="n">new_value</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">expanded_module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                        <span class="n">old_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">)</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Set the </span><span class="si">{</span><span class="n">attribute_name</span><span class="si">}</span><span class="s2"> attribute of the model to </span><span class="si">{</span><span class="n">new_value</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">old_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

                    <span class="c1"># For `unload_lora_weights()`.</span>
                    <span class="c1"># TODO: this could lead to more memory overhead if the number of overwritten params</span>
                    <span class="c1"># are large. Should be revisited later and tackled through a `discard_original_layers` arg.</span>
                    <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">current_module_name</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_weight</span>
                    <span class="k">if</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">current_module_name</span><span class="si">}</span><span class="s2">.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">module_bias</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">overwritten_params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">transformer</span><span class="o">.</span><span class="n">_overwritten_params</span> <span class="o">=</span> <span class="n">overwritten_params</span>

        <span class="k">return</span> <span class="n">has_param_with_shape_update</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_expand_lora_state_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">lora_state_dict</span><span class="p">):</span>
        <span class="n">expanded_module_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">transformer_state_dict</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">parameters_dict</span><span class="p">()</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="n">lora_module_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">key</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;.lora_A.weight&quot;</span><span class="p">)]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">lora_state_dict</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.lora_A.weight&quot;</span><span class="p">)]</span>
        <span class="n">lora_module_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">name</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="p">:]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">lora_module_names</span> <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)]</span>
        <span class="n">lora_module_names</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">lora_module_names</span><span class="p">))</span>
        <span class="n">transformer_module_names</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">({</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">transformer</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">()})</span>
        <span class="n">unexpected_modules</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">lora_module_names</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">transformer_module_names</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unexpected_modules</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found unexpected modules: </span><span class="si">{</span><span class="n">unexpected_modules</span><span class="si">}</span><span class="s2">. These will be ignored.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">lora_module_names</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unexpected_modules</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">base_param_name</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.base_layer.weight&quot;</span>
                <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.base_layer.weight&quot;</span> <span class="ow">in</span> <span class="n">transformer_state_dict</span>
                <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.weight&quot;</span>
            <span class="p">)</span>
            <span class="n">base_weight_param</span> <span class="o">=</span> <span class="n">transformer_state_dict</span><span class="p">[</span><span class="n">base_param_name</span><span class="p">]</span>
            <span class="n">lora_A_param</span> <span class="o">=</span> <span class="n">lora_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">k</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span><span class="p">]</span>

            <span class="c1"># TODO (sayakpaul): Handle the cases when we actually need to expand when using quantization.</span>
            <span class="n">base_module_shape</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_calculate_module_shape</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span> <span class="n">base_weight_param_name</span><span class="o">=</span><span class="n">base_param_name</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">base_module_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">lora_A_param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">lora_A_param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">base_weight_param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">expanded_state_dict_weight</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
                <span class="n">expanded_state_dict_weight</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">lora_A_param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">lora_A_param</span>
                <span class="n">lora_state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}{</span><span class="n">k</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">expanded_state_dict_weight</span>
                <span class="n">expanded_module_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">base_module_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lora_A_param</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;This LoRA param (</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">.lora_A.weight) has an incompatible shape </span><span class="si">{</span><span class="n">lora_A_param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. Please open an issue to file for a feature request - https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">expanded_module_names</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following LoRA modules were zero padded to match the state dict of </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">expanded_module_names</span><span class="si">}</span><span class="s2">. Please open an issue if you think this was unexpected - https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">lora_state_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_calculate_module_shape</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="s2">&quot;nn.Cell&quot;</span><span class="p">,</span>
        <span class="n">base_module</span><span class="p">:</span> <span class="s2">&quot;mint.nn.Linear&quot;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">base_weight_param_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_get_weight_shape</span><span class="p">(</span><span class="n">weight</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="c1"># if weight.__class__.__name__ == &quot;Params4bit&quot;:</span>
            <span class="c1">#     return weight.quant_state.shape</span>
            <span class="c1"># elif weight.__class__.__name__ == &quot;GGUFParameter&quot;:</span>
            <span class="c1">#     return weight.quant_shape</span>
            <span class="c1"># else:</span>
            <span class="k">return</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">base_module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_get_weight_shape</span><span class="p">(</span><span class="n">base_module</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">base_weight_param_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">base_weight_param_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid `base_weight_param_name` passed as it does not end with &#39;.weight&#39; </span><span class="si">{</span><span class="n">base_weight_param_name</span><span class="si">=}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">module_path</span> <span class="o">=</span> <span class="n">base_weight_param_name</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">submodule</span> <span class="o">=</span> <span class="n">get_submodule_by_name</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">module_path</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_get_weight_shape</span><span class="p">(</span><span class="n">submodule</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either `base_module` or `base_weight_param_name` must be provided.&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_prepare_outputs</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_alphas</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_metadata</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_dict</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">return_alphas</span><span class="p">:</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_metadata</span><span class="p">:</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">return_alphas</span> <span class="ow">or</span> <span class="n">return_metadata</span><span class="p">)</span> <span class="k">else</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;The provided state dict contains normalization layers in addition to LoRA layers. The normalization layers will be directly updated the state_dict of the transformer &quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="s2">&quot;as opposed to the LoRA layers that will co-exist separately until the &#39;fuse_lora()&#39; method is called. That is to say, the normalization layers will always be directly &quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="s2">&quot;fused into the transformer and can only be unfused if `discard_original_layers=True` is passed.&quot;</span>
        <span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`FluxTransformer2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        transformer (`FluxTransformer2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">return_alphas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>

    <span class="n">has_lora_keys</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="c1"># Flux Control LoRAs also have norm keys</span>
    <span class="n">has_norm_keys</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">norm_key</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">for</span> <span class="n">norm_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_lora_supported_norm_keys</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">has_lora_keys</span> <span class="ow">or</span> <span class="n">has_norm_keys</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="n">transformer_lora_state_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">k</span>
    <span class="p">}</span>
    <span class="n">transformer_norm_state_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">norm_key</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">norm_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_lora_supported_norm_keys</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
    <span class="n">has_param_with_expanded_shape</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">has_param_with_expanded_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_transformer_param_shape_or_error_</span><span class="p">(</span>
            <span class="n">transformer</span><span class="p">,</span> <span class="n">transformer_lora_state_dict</span><span class="p">,</span> <span class="n">transformer_norm_state_dict</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">has_param_with_expanded_shape</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;The LoRA weights contain parameters that have different shapes that expected by the transformer. &quot;</span>
            <span class="s2">&quot;As a result, the state_dict of the transformer has been expanded to match the LoRA parameter shapes. &quot;</span>
            <span class="s2">&quot;To get a comprehensive list of parameter names that were modified, enable debug logging.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_lora_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">transformer_lora_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_lora_state_dict</span><span class="p">(</span>
            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span> <span class="n">lora_state_dict</span><span class="o">=</span><span class="n">transformer_lora_state_dict</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">transformer_lora_state_dict</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">transformer_lora_state_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]})</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_norm_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_norm_into_transformer</span><span class="p">(</span>
            <span class="n">transformer_norm_state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">discard_original_layers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">return_alphas</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">return_alphas</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="c1"># TODO (sayakpaul): to a follow-up to clean and try to unify the conditions.</span>
    <span class="n">is_kohya</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.lora_down.weight&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_kohya</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_kohya_flux_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># Kohya already takes care of scaling the LoRA parameters with alpha.</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
            <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">is_xlabs</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;processor&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_xlabs</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_xlabs_flux_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="c1"># xlabs doesn&#39;t use `alpha`.</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
            <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">is_bfl_control</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;query_norm.scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_bfl_control</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_bfl_flux_control_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
            <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># For state dicts like</span>
    <span class="c1"># https://huggingface.co/TheLastBen/Jon_Snow_Flux_LoRA</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">network_alphas</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;alpha&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
            <span class="n">alpha_value</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="c1"># todo: unavailable mint interface</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">alpha_value</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">alpha_value</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span>
                <span class="n">alpha_value</span><span class="p">,</span> <span class="nb">float</span>
            <span class="p">):</span>
                <span class="n">network_alphas</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The alpha key (</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">) seems to be incorrect. If you think this error is unexpected, please open as issue.&quot;</span>
                <span class="p">)</span>

    <span class="k">if</span> <span class="n">return_alphas</span> <span class="ow">or</span> <span class="n">return_lora_metadata</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_prepare_outputs</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">return_alphas</span><span class="o">=</span><span class="n">return_alphas</span><span class="p">,</span>
            <span class="n">return_metadata</span><span class="o">=</span><span class="n">return_lora_metadata</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights with unet-&gt;transformer</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        text_encoder_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the text encoder to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `transformer_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_adapter_metadata</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">text_encoder_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;, &#39;text_encoder&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">:</span>
        <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unload_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">FluxLoraLoaderMixin</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">(</span><span class="n">reset_to_overwritten_params</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.unload_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Unloads the LoRA parameters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>reset_to_overwritten_params</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to reset the LoRA-loaded modules
to their original params. Refer to the <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux">Flux
documentation</a> to learn more.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="c1"># Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="o">...</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reset_to_overwritten_params</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unloads the LoRA parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        reset_to_overwritten_params (`bool`, defaults to `False`): Whether to reset the LoRA-loaded modules</span>
<span class="sd">            to their original params. Refer to the [Flux</span>
<span class="sd">            documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) to learn more.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">    &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">    &gt;&gt;&gt; ...</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>

    <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_transformer_norm_layers&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">:</span>
        <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">_transformer_norm_layers</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">reset_to_overwritten_params</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="p">,</span> <span class="s2">&quot;_overwritten_params&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">overwritten_params</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">_overwritten_params</span>
        <span class="n">module_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">overwritten_params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">param_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">):</span>
                <span class="n">module_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">param_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.weight&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">transformer</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">))</span> <span class="ow">and</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
                <span class="n">module_weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                <span class="n">module_bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">bias</span> <span class="o">=</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

                <span class="n">parent_module_name</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">current_module_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">rpartition</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
                <span class="n">parent_module</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">get_submodule</span><span class="p">(</span><span class="n">parent_module_name</span><span class="p">)</span>

                <span class="n">current_param_weight</span> <span class="o">=</span> <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.weight&quot;</span><span class="p">]</span>
                <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">original_module</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">module_weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                <span class="n">tmp_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">current_param_weight</span><span class="p">}</span>
                <span class="k">if</span> <span class="n">module_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">tmp_state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">overwritten_params</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">.bias&quot;</span><span class="p">]})</span>
                <span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">original_module</span><span class="p">,</span> <span class="n">tmp_state_dict</span><span class="p">,</span> <span class="n">strict_load</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">parent_module</span><span class="p">,</span> <span class="n">current_module_name</span><span class="p">,</span> <span class="n">original_module</span><span class="p">)</span>

                <span class="k">del</span> <span class="n">tmp_state_dict</span>

                <span class="k">if</span> <span class="n">current_module_name</span> <span class="ow">in</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">:</span>
                    <span class="n">attribute_name</span> <span class="o">=</span> <span class="n">_MODULE_NAME_TO_ATTRIBUTE_MAP_FLUX</span><span class="p">[</span><span class="n">current_module_name</span><span class="p">]</span>
                    <span class="n">new_value</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">current_param_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">old_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">)</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">attribute_name</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Set the </span><span class="si">{</span><span class="n">attribute_name</span><span class="si">}</span><span class="s2"> attribute of the model to </span><span class="si">{</span><span class="n">new_value</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">old_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>CogVideoXTransformer3DModel</code>]. Specific to [<code>CogVideoXPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CogVideoXLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`CogVideoXTransformer3DModel`]. Specific to [`CogVideoXPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;CogVideoXTransformer3DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`CogVideoXTransformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Adapted from mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights without support for text encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CogVideoXTransformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;CogVideoXTransformer3DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`CogVideoXTransformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span>
<span class="normal">2883</span>
<span class="normal">2884</span>
<span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span>
<span class="normal">2798</span>
<span class="normal">2799</span>
<span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span>
<span class="normal">2983</span>
<span class="normal">2984</span>
<span class="normal">2985</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Adapted from mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights without support for text encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">CogVideoXLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>MochiTransformer3DModel</code>]. Specific to [<code>MochiPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Mochi1LoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`MochiTransformer3DModel`]. Specific to [`MochiPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;MochiTransformer3DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`MochiTransformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`MochiTransformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;MochiTransformer3DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`MochiTransformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span>
<span class="normal">3121</span>
<span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Mochi1LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Mochi1LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>LTXVideoTransformer3DModel</code>]. Specific to [<code>LTXPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LTXVideoLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`LTXVideoTransformer3DModel`]. Specific to [`LTXPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">is_non_diffusers_format</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_non_diffusers_format</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_ltxv_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;LTXVideoTransformer3DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`LTXVideoTransformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`LTXVideoTransformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;LTXVideoTransformer3DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`LTXVideoTransformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">is_non_diffusers_format</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_non_diffusers_format</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_ltxv_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span>
<span class="normal">3570</span>
<span class="normal">3571</span>
<span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">LTXVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.LTXVideoLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>SanaTransformer2DModel</code>]. Specific to [<code>SanaPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span>
<span class="normal">3741</span>
<span class="normal">3742</span>
<span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span>
<span class="normal">3796</span>
<span class="normal">3797</span>
<span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span>
<span class="normal">3990</span>
<span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SanaLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`SanaTransformer2DModel`]. Specific to [`SanaPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;SanaTransformer2DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`SanaTransformer2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`SanaTransformer2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span>
<span class="normal">3865</span>
<span class="normal">3866</span>
<span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;SanaTransformer2DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`SanaTransformer2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span>
<span class="normal">3741</span>
<span class="normal">3742</span>
<span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SanaLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3991</span>
<span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>HunyuanVideoTransformer3DModel</code>]. Specific to [<code>HunyuanVideoPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span>
<span class="normal">4116</span>
<span class="normal">4117</span>
<span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span>
<span class="normal">4137</span>
<span class="normal">4138</span>
<span class="normal">4139</span>
<span class="normal">4140</span>
<span class="normal">4141</span>
<span class="normal">4142</span>
<span class="normal">4143</span>
<span class="normal">4144</span>
<span class="normal">4145</span>
<span class="normal">4146</span>
<span class="normal">4147</span>
<span class="normal">4148</span>
<span class="normal">4149</span>
<span class="normal">4150</span>
<span class="normal">4151</span>
<span class="normal">4152</span>
<span class="normal">4153</span>
<span class="normal">4154</span>
<span class="normal">4155</span>
<span class="normal">4156</span>
<span class="normal">4157</span>
<span class="normal">4158</span>
<span class="normal">4159</span>
<span class="normal">4160</span>
<span class="normal">4161</span>
<span class="normal">4162</span>
<span class="normal">4163</span>
<span class="normal">4164</span>
<span class="normal">4165</span>
<span class="normal">4166</span>
<span class="normal">4167</span>
<span class="normal">4168</span>
<span class="normal">4169</span>
<span class="normal">4170</span>
<span class="normal">4171</span>
<span class="normal">4172</span>
<span class="normal">4173</span>
<span class="normal">4174</span>
<span class="normal">4175</span>
<span class="normal">4176</span>
<span class="normal">4177</span>
<span class="normal">4178</span>
<span class="normal">4179</span>
<span class="normal">4180</span>
<span class="normal">4181</span>
<span class="normal">4182</span>
<span class="normal">4183</span>
<span class="normal">4184</span>
<span class="normal">4185</span>
<span class="normal">4186</span>
<span class="normal">4187</span>
<span class="normal">4188</span>
<span class="normal">4189</span>
<span class="normal">4190</span>
<span class="normal">4191</span>
<span class="normal">4192</span>
<span class="normal">4193</span>
<span class="normal">4194</span>
<span class="normal">4195</span>
<span class="normal">4196</span>
<span class="normal">4197</span>
<span class="normal">4198</span>
<span class="normal">4199</span>
<span class="normal">4200</span>
<span class="normal">4201</span>
<span class="normal">4202</span>
<span class="normal">4203</span>
<span class="normal">4204</span>
<span class="normal">4205</span>
<span class="normal">4206</span>
<span class="normal">4207</span>
<span class="normal">4208</span>
<span class="normal">4209</span>
<span class="normal">4210</span>
<span class="normal">4211</span>
<span class="normal">4212</span>
<span class="normal">4213</span>
<span class="normal">4214</span>
<span class="normal">4215</span>
<span class="normal">4216</span>
<span class="normal">4217</span>
<span class="normal">4218</span>
<span class="normal">4219</span>
<span class="normal">4220</span>
<span class="normal">4221</span>
<span class="normal">4222</span>
<span class="normal">4223</span>
<span class="normal">4224</span>
<span class="normal">4225</span>
<span class="normal">4226</span>
<span class="normal">4227</span>
<span class="normal">4228</span>
<span class="normal">4229</span>
<span class="normal">4230</span>
<span class="normal">4231</span>
<span class="normal">4232</span>
<span class="normal">4233</span>
<span class="normal">4234</span>
<span class="normal">4235</span>
<span class="normal">4236</span>
<span class="normal">4237</span>
<span class="normal">4238</span>
<span class="normal">4239</span>
<span class="normal">4240</span>
<span class="normal">4241</span>
<span class="normal">4242</span>
<span class="normal">4243</span>
<span class="normal">4244</span>
<span class="normal">4245</span>
<span class="normal">4246</span>
<span class="normal">4247</span>
<span class="normal">4248</span>
<span class="normal">4249</span>
<span class="normal">4250</span>
<span class="normal">4251</span>
<span class="normal">4252</span>
<span class="normal">4253</span>
<span class="normal">4254</span>
<span class="normal">4255</span>
<span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span>
<span class="normal">4261</span>
<span class="normal">4262</span>
<span class="normal">4263</span>
<span class="normal">4264</span>
<span class="normal">4265</span>
<span class="normal">4266</span>
<span class="normal">4267</span>
<span class="normal">4268</span>
<span class="normal">4269</span>
<span class="normal">4270</span>
<span class="normal">4271</span>
<span class="normal">4272</span>
<span class="normal">4273</span>
<span class="normal">4274</span>
<span class="normal">4275</span>
<span class="normal">4276</span>
<span class="normal">4277</span>
<span class="normal">4278</span>
<span class="normal">4279</span>
<span class="normal">4280</span>
<span class="normal">4281</span>
<span class="normal">4282</span>
<span class="normal">4283</span>
<span class="normal">4284</span>
<span class="normal">4285</span>
<span class="normal">4286</span>
<span class="normal">4287</span>
<span class="normal">4288</span>
<span class="normal">4289</span>
<span class="normal">4290</span>
<span class="normal">4291</span>
<span class="normal">4292</span>
<span class="normal">4293</span>
<span class="normal">4294</span>
<span class="normal">4295</span>
<span class="normal">4296</span>
<span class="normal">4297</span>
<span class="normal">4298</span>
<span class="normal">4299</span>
<span class="normal">4300</span>
<span class="normal">4301</span>
<span class="normal">4302</span>
<span class="normal">4303</span>
<span class="normal">4304</span>
<span class="normal">4305</span>
<span class="normal">4306</span>
<span class="normal">4307</span>
<span class="normal">4308</span>
<span class="normal">4309</span>
<span class="normal">4310</span>
<span class="normal">4311</span>
<span class="normal">4312</span>
<span class="normal">4313</span>
<span class="normal">4314</span>
<span class="normal">4315</span>
<span class="normal">4316</span>
<span class="normal">4317</span>
<span class="normal">4318</span>
<span class="normal">4319</span>
<span class="normal">4320</span>
<span class="normal">4321</span>
<span class="normal">4322</span>
<span class="normal">4323</span>
<span class="normal">4324</span>
<span class="normal">4325</span>
<span class="normal">4326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">HunyuanVideoLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`HunyuanVideoTransformer3DModel`]. Specific to [`HunyuanVideoPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading original format HunyuanVideo LoRA checkpoints.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">is_original_hunyuan_video</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;img_attn_qkv&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_original_hunyuan_video</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_hunyuan_video_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;HunyuanVideoTransformer3DModel # noqa</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`HunyuanVideoTransformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4263</span>
<span class="normal">4264</span>
<span class="normal">4265</span>
<span class="normal">4266</span>
<span class="normal">4267</span>
<span class="normal">4268</span>
<span class="normal">4269</span>
<span class="normal">4270</span>
<span class="normal">4271</span>
<span class="normal">4272</span>
<span class="normal">4273</span>
<span class="normal">4274</span>
<span class="normal">4275</span>
<span class="normal">4276</span>
<span class="normal">4277</span>
<span class="normal">4278</span>
<span class="normal">4279</span>
<span class="normal">4280</span>
<span class="normal">4281</span>
<span class="normal">4282</span>
<span class="normal">4283</span>
<span class="normal">4284</span>
<span class="normal">4285</span>
<span class="normal">4286</span>
<span class="normal">4287</span>
<span class="normal">4288</span>
<span class="normal">4289</span>
<span class="normal">4290</span>
<span class="normal">4291</span>
<span class="normal">4292</span>
<span class="normal">4293</span>
<span class="normal">4294</span>
<span class="normal">4295</span>
<span class="normal">4296</span>
<span class="normal">4297</span>
<span class="normal">4298</span>
<span class="normal">4299</span>
<span class="normal">4300</span>
<span class="normal">4301</span>
<span class="normal">4302</span>
<span class="normal">4303</span>
<span class="normal">4304</span>
<span class="normal">4305</span>
<span class="normal">4306</span>
<span class="normal">4307</span>
<span class="normal">4308</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`HunyuanVideoTransformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4164</span>
<span class="normal">4165</span>
<span class="normal">4166</span>
<span class="normal">4167</span>
<span class="normal">4168</span>
<span class="normal">4169</span>
<span class="normal">4170</span>
<span class="normal">4171</span>
<span class="normal">4172</span>
<span class="normal">4173</span>
<span class="normal">4174</span>
<span class="normal">4175</span>
<span class="normal">4176</span>
<span class="normal">4177</span>
<span class="normal">4178</span>
<span class="normal">4179</span>
<span class="normal">4180</span>
<span class="normal">4181</span>
<span class="normal">4182</span>
<span class="normal">4183</span>
<span class="normal">4184</span>
<span class="normal">4185</span>
<span class="normal">4186</span>
<span class="normal">4187</span>
<span class="normal">4188</span>
<span class="normal">4189</span>
<span class="normal">4190</span>
<span class="normal">4191</span>
<span class="normal">4192</span>
<span class="normal">4193</span>
<span class="normal">4194</span>
<span class="normal">4195</span>
<span class="normal">4196</span>
<span class="normal">4197</span>
<span class="normal">4198</span>
<span class="normal">4199</span>
<span class="normal">4200</span>
<span class="normal">4201</span>
<span class="normal">4202</span>
<span class="normal">4203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;HunyuanVideoTransformer3DModel # noqa</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`HunyuanVideoTransformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span>
<span class="normal">4137</span>
<span class="normal">4138</span>
<span class="normal">4139</span>
<span class="normal">4140</span>
<span class="normal">4141</span>
<span class="normal">4142</span>
<span class="normal">4143</span>
<span class="normal">4144</span>
<span class="normal">4145</span>
<span class="normal">4146</span>
<span class="normal">4147</span>
<span class="normal">4148</span>
<span class="normal">4149</span>
<span class="normal">4150</span>
<span class="normal">4151</span>
<span class="normal">4152</span>
<span class="normal">4153</span>
<span class="normal">4154</span>
<span class="normal">4155</span>
<span class="normal">4156</span>
<span class="normal">4157</span>
<span class="normal">4158</span>
<span class="normal">4159</span>
<span class="normal">4160</span>
<span class="normal">4161</span>
<span class="normal">4162</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading original format HunyuanVideo LoRA checkpoints.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span>
<span class="normal">4089</span>
<span class="normal">4090</span>
<span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading original format HunyuanVideo LoRA checkpoints.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">is_original_hunyuan_video</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;img_attn_qkv&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_original_hunyuan_video</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_hunyuan_video_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4205</span>
<span class="normal">4206</span>
<span class="normal">4207</span>
<span class="normal">4208</span>
<span class="normal">4209</span>
<span class="normal">4210</span>
<span class="normal">4211</span>
<span class="normal">4212</span>
<span class="normal">4213</span>
<span class="normal">4214</span>
<span class="normal">4215</span>
<span class="normal">4216</span>
<span class="normal">4217</span>
<span class="normal">4218</span>
<span class="normal">4219</span>
<span class="normal">4220</span>
<span class="normal">4221</span>
<span class="normal">4222</span>
<span class="normal">4223</span>
<span class="normal">4224</span>
<span class="normal">4225</span>
<span class="normal">4226</span>
<span class="normal">4227</span>
<span class="normal">4228</span>
<span class="normal">4229</span>
<span class="normal">4230</span>
<span class="normal">4231</span>
<span class="normal">4232</span>
<span class="normal">4233</span>
<span class="normal">4234</span>
<span class="normal">4235</span>
<span class="normal">4236</span>
<span class="normal">4237</span>
<span class="normal">4238</span>
<span class="normal">4239</span>
<span class="normal">4240</span>
<span class="normal">4241</span>
<span class="normal">4242</span>
<span class="normal">4243</span>
<span class="normal">4244</span>
<span class="normal">4245</span>
<span class="normal">4246</span>
<span class="normal">4247</span>
<span class="normal">4248</span>
<span class="normal">4249</span>
<span class="normal">4250</span>
<span class="normal">4251</span>
<span class="normal">4252</span>
<span class="normal">4253</span>
<span class="normal">4254</span>
<span class="normal">4255</span>
<span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, mindspore.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">HunyuanVideoLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.HunyuanVideoLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4311</span>
<span class="normal">4312</span>
<span class="normal">4313</span>
<span class="normal">4314</span>
<span class="normal">4315</span>
<span class="normal">4316</span>
<span class="normal">4317</span>
<span class="normal">4318</span>
<span class="normal">4319</span>
<span class="normal">4320</span>
<span class="normal">4321</span>
<span class="normal">4322</span>
<span class="normal">4323</span>
<span class="normal">4324</span>
<span class="normal">4325</span>
<span class="normal">4326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>Lumina2Transformer2DModel</code>]. Specific to [<code>Lumina2Text2ImgPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4329</span>
<span class="normal">4330</span>
<span class="normal">4331</span>
<span class="normal">4332</span>
<span class="normal">4333</span>
<span class="normal">4334</span>
<span class="normal">4335</span>
<span class="normal">4336</span>
<span class="normal">4337</span>
<span class="normal">4338</span>
<span class="normal">4339</span>
<span class="normal">4340</span>
<span class="normal">4341</span>
<span class="normal">4342</span>
<span class="normal">4343</span>
<span class="normal">4344</span>
<span class="normal">4345</span>
<span class="normal">4346</span>
<span class="normal">4347</span>
<span class="normal">4348</span>
<span class="normal">4349</span>
<span class="normal">4350</span>
<span class="normal">4351</span>
<span class="normal">4352</span>
<span class="normal">4353</span>
<span class="normal">4354</span>
<span class="normal">4355</span>
<span class="normal">4356</span>
<span class="normal">4357</span>
<span class="normal">4358</span>
<span class="normal">4359</span>
<span class="normal">4360</span>
<span class="normal">4361</span>
<span class="normal">4362</span>
<span class="normal">4363</span>
<span class="normal">4364</span>
<span class="normal">4365</span>
<span class="normal">4366</span>
<span class="normal">4367</span>
<span class="normal">4368</span>
<span class="normal">4369</span>
<span class="normal">4370</span>
<span class="normal">4371</span>
<span class="normal">4372</span>
<span class="normal">4373</span>
<span class="normal">4374</span>
<span class="normal">4375</span>
<span class="normal">4376</span>
<span class="normal">4377</span>
<span class="normal">4378</span>
<span class="normal">4379</span>
<span class="normal">4380</span>
<span class="normal">4381</span>
<span class="normal">4382</span>
<span class="normal">4383</span>
<span class="normal">4384</span>
<span class="normal">4385</span>
<span class="normal">4386</span>
<span class="normal">4387</span>
<span class="normal">4388</span>
<span class="normal">4389</span>
<span class="normal">4390</span>
<span class="normal">4391</span>
<span class="normal">4392</span>
<span class="normal">4393</span>
<span class="normal">4394</span>
<span class="normal">4395</span>
<span class="normal">4396</span>
<span class="normal">4397</span>
<span class="normal">4398</span>
<span class="normal">4399</span>
<span class="normal">4400</span>
<span class="normal">4401</span>
<span class="normal">4402</span>
<span class="normal">4403</span>
<span class="normal">4404</span>
<span class="normal">4405</span>
<span class="normal">4406</span>
<span class="normal">4407</span>
<span class="normal">4408</span>
<span class="normal">4409</span>
<span class="normal">4410</span>
<span class="normal">4411</span>
<span class="normal">4412</span>
<span class="normal">4413</span>
<span class="normal">4414</span>
<span class="normal">4415</span>
<span class="normal">4416</span>
<span class="normal">4417</span>
<span class="normal">4418</span>
<span class="normal">4419</span>
<span class="normal">4420</span>
<span class="normal">4421</span>
<span class="normal">4422</span>
<span class="normal">4423</span>
<span class="normal">4424</span>
<span class="normal">4425</span>
<span class="normal">4426</span>
<span class="normal">4427</span>
<span class="normal">4428</span>
<span class="normal">4429</span>
<span class="normal">4430</span>
<span class="normal">4431</span>
<span class="normal">4432</span>
<span class="normal">4433</span>
<span class="normal">4434</span>
<span class="normal">4435</span>
<span class="normal">4436</span>
<span class="normal">4437</span>
<span class="normal">4438</span>
<span class="normal">4439</span>
<span class="normal">4440</span>
<span class="normal">4441</span>
<span class="normal">4442</span>
<span class="normal">4443</span>
<span class="normal">4444</span>
<span class="normal">4445</span>
<span class="normal">4446</span>
<span class="normal">4447</span>
<span class="normal">4448</span>
<span class="normal">4449</span>
<span class="normal">4450</span>
<span class="normal">4451</span>
<span class="normal">4452</span>
<span class="normal">4453</span>
<span class="normal">4454</span>
<span class="normal">4455</span>
<span class="normal">4456</span>
<span class="normal">4457</span>
<span class="normal">4458</span>
<span class="normal">4459</span>
<span class="normal">4460</span>
<span class="normal">4461</span>
<span class="normal">4462</span>
<span class="normal">4463</span>
<span class="normal">4464</span>
<span class="normal">4465</span>
<span class="normal">4466</span>
<span class="normal">4467</span>
<span class="normal">4468</span>
<span class="normal">4469</span>
<span class="normal">4470</span>
<span class="normal">4471</span>
<span class="normal">4472</span>
<span class="normal">4473</span>
<span class="normal">4474</span>
<span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span>
<span class="normal">4484</span>
<span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span>
<span class="normal">4506</span>
<span class="normal">4507</span>
<span class="normal">4508</span>
<span class="normal">4509</span>
<span class="normal">4510</span>
<span class="normal">4511</span>
<span class="normal">4512</span>
<span class="normal">4513</span>
<span class="normal">4514</span>
<span class="normal">4515</span>
<span class="normal">4516</span>
<span class="normal">4517</span>
<span class="normal">4518</span>
<span class="normal">4519</span>
<span class="normal">4520</span>
<span class="normal">4521</span>
<span class="normal">4522</span>
<span class="normal">4523</span>
<span class="normal">4524</span>
<span class="normal">4525</span>
<span class="normal">4526</span>
<span class="normal">4527</span>
<span class="normal">4528</span>
<span class="normal">4529</span>
<span class="normal">4530</span>
<span class="normal">4531</span>
<span class="normal">4532</span>
<span class="normal">4533</span>
<span class="normal">4534</span>
<span class="normal">4535</span>
<span class="normal">4536</span>
<span class="normal">4537</span>
<span class="normal">4538</span>
<span class="normal">4539</span>
<span class="normal">4540</span>
<span class="normal">4541</span>
<span class="normal">4542</span>
<span class="normal">4543</span>
<span class="normal">4544</span>
<span class="normal">4545</span>
<span class="normal">4546</span>
<span class="normal">4547</span>
<span class="normal">4548</span>
<span class="normal">4549</span>
<span class="normal">4550</span>
<span class="normal">4551</span>
<span class="normal">4552</span>
<span class="normal">4553</span>
<span class="normal">4554</span>
<span class="normal">4555</span>
<span class="normal">4556</span>
<span class="normal">4557</span>
<span class="normal">4558</span>
<span class="normal">4559</span>
<span class="normal">4560</span>
<span class="normal">4561</span>
<span class="normal">4562</span>
<span class="normal">4563</span>
<span class="normal">4564</span>
<span class="normal">4565</span>
<span class="normal">4566</span>
<span class="normal">4567</span>
<span class="normal">4568</span>
<span class="normal">4569</span>
<span class="normal">4570</span>
<span class="normal">4571</span>
<span class="normal">4572</span>
<span class="normal">4573</span>
<span class="normal">4574</span>
<span class="normal">4575</span>
<span class="normal">4576</span>
<span class="normal">4577</span>
<span class="normal">4578</span>
<span class="normal">4579</span>
<span class="normal">4580</span>
<span class="normal">4581</span>
<span class="normal">4582</span>
<span class="normal">4583</span>
<span class="normal">4584</span>
<span class="normal">4585</span>
<span class="normal">4586</span>
<span class="normal">4587</span>
<span class="normal">4588</span>
<span class="normal">4589</span>
<span class="normal">4590</span>
<span class="normal">4591</span>
<span class="normal">4592</span>
<span class="normal">4593</span>
<span class="normal">4594</span>
<span class="normal">4595</span>
<span class="normal">4596</span>
<span class="normal">4597</span>
<span class="normal">4598</span>
<span class="normal">4599</span>
<span class="normal">4600</span>
<span class="normal">4601</span>
<span class="normal">4602</span>
<span class="normal">4603</span>
<span class="normal">4604</span>
<span class="normal">4605</span>
<span class="normal">4606</span>
<span class="normal">4607</span>
<span class="normal">4608</span>
<span class="normal">4609</span>
<span class="normal">4610</span>
<span class="normal">4611</span>
<span class="normal">4612</span>
<span class="normal">4613</span>
<span class="normal">4614</span>
<span class="normal">4615</span>
<span class="normal">4616</span>
<span class="normal">4617</span>
<span class="normal">4618</span>
<span class="normal">4619</span>
<span class="normal">4620</span>
<span class="normal">4621</span>
<span class="normal">4622</span>
<span class="normal">4623</span>
<span class="normal">4624</span>
<span class="normal">4625</span>
<span class="normal">4626</span>
<span class="normal">4627</span>
<span class="normal">4628</span>
<span class="normal">4629</span>
<span class="normal">4630</span>
<span class="normal">4631</span>
<span class="normal">4632</span>
<span class="normal">4633</span>
<span class="normal">4634</span>
<span class="normal">4635</span>
<span class="normal">4636</span>
<span class="normal">4637</span>
<span class="normal">4638</span>
<span class="normal">4639</span>
<span class="normal">4640</span>
<span class="normal">4641</span>
<span class="normal">4642</span>
<span class="normal">4643</span>
<span class="normal">4644</span>
<span class="normal">4645</span>
<span class="normal">4646</span>
<span class="normal">4647</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Lumina2LoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`Lumina2Transformer2DModel`]. Specific to [`Lumina2Text2ImgPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="c1"># conversion.</span>
        <span class="n">non_diffusers</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">non_diffusers</span><span class="p">:</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lumina2_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;Lumina2Transformer2DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`Lumina2Transformer2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SanaLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4584</span>
<span class="normal">4585</span>
<span class="normal">4586</span>
<span class="normal">4587</span>
<span class="normal">4588</span>
<span class="normal">4589</span>
<span class="normal">4590</span>
<span class="normal">4591</span>
<span class="normal">4592</span>
<span class="normal">4593</span>
<span class="normal">4594</span>
<span class="normal">4595</span>
<span class="normal">4596</span>
<span class="normal">4597</span>
<span class="normal">4598</span>
<span class="normal">4599</span>
<span class="normal">4600</span>
<span class="normal">4601</span>
<span class="normal">4602</span>
<span class="normal">4603</span>
<span class="normal">4604</span>
<span class="normal">4605</span>
<span class="normal">4606</span>
<span class="normal">4607</span>
<span class="normal">4608</span>
<span class="normal">4609</span>
<span class="normal">4610</span>
<span class="normal">4611</span>
<span class="normal">4612</span>
<span class="normal">4613</span>
<span class="normal">4614</span>
<span class="normal">4615</span>
<span class="normal">4616</span>
<span class="normal">4617</span>
<span class="normal">4618</span>
<span class="normal">4619</span>
<span class="normal">4620</span>
<span class="normal">4621</span>
<span class="normal">4622</span>
<span class="normal">4623</span>
<span class="normal">4624</span>
<span class="normal">4625</span>
<span class="normal">4626</span>
<span class="normal">4627</span>
<span class="normal">4628</span>
<span class="normal">4629</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Lumina2Transformer2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span>
<span class="normal">4506</span>
<span class="normal">4507</span>
<span class="normal">4508</span>
<span class="normal">4509</span>
<span class="normal">4510</span>
<span class="normal">4511</span>
<span class="normal">4512</span>
<span class="normal">4513</span>
<span class="normal">4514</span>
<span class="normal">4515</span>
<span class="normal">4516</span>
<span class="normal">4517</span>
<span class="normal">4518</span>
<span class="normal">4519</span>
<span class="normal">4520</span>
<span class="normal">4521</span>
<span class="normal">4522</span>
<span class="normal">4523</span>
<span class="normal">4524</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;Lumina2Transformer2DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`Lumina2Transformer2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4439</span>
<span class="normal">4440</span>
<span class="normal">4441</span>
<span class="normal">4442</span>
<span class="normal">4443</span>
<span class="normal">4444</span>
<span class="normal">4445</span>
<span class="normal">4446</span>
<span class="normal">4447</span>
<span class="normal">4448</span>
<span class="normal">4449</span>
<span class="normal">4450</span>
<span class="normal">4451</span>
<span class="normal">4452</span>
<span class="normal">4453</span>
<span class="normal">4454</span>
<span class="normal">4455</span>
<span class="normal">4456</span>
<span class="normal">4457</span>
<span class="normal">4458</span>
<span class="normal">4459</span>
<span class="normal">4460</span>
<span class="normal">4461</span>
<span class="normal">4462</span>
<span class="normal">4463</span>
<span class="normal">4464</span>
<span class="normal">4465</span>
<span class="normal">4466</span>
<span class="normal">4467</span>
<span class="normal">4468</span>
<span class="normal">4469</span>
<span class="normal">4470</span>
<span class="normal">4471</span>
<span class="normal">4472</span>
<span class="normal">4473</span>
<span class="normal">4474</span>
<span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4337</span>
<span class="normal">4338</span>
<span class="normal">4339</span>
<span class="normal">4340</span>
<span class="normal">4341</span>
<span class="normal">4342</span>
<span class="normal">4343</span>
<span class="normal">4344</span>
<span class="normal">4345</span>
<span class="normal">4346</span>
<span class="normal">4347</span>
<span class="normal">4348</span>
<span class="normal">4349</span>
<span class="normal">4350</span>
<span class="normal">4351</span>
<span class="normal">4352</span>
<span class="normal">4353</span>
<span class="normal">4354</span>
<span class="normal">4355</span>
<span class="normal">4356</span>
<span class="normal">4357</span>
<span class="normal">4358</span>
<span class="normal">4359</span>
<span class="normal">4360</span>
<span class="normal">4361</span>
<span class="normal">4362</span>
<span class="normal">4363</span>
<span class="normal">4364</span>
<span class="normal">4365</span>
<span class="normal">4366</span>
<span class="normal">4367</span>
<span class="normal">4368</span>
<span class="normal">4369</span>
<span class="normal">4370</span>
<span class="normal">4371</span>
<span class="normal">4372</span>
<span class="normal">4373</span>
<span class="normal">4374</span>
<span class="normal">4375</span>
<span class="normal">4376</span>
<span class="normal">4377</span>
<span class="normal">4378</span>
<span class="normal">4379</span>
<span class="normal">4380</span>
<span class="normal">4381</span>
<span class="normal">4382</span>
<span class="normal">4383</span>
<span class="normal">4384</span>
<span class="normal">4385</span>
<span class="normal">4386</span>
<span class="normal">4387</span>
<span class="normal">4388</span>
<span class="normal">4389</span>
<span class="normal">4390</span>
<span class="normal">4391</span>
<span class="normal">4392</span>
<span class="normal">4393</span>
<span class="normal">4394</span>
<span class="normal">4395</span>
<span class="normal">4396</span>
<span class="normal">4397</span>
<span class="normal">4398</span>
<span class="normal">4399</span>
<span class="normal">4400</span>
<span class="normal">4401</span>
<span class="normal">4402</span>
<span class="normal">4403</span>
<span class="normal">4404</span>
<span class="normal">4405</span>
<span class="normal">4406</span>
<span class="normal">4407</span>
<span class="normal">4408</span>
<span class="normal">4409</span>
<span class="normal">4410</span>
<span class="normal">4411</span>
<span class="normal">4412</span>
<span class="normal">4413</span>
<span class="normal">4414</span>
<span class="normal">4415</span>
<span class="normal">4416</span>
<span class="normal">4417</span>
<span class="normal">4418</span>
<span class="normal">4419</span>
<span class="normal">4420</span>
<span class="normal">4421</span>
<span class="normal">4422</span>
<span class="normal">4423</span>
<span class="normal">4424</span>
<span class="normal">4425</span>
<span class="normal">4426</span>
<span class="normal">4427</span>
<span class="normal">4428</span>
<span class="normal">4429</span>
<span class="normal">4430</span>
<span class="normal">4431</span>
<span class="normal">4432</span>
<span class="normal">4433</span>
<span class="normal">4434</span>
<span class="normal">4435</span>
<span class="normal">4436</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="c1"># conversion.</span>
    <span class="n">non_diffusers</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">non_diffusers</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lumina2_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4526</span>
<span class="normal">4527</span>
<span class="normal">4528</span>
<span class="normal">4529</span>
<span class="normal">4530</span>
<span class="normal">4531</span>
<span class="normal">4532</span>
<span class="normal">4533</span>
<span class="normal">4534</span>
<span class="normal">4535</span>
<span class="normal">4536</span>
<span class="normal">4537</span>
<span class="normal">4538</span>
<span class="normal">4539</span>
<span class="normal">4540</span>
<span class="normal">4541</span>
<span class="normal">4542</span>
<span class="normal">4543</span>
<span class="normal">4544</span>
<span class="normal">4545</span>
<span class="normal">4546</span>
<span class="normal">4547</span>
<span class="normal">4548</span>
<span class="normal">4549</span>
<span class="normal">4550</span>
<span class="normal">4551</span>
<span class="normal">4552</span>
<span class="normal">4553</span>
<span class="normal">4554</span>
<span class="normal">4555</span>
<span class="normal">4556</span>
<span class="normal">4557</span>
<span class="normal">4558</span>
<span class="normal">4559</span>
<span class="normal">4560</span>
<span class="normal">4561</span>
<span class="normal">4562</span>
<span class="normal">4563</span>
<span class="normal">4564</span>
<span class="normal">4565</span>
<span class="normal">4566</span>
<span class="normal">4567</span>
<span class="normal">4568</span>
<span class="normal">4569</span>
<span class="normal">4570</span>
<span class="normal">4571</span>
<span class="normal">4572</span>
<span class="normal">4573</span>
<span class="normal">4574</span>
<span class="normal">4575</span>
<span class="normal">4576</span>
<span class="normal">4577</span>
<span class="normal">4578</span>
<span class="normal">4579</span>
<span class="normal">4580</span>
<span class="normal">4581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">Lumina2LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.Lumina2LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4632</span>
<span class="normal">4633</span>
<span class="normal">4634</span>
<span class="normal">4635</span>
<span class="normal">4636</span>
<span class="normal">4637</span>
<span class="normal">4638</span>
<span class="normal">4639</span>
<span class="normal">4640</span>
<span class="normal">4641</span>
<span class="normal">4642</span>
<span class="normal">4643</span>
<span class="normal">4644</span>
<span class="normal">4645</span>
<span class="normal">4646</span>
<span class="normal">4647</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>WanTransformer3DModel</code>]. Specific to [<code>WanPipeline</code>] and <code>[WanImageToVideoPipeline</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4650</span>
<span class="normal">4651</span>
<span class="normal">4652</span>
<span class="normal">4653</span>
<span class="normal">4654</span>
<span class="normal">4655</span>
<span class="normal">4656</span>
<span class="normal">4657</span>
<span class="normal">4658</span>
<span class="normal">4659</span>
<span class="normal">4660</span>
<span class="normal">4661</span>
<span class="normal">4662</span>
<span class="normal">4663</span>
<span class="normal">4664</span>
<span class="normal">4665</span>
<span class="normal">4666</span>
<span class="normal">4667</span>
<span class="normal">4668</span>
<span class="normal">4669</span>
<span class="normal">4670</span>
<span class="normal">4671</span>
<span class="normal">4672</span>
<span class="normal">4673</span>
<span class="normal">4674</span>
<span class="normal">4675</span>
<span class="normal">4676</span>
<span class="normal">4677</span>
<span class="normal">4678</span>
<span class="normal">4679</span>
<span class="normal">4680</span>
<span class="normal">4681</span>
<span class="normal">4682</span>
<span class="normal">4683</span>
<span class="normal">4684</span>
<span class="normal">4685</span>
<span class="normal">4686</span>
<span class="normal">4687</span>
<span class="normal">4688</span>
<span class="normal">4689</span>
<span class="normal">4690</span>
<span class="normal">4691</span>
<span class="normal">4692</span>
<span class="normal">4693</span>
<span class="normal">4694</span>
<span class="normal">4695</span>
<span class="normal">4696</span>
<span class="normal">4697</span>
<span class="normal">4698</span>
<span class="normal">4699</span>
<span class="normal">4700</span>
<span class="normal">4701</span>
<span class="normal">4702</span>
<span class="normal">4703</span>
<span class="normal">4704</span>
<span class="normal">4705</span>
<span class="normal">4706</span>
<span class="normal">4707</span>
<span class="normal">4708</span>
<span class="normal">4709</span>
<span class="normal">4710</span>
<span class="normal">4711</span>
<span class="normal">4712</span>
<span class="normal">4713</span>
<span class="normal">4714</span>
<span class="normal">4715</span>
<span class="normal">4716</span>
<span class="normal">4717</span>
<span class="normal">4718</span>
<span class="normal">4719</span>
<span class="normal">4720</span>
<span class="normal">4721</span>
<span class="normal">4722</span>
<span class="normal">4723</span>
<span class="normal">4724</span>
<span class="normal">4725</span>
<span class="normal">4726</span>
<span class="normal">4727</span>
<span class="normal">4728</span>
<span class="normal">4729</span>
<span class="normal">4730</span>
<span class="normal">4731</span>
<span class="normal">4732</span>
<span class="normal">4733</span>
<span class="normal">4734</span>
<span class="normal">4735</span>
<span class="normal">4736</span>
<span class="normal">4737</span>
<span class="normal">4738</span>
<span class="normal">4739</span>
<span class="normal">4740</span>
<span class="normal">4741</span>
<span class="normal">4742</span>
<span class="normal">4743</span>
<span class="normal">4744</span>
<span class="normal">4745</span>
<span class="normal">4746</span>
<span class="normal">4747</span>
<span class="normal">4748</span>
<span class="normal">4749</span>
<span class="normal">4750</span>
<span class="normal">4751</span>
<span class="normal">4752</span>
<span class="normal">4753</span>
<span class="normal">4754</span>
<span class="normal">4755</span>
<span class="normal">4756</span>
<span class="normal">4757</span>
<span class="normal">4758</span>
<span class="normal">4759</span>
<span class="normal">4760</span>
<span class="normal">4761</span>
<span class="normal">4762</span>
<span class="normal">4763</span>
<span class="normal">4764</span>
<span class="normal">4765</span>
<span class="normal">4766</span>
<span class="normal">4767</span>
<span class="normal">4768</span>
<span class="normal">4769</span>
<span class="normal">4770</span>
<span class="normal">4771</span>
<span class="normal">4772</span>
<span class="normal">4773</span>
<span class="normal">4774</span>
<span class="normal">4775</span>
<span class="normal">4776</span>
<span class="normal">4777</span>
<span class="normal">4778</span>
<span class="normal">4779</span>
<span class="normal">4780</span>
<span class="normal">4781</span>
<span class="normal">4782</span>
<span class="normal">4783</span>
<span class="normal">4784</span>
<span class="normal">4785</span>
<span class="normal">4786</span>
<span class="normal">4787</span>
<span class="normal">4788</span>
<span class="normal">4789</span>
<span class="normal">4790</span>
<span class="normal">4791</span>
<span class="normal">4792</span>
<span class="normal">4793</span>
<span class="normal">4794</span>
<span class="normal">4795</span>
<span class="normal">4796</span>
<span class="normal">4797</span>
<span class="normal">4798</span>
<span class="normal">4799</span>
<span class="normal">4800</span>
<span class="normal">4801</span>
<span class="normal">4802</span>
<span class="normal">4803</span>
<span class="normal">4804</span>
<span class="normal">4805</span>
<span class="normal">4806</span>
<span class="normal">4807</span>
<span class="normal">4808</span>
<span class="normal">4809</span>
<span class="normal">4810</span>
<span class="normal">4811</span>
<span class="normal">4812</span>
<span class="normal">4813</span>
<span class="normal">4814</span>
<span class="normal">4815</span>
<span class="normal">4816</span>
<span class="normal">4817</span>
<span class="normal">4818</span>
<span class="normal">4819</span>
<span class="normal">4820</span>
<span class="normal">4821</span>
<span class="normal">4822</span>
<span class="normal">4823</span>
<span class="normal">4824</span>
<span class="normal">4825</span>
<span class="normal">4826</span>
<span class="normal">4827</span>
<span class="normal">4828</span>
<span class="normal">4829</span>
<span class="normal">4830</span>
<span class="normal">4831</span>
<span class="normal">4832</span>
<span class="normal">4833</span>
<span class="normal">4834</span>
<span class="normal">4835</span>
<span class="normal">4836</span>
<span class="normal">4837</span>
<span class="normal">4838</span>
<span class="normal">4839</span>
<span class="normal">4840</span>
<span class="normal">4841</span>
<span class="normal">4842</span>
<span class="normal">4843</span>
<span class="normal">4844</span>
<span class="normal">4845</span>
<span class="normal">4846</span>
<span class="normal">4847</span>
<span class="normal">4848</span>
<span class="normal">4849</span>
<span class="normal">4850</span>
<span class="normal">4851</span>
<span class="normal">4852</span>
<span class="normal">4853</span>
<span class="normal">4854</span>
<span class="normal">4855</span>
<span class="normal">4856</span>
<span class="normal">4857</span>
<span class="normal">4858</span>
<span class="normal">4859</span>
<span class="normal">4860</span>
<span class="normal">4861</span>
<span class="normal">4862</span>
<span class="normal">4863</span>
<span class="normal">4864</span>
<span class="normal">4865</span>
<span class="normal">4866</span>
<span class="normal">4867</span>
<span class="normal">4868</span>
<span class="normal">4869</span>
<span class="normal">4870</span>
<span class="normal">4871</span>
<span class="normal">4872</span>
<span class="normal">4873</span>
<span class="normal">4874</span>
<span class="normal">4875</span>
<span class="normal">4876</span>
<span class="normal">4877</span>
<span class="normal">4878</span>
<span class="normal">4879</span>
<span class="normal">4880</span>
<span class="normal">4881</span>
<span class="normal">4882</span>
<span class="normal">4883</span>
<span class="normal">4884</span>
<span class="normal">4885</span>
<span class="normal">4886</span>
<span class="normal">4887</span>
<span class="normal">4888</span>
<span class="normal">4889</span>
<span class="normal">4890</span>
<span class="normal">4891</span>
<span class="normal">4892</span>
<span class="normal">4893</span>
<span class="normal">4894</span>
<span class="normal">4895</span>
<span class="normal">4896</span>
<span class="normal">4897</span>
<span class="normal">4898</span>
<span class="normal">4899</span>
<span class="normal">4900</span>
<span class="normal">4901</span>
<span class="normal">4902</span>
<span class="normal">4903</span>
<span class="normal">4904</span>
<span class="normal">4905</span>
<span class="normal">4906</span>
<span class="normal">4907</span>
<span class="normal">4908</span>
<span class="normal">4909</span>
<span class="normal">4910</span>
<span class="normal">4911</span>
<span class="normal">4912</span>
<span class="normal">4913</span>
<span class="normal">4914</span>
<span class="normal">4915</span>
<span class="normal">4916</span>
<span class="normal">4917</span>
<span class="normal">4918</span>
<span class="normal">4919</span>
<span class="normal">4920</span>
<span class="normal">4921</span>
<span class="normal">4922</span>
<span class="normal">4923</span>
<span class="normal">4924</span>
<span class="normal">4925</span>
<span class="normal">4926</span>
<span class="normal">4927</span>
<span class="normal">4928</span>
<span class="normal">4929</span>
<span class="normal">4930</span>
<span class="normal">4931</span>
<span class="normal">4932</span>
<span class="normal">4933</span>
<span class="normal">4934</span>
<span class="normal">4935</span>
<span class="normal">4936</span>
<span class="normal">4937</span>
<span class="normal">4938</span>
<span class="normal">4939</span>
<span class="normal">4940</span>
<span class="normal">4941</span>
<span class="normal">4942</span>
<span class="normal">4943</span>
<span class="normal">4944</span>
<span class="normal">4945</span>
<span class="normal">4946</span>
<span class="normal">4947</span>
<span class="normal">4948</span>
<span class="normal">4949</span>
<span class="normal">4950</span>
<span class="normal">4951</span>
<span class="normal">4952</span>
<span class="normal">4953</span>
<span class="normal">4954</span>
<span class="normal">4955</span>
<span class="normal">4956</span>
<span class="normal">4957</span>
<span class="normal">4958</span>
<span class="normal">4959</span>
<span class="normal">4960</span>
<span class="normal">4961</span>
<span class="normal">4962</span>
<span class="normal">4963</span>
<span class="normal">4964</span>
<span class="normal">4965</span>
<span class="normal">4966</span>
<span class="normal">4967</span>
<span class="normal">4968</span>
<span class="normal">4969</span>
<span class="normal">4970</span>
<span class="normal">4971</span>
<span class="normal">4972</span>
<span class="normal">4973</span>
<span class="normal">4974</span>
<span class="normal">4975</span>
<span class="normal">4976</span>
<span class="normal">4977</span>
<span class="normal">4978</span>
<span class="normal">4979</span>
<span class="normal">4980</span>
<span class="normal">4981</span>
<span class="normal">4982</span>
<span class="normal">4983</span>
<span class="normal">4984</span>
<span class="normal">4985</span>
<span class="normal">4986</span>
<span class="normal">4987</span>
<span class="normal">4988</span>
<span class="normal">4989</span>
<span class="normal">4990</span>
<span class="normal">4991</span>
<span class="normal">4992</span>
<span class="normal">4993</span>
<span class="normal">4994</span>
<span class="normal">4995</span>
<span class="normal">4996</span>
<span class="normal">4997</span>
<span class="normal">4998</span>
<span class="normal">4999</span>
<span class="normal">5000</span>
<span class="normal">5001</span>
<span class="normal">5002</span>
<span class="normal">5003</span>
<span class="normal">5004</span>
<span class="normal">5005</span>
<span class="normal">5006</span>
<span class="normal">5007</span>
<span class="normal">5008</span>
<span class="normal">5009</span>
<span class="normal">5010</span>
<span class="normal">5011</span>
<span class="normal">5012</span>
<span class="normal">5013</span>
<span class="normal">5014</span>
<span class="normal">5015</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">WanLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`WanTransformer3DModel`]. Specific to [`WanPipeline`] and `[WanImageToVideoPipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_musubi_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">image_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;transformer.blocks.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">num_blocks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">({</span><span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;blocks.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span> <span class="k">if</span> <span class="s2">&quot;blocks.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">})</span>
            <span class="n">is_i2v_lora</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;add_k_proj&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;add_v_proj&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.lora_B.bias&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">is_i2v_lora</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">state_dict</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;k_img&quot;</span><span class="p">,</span> <span class="s2">&quot;v_img&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;add_k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;add_v_proj&quot;</span><span class="p">]):</span>
                    <span class="c1"># These keys should exist if the block `i` was part of the T2V LoRA.</span>
                    <span class="n">ref_key_lora_A</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_A.weight&quot;</span>
                    <span class="n">ref_key_lora_B</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.weight&quot;</span>

                    <span class="k">if</span> <span class="n">ref_key_lora_A</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span> <span class="ow">or</span> <span class="n">ref_key_lora_B</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_A.weight&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_B.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.weight&quot;</span><span class="p">]</span>
                    <span class="p">)</span>

                    <span class="c1"># If the original LoRA had biases (indicated by has_bias)</span>
                    <span class="c1"># AND the specific reference bias key exists for this block.</span>

                    <span class="n">ref_key_lora_B_bias</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.bias&quot;</span>
                    <span class="k">if</span> <span class="n">has_bias</span> <span class="ow">and</span> <span class="n">ref_key_lora_B_bias</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                        <span class="n">ref_lora_B_bias_tensor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">ref_key_lora_B_bias</span><span class="p">]</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_B.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                            <span class="n">ref_lora_B_bias_tensor</span><span class="p">,</span>
                        <span class="p">)</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># convert T2V LoRA to I2V LoRA (when loaded to Wan I2V) by adding zeros for the additional (missing) _img layers</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;WanTransformer3DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`WanTransformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4952</span>
<span class="normal">4953</span>
<span class="normal">4954</span>
<span class="normal">4955</span>
<span class="normal">4956</span>
<span class="normal">4957</span>
<span class="normal">4958</span>
<span class="normal">4959</span>
<span class="normal">4960</span>
<span class="normal">4961</span>
<span class="normal">4962</span>
<span class="normal">4963</span>
<span class="normal">4964</span>
<span class="normal">4965</span>
<span class="normal">4966</span>
<span class="normal">4967</span>
<span class="normal">4968</span>
<span class="normal">4969</span>
<span class="normal">4970</span>
<span class="normal">4971</span>
<span class="normal">4972</span>
<span class="normal">4973</span>
<span class="normal">4974</span>
<span class="normal">4975</span>
<span class="normal">4976</span>
<span class="normal">4977</span>
<span class="normal">4978</span>
<span class="normal">4979</span>
<span class="normal">4980</span>
<span class="normal">4981</span>
<span class="normal">4982</span>
<span class="normal">4983</span>
<span class="normal">4984</span>
<span class="normal">4985</span>
<span class="normal">4986</span>
<span class="normal">4987</span>
<span class="normal">4988</span>
<span class="normal">4989</span>
<span class="normal">4990</span>
<span class="normal">4991</span>
<span class="normal">4992</span>
<span class="normal">4993</span>
<span class="normal">4994</span>
<span class="normal">4995</span>
<span class="normal">4996</span>
<span class="normal">4997</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`WanTransformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4853</span>
<span class="normal">4854</span>
<span class="normal">4855</span>
<span class="normal">4856</span>
<span class="normal">4857</span>
<span class="normal">4858</span>
<span class="normal">4859</span>
<span class="normal">4860</span>
<span class="normal">4861</span>
<span class="normal">4862</span>
<span class="normal">4863</span>
<span class="normal">4864</span>
<span class="normal">4865</span>
<span class="normal">4866</span>
<span class="normal">4867</span>
<span class="normal">4868</span>
<span class="normal">4869</span>
<span class="normal">4870</span>
<span class="normal">4871</span>
<span class="normal">4872</span>
<span class="normal">4873</span>
<span class="normal">4874</span>
<span class="normal">4875</span>
<span class="normal">4876</span>
<span class="normal">4877</span>
<span class="normal">4878</span>
<span class="normal">4879</span>
<span class="normal">4880</span>
<span class="normal">4881</span>
<span class="normal">4882</span>
<span class="normal">4883</span>
<span class="normal">4884</span>
<span class="normal">4885</span>
<span class="normal">4886</span>
<span class="normal">4887</span>
<span class="normal">4888</span>
<span class="normal">4889</span>
<span class="normal">4890</span>
<span class="normal">4891</span>
<span class="normal">4892</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;WanTransformer3DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`WanTransformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4803</span>
<span class="normal">4804</span>
<span class="normal">4805</span>
<span class="normal">4806</span>
<span class="normal">4807</span>
<span class="normal">4808</span>
<span class="normal">4809</span>
<span class="normal">4810</span>
<span class="normal">4811</span>
<span class="normal">4812</span>
<span class="normal">4813</span>
<span class="normal">4814</span>
<span class="normal">4815</span>
<span class="normal">4816</span>
<span class="normal">4817</span>
<span class="normal">4818</span>
<span class="normal">4819</span>
<span class="normal">4820</span>
<span class="normal">4821</span>
<span class="normal">4822</span>
<span class="normal">4823</span>
<span class="normal">4824</span>
<span class="normal">4825</span>
<span class="normal">4826</span>
<span class="normal">4827</span>
<span class="normal">4828</span>
<span class="normal">4829</span>
<span class="normal">4830</span>
<span class="normal">4831</span>
<span class="normal">4832</span>
<span class="normal">4833</span>
<span class="normal">4834</span>
<span class="normal">4835</span>
<span class="normal">4836</span>
<span class="normal">4837</span>
<span class="normal">4838</span>
<span class="normal">4839</span>
<span class="normal">4840</span>
<span class="normal">4841</span>
<span class="normal">4842</span>
<span class="normal">4843</span>
<span class="normal">4844</span>
<span class="normal">4845</span>
<span class="normal">4846</span>
<span class="normal">4847</span>
<span class="normal">4848</span>
<span class="normal">4849</span>
<span class="normal">4850</span>
<span class="normal">4851</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># convert T2V LoRA to I2V LoRA (when loaded to Wan I2V) by adding zeros for the additional (missing) _img layers</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4658</span>
<span class="normal">4659</span>
<span class="normal">4660</span>
<span class="normal">4661</span>
<span class="normal">4662</span>
<span class="normal">4663</span>
<span class="normal">4664</span>
<span class="normal">4665</span>
<span class="normal">4666</span>
<span class="normal">4667</span>
<span class="normal">4668</span>
<span class="normal">4669</span>
<span class="normal">4670</span>
<span class="normal">4671</span>
<span class="normal">4672</span>
<span class="normal">4673</span>
<span class="normal">4674</span>
<span class="normal">4675</span>
<span class="normal">4676</span>
<span class="normal">4677</span>
<span class="normal">4678</span>
<span class="normal">4679</span>
<span class="normal">4680</span>
<span class="normal">4681</span>
<span class="normal">4682</span>
<span class="normal">4683</span>
<span class="normal">4684</span>
<span class="normal">4685</span>
<span class="normal">4686</span>
<span class="normal">4687</span>
<span class="normal">4688</span>
<span class="normal">4689</span>
<span class="normal">4690</span>
<span class="normal">4691</span>
<span class="normal">4692</span>
<span class="normal">4693</span>
<span class="normal">4694</span>
<span class="normal">4695</span>
<span class="normal">4696</span>
<span class="normal">4697</span>
<span class="normal">4698</span>
<span class="normal">4699</span>
<span class="normal">4700</span>
<span class="normal">4701</span>
<span class="normal">4702</span>
<span class="normal">4703</span>
<span class="normal">4704</span>
<span class="normal">4705</span>
<span class="normal">4706</span>
<span class="normal">4707</span>
<span class="normal">4708</span>
<span class="normal">4709</span>
<span class="normal">4710</span>
<span class="normal">4711</span>
<span class="normal">4712</span>
<span class="normal">4713</span>
<span class="normal">4714</span>
<span class="normal">4715</span>
<span class="normal">4716</span>
<span class="normal">4717</span>
<span class="normal">4718</span>
<span class="normal">4719</span>
<span class="normal">4720</span>
<span class="normal">4721</span>
<span class="normal">4722</span>
<span class="normal">4723</span>
<span class="normal">4724</span>
<span class="normal">4725</span>
<span class="normal">4726</span>
<span class="normal">4727</span>
<span class="normal">4728</span>
<span class="normal">4729</span>
<span class="normal">4730</span>
<span class="normal">4731</span>
<span class="normal">4732</span>
<span class="normal">4733</span>
<span class="normal">4734</span>
<span class="normal">4735</span>
<span class="normal">4736</span>
<span class="normal">4737</span>
<span class="normal">4738</span>
<span class="normal">4739</span>
<span class="normal">4740</span>
<span class="normal">4741</span>
<span class="normal">4742</span>
<span class="normal">4743</span>
<span class="normal">4744</span>
<span class="normal">4745</span>
<span class="normal">4746</span>
<span class="normal">4747</span>
<span class="normal">4748</span>
<span class="normal">4749</span>
<span class="normal">4750</span>
<span class="normal">4751</span>
<span class="normal">4752</span>
<span class="normal">4753</span>
<span class="normal">4754</span>
<span class="normal">4755</span>
<span class="normal">4756</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_musubi_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4894</span>
<span class="normal">4895</span>
<span class="normal">4896</span>
<span class="normal">4897</span>
<span class="normal">4898</span>
<span class="normal">4899</span>
<span class="normal">4900</span>
<span class="normal">4901</span>
<span class="normal">4902</span>
<span class="normal">4903</span>
<span class="normal">4904</span>
<span class="normal">4905</span>
<span class="normal">4906</span>
<span class="normal">4907</span>
<span class="normal">4908</span>
<span class="normal">4909</span>
<span class="normal">4910</span>
<span class="normal">4911</span>
<span class="normal">4912</span>
<span class="normal">4913</span>
<span class="normal">4914</span>
<span class="normal">4915</span>
<span class="normal">4916</span>
<span class="normal">4917</span>
<span class="normal">4918</span>
<span class="normal">4919</span>
<span class="normal">4920</span>
<span class="normal">4921</span>
<span class="normal">4922</span>
<span class="normal">4923</span>
<span class="normal">4924</span>
<span class="normal">4925</span>
<span class="normal">4926</span>
<span class="normal">4927</span>
<span class="normal">4928</span>
<span class="normal">4929</span>
<span class="normal">4930</span>
<span class="normal">4931</span>
<span class="normal">4932</span>
<span class="normal">4933</span>
<span class="normal">4934</span>
<span class="normal">4935</span>
<span class="normal">4936</span>
<span class="normal">4937</span>
<span class="normal">4938</span>
<span class="normal">4939</span>
<span class="normal">4940</span>
<span class="normal">4941</span>
<span class="normal">4942</span>
<span class="normal">4943</span>
<span class="normal">4944</span>
<span class="normal">4945</span>
<span class="normal">4946</span>
<span class="normal">4947</span>
<span class="normal">4948</span>
<span class="normal">4949</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">WanLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5000</span>
<span class="normal">5001</span>
<span class="normal">5002</span>
<span class="normal">5003</span>
<span class="normal">5004</span>
<span class="normal">5005</span>
<span class="normal">5006</span>
<span class="normal">5007</span>
<span class="normal">5008</span>
<span class="normal">5009</span>
<span class="normal">5010</span>
<span class="normal">5011</span>
<span class="normal">5012</span>
<span class="normal">5013</span>
<span class="normal">5014</span>
<span class="normal">5015</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>SkyReelsV2Transformer3DModel</code>].</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5018</span>
<span class="normal">5019</span>
<span class="normal">5020</span>
<span class="normal">5021</span>
<span class="normal">5022</span>
<span class="normal">5023</span>
<span class="normal">5024</span>
<span class="normal">5025</span>
<span class="normal">5026</span>
<span class="normal">5027</span>
<span class="normal">5028</span>
<span class="normal">5029</span>
<span class="normal">5030</span>
<span class="normal">5031</span>
<span class="normal">5032</span>
<span class="normal">5033</span>
<span class="normal">5034</span>
<span class="normal">5035</span>
<span class="normal">5036</span>
<span class="normal">5037</span>
<span class="normal">5038</span>
<span class="normal">5039</span>
<span class="normal">5040</span>
<span class="normal">5041</span>
<span class="normal">5042</span>
<span class="normal">5043</span>
<span class="normal">5044</span>
<span class="normal">5045</span>
<span class="normal">5046</span>
<span class="normal">5047</span>
<span class="normal">5048</span>
<span class="normal">5049</span>
<span class="normal">5050</span>
<span class="normal">5051</span>
<span class="normal">5052</span>
<span class="normal">5053</span>
<span class="normal">5054</span>
<span class="normal">5055</span>
<span class="normal">5056</span>
<span class="normal">5057</span>
<span class="normal">5058</span>
<span class="normal">5059</span>
<span class="normal">5060</span>
<span class="normal">5061</span>
<span class="normal">5062</span>
<span class="normal">5063</span>
<span class="normal">5064</span>
<span class="normal">5065</span>
<span class="normal">5066</span>
<span class="normal">5067</span>
<span class="normal">5068</span>
<span class="normal">5069</span>
<span class="normal">5070</span>
<span class="normal">5071</span>
<span class="normal">5072</span>
<span class="normal">5073</span>
<span class="normal">5074</span>
<span class="normal">5075</span>
<span class="normal">5076</span>
<span class="normal">5077</span>
<span class="normal">5078</span>
<span class="normal">5079</span>
<span class="normal">5080</span>
<span class="normal">5081</span>
<span class="normal">5082</span>
<span class="normal">5083</span>
<span class="normal">5084</span>
<span class="normal">5085</span>
<span class="normal">5086</span>
<span class="normal">5087</span>
<span class="normal">5088</span>
<span class="normal">5089</span>
<span class="normal">5090</span>
<span class="normal">5091</span>
<span class="normal">5092</span>
<span class="normal">5093</span>
<span class="normal">5094</span>
<span class="normal">5095</span>
<span class="normal">5096</span>
<span class="normal">5097</span>
<span class="normal">5098</span>
<span class="normal">5099</span>
<span class="normal">5100</span>
<span class="normal">5101</span>
<span class="normal">5102</span>
<span class="normal">5103</span>
<span class="normal">5104</span>
<span class="normal">5105</span>
<span class="normal">5106</span>
<span class="normal">5107</span>
<span class="normal">5108</span>
<span class="normal">5109</span>
<span class="normal">5110</span>
<span class="normal">5111</span>
<span class="normal">5112</span>
<span class="normal">5113</span>
<span class="normal">5114</span>
<span class="normal">5115</span>
<span class="normal">5116</span>
<span class="normal">5117</span>
<span class="normal">5118</span>
<span class="normal">5119</span>
<span class="normal">5120</span>
<span class="normal">5121</span>
<span class="normal">5122</span>
<span class="normal">5123</span>
<span class="normal">5124</span>
<span class="normal">5125</span>
<span class="normal">5126</span>
<span class="normal">5127</span>
<span class="normal">5128</span>
<span class="normal">5129</span>
<span class="normal">5130</span>
<span class="normal">5131</span>
<span class="normal">5132</span>
<span class="normal">5133</span>
<span class="normal">5134</span>
<span class="normal">5135</span>
<span class="normal">5136</span>
<span class="normal">5137</span>
<span class="normal">5138</span>
<span class="normal">5139</span>
<span class="normal">5140</span>
<span class="normal">5141</span>
<span class="normal">5142</span>
<span class="normal">5143</span>
<span class="normal">5144</span>
<span class="normal">5145</span>
<span class="normal">5146</span>
<span class="normal">5147</span>
<span class="normal">5148</span>
<span class="normal">5149</span>
<span class="normal">5150</span>
<span class="normal">5151</span>
<span class="normal">5152</span>
<span class="normal">5153</span>
<span class="normal">5154</span>
<span class="normal">5155</span>
<span class="normal">5156</span>
<span class="normal">5157</span>
<span class="normal">5158</span>
<span class="normal">5159</span>
<span class="normal">5160</span>
<span class="normal">5161</span>
<span class="normal">5162</span>
<span class="normal">5163</span>
<span class="normal">5164</span>
<span class="normal">5165</span>
<span class="normal">5166</span>
<span class="normal">5167</span>
<span class="normal">5168</span>
<span class="normal">5169</span>
<span class="normal">5170</span>
<span class="normal">5171</span>
<span class="normal">5172</span>
<span class="normal">5173</span>
<span class="normal">5174</span>
<span class="normal">5175</span>
<span class="normal">5176</span>
<span class="normal">5177</span>
<span class="normal">5178</span>
<span class="normal">5179</span>
<span class="normal">5180</span>
<span class="normal">5181</span>
<span class="normal">5182</span>
<span class="normal">5183</span>
<span class="normal">5184</span>
<span class="normal">5185</span>
<span class="normal">5186</span>
<span class="normal">5187</span>
<span class="normal">5188</span>
<span class="normal">5189</span>
<span class="normal">5190</span>
<span class="normal">5191</span>
<span class="normal">5192</span>
<span class="normal">5193</span>
<span class="normal">5194</span>
<span class="normal">5195</span>
<span class="normal">5196</span>
<span class="normal">5197</span>
<span class="normal">5198</span>
<span class="normal">5199</span>
<span class="normal">5200</span>
<span class="normal">5201</span>
<span class="normal">5202</span>
<span class="normal">5203</span>
<span class="normal">5204</span>
<span class="normal">5205</span>
<span class="normal">5206</span>
<span class="normal">5207</span>
<span class="normal">5208</span>
<span class="normal">5209</span>
<span class="normal">5210</span>
<span class="normal">5211</span>
<span class="normal">5212</span>
<span class="normal">5213</span>
<span class="normal">5214</span>
<span class="normal">5215</span>
<span class="normal">5216</span>
<span class="normal">5217</span>
<span class="normal">5218</span>
<span class="normal">5219</span>
<span class="normal">5220</span>
<span class="normal">5221</span>
<span class="normal">5222</span>
<span class="normal">5223</span>
<span class="normal">5224</span>
<span class="normal">5225</span>
<span class="normal">5226</span>
<span class="normal">5227</span>
<span class="normal">5228</span>
<span class="normal">5229</span>
<span class="normal">5230</span>
<span class="normal">5231</span>
<span class="normal">5232</span>
<span class="normal">5233</span>
<span class="normal">5234</span>
<span class="normal">5235</span>
<span class="normal">5236</span>
<span class="normal">5237</span>
<span class="normal">5238</span>
<span class="normal">5239</span>
<span class="normal">5240</span>
<span class="normal">5241</span>
<span class="normal">5242</span>
<span class="normal">5243</span>
<span class="normal">5244</span>
<span class="normal">5245</span>
<span class="normal">5246</span>
<span class="normal">5247</span>
<span class="normal">5248</span>
<span class="normal">5249</span>
<span class="normal">5250</span>
<span class="normal">5251</span>
<span class="normal">5252</span>
<span class="normal">5253</span>
<span class="normal">5254</span>
<span class="normal">5255</span>
<span class="normal">5256</span>
<span class="normal">5257</span>
<span class="normal">5258</span>
<span class="normal">5259</span>
<span class="normal">5260</span>
<span class="normal">5261</span>
<span class="normal">5262</span>
<span class="normal">5263</span>
<span class="normal">5264</span>
<span class="normal">5265</span>
<span class="normal">5266</span>
<span class="normal">5267</span>
<span class="normal">5268</span>
<span class="normal">5269</span>
<span class="normal">5270</span>
<span class="normal">5271</span>
<span class="normal">5272</span>
<span class="normal">5273</span>
<span class="normal">5274</span>
<span class="normal">5275</span>
<span class="normal">5276</span>
<span class="normal">5277</span>
<span class="normal">5278</span>
<span class="normal">5279</span>
<span class="normal">5280</span>
<span class="normal">5281</span>
<span class="normal">5282</span>
<span class="normal">5283</span>
<span class="normal">5284</span>
<span class="normal">5285</span>
<span class="normal">5286</span>
<span class="normal">5287</span>
<span class="normal">5288</span>
<span class="normal">5289</span>
<span class="normal">5290</span>
<span class="normal">5291</span>
<span class="normal">5292</span>
<span class="normal">5293</span>
<span class="normal">5294</span>
<span class="normal">5295</span>
<span class="normal">5296</span>
<span class="normal">5297</span>
<span class="normal">5298</span>
<span class="normal">5299</span>
<span class="normal">5300</span>
<span class="normal">5301</span>
<span class="normal">5302</span>
<span class="normal">5303</span>
<span class="normal">5304</span>
<span class="normal">5305</span>
<span class="normal">5306</span>
<span class="normal">5307</span>
<span class="normal">5308</span>
<span class="normal">5309</span>
<span class="normal">5310</span>
<span class="normal">5311</span>
<span class="normal">5312</span>
<span class="normal">5313</span>
<span class="normal">5314</span>
<span class="normal">5315</span>
<span class="normal">5316</span>
<span class="normal">5317</span>
<span class="normal">5318</span>
<span class="normal">5319</span>
<span class="normal">5320</span>
<span class="normal">5321</span>
<span class="normal">5322</span>
<span class="normal">5323</span>
<span class="normal">5324</span>
<span class="normal">5325</span>
<span class="normal">5326</span>
<span class="normal">5327</span>
<span class="normal">5328</span>
<span class="normal">5329</span>
<span class="normal">5330</span>
<span class="normal">5331</span>
<span class="normal">5332</span>
<span class="normal">5333</span>
<span class="normal">5334</span>
<span class="normal">5335</span>
<span class="normal">5336</span>
<span class="normal">5337</span>
<span class="normal">5338</span>
<span class="normal">5339</span>
<span class="normal">5340</span>
<span class="normal">5341</span>
<span class="normal">5342</span>
<span class="normal">5343</span>
<span class="normal">5344</span>
<span class="normal">5345</span>
<span class="normal">5346</span>
<span class="normal">5347</span>
<span class="normal">5348</span>
<span class="normal">5349</span>
<span class="normal">5350</span>
<span class="normal">5351</span>
<span class="normal">5352</span>
<span class="normal">5353</span>
<span class="normal">5354</span>
<span class="normal">5355</span>
<span class="normal">5356</span>
<span class="normal">5357</span>
<span class="normal">5358</span>
<span class="normal">5359</span>
<span class="normal">5360</span>
<span class="normal">5361</span>
<span class="normal">5362</span>
<span class="normal">5363</span>
<span class="normal">5364</span>
<span class="normal">5365</span>
<span class="normal">5366</span>
<span class="normal">5367</span>
<span class="normal">5368</span>
<span class="normal">5369</span>
<span class="normal">5370</span>
<span class="normal">5371</span>
<span class="normal">5372</span>
<span class="normal">5373</span>
<span class="normal">5374</span>
<span class="normal">5375</span>
<span class="normal">5376</span>
<span class="normal">5377</span>
<span class="normal">5378</span>
<span class="normal">5379</span>
<span class="normal">5380</span>
<span class="normal">5381</span>
<span class="normal">5382</span>
<span class="normal">5383</span>
<span class="normal">5384</span>
<span class="normal">5385</span>
<span class="normal">5386</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SkyReelsV2LoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`SkyReelsV2Transformer3DModel`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">                When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_musubi_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.WanLoraLoaderMixin._maybe_expand_t2v_lora_for_i2v</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">transformer</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">image_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state_dict</span>

        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;transformer.blocks.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
            <span class="n">num_blocks</span> <span class="o">=</span> <span class="nb">len</span><span class="p">({</span><span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;blocks.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span> <span class="k">if</span> <span class="s2">&quot;blocks.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">})</span>
            <span class="n">is_i2v_lora</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;add_k_proj&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;add_v_proj&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
            <span class="n">has_bias</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;.lora_B.bias&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">is_i2v_lora</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">state_dict</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">o</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;k_img&quot;</span><span class="p">,</span> <span class="s2">&quot;v_img&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;add_k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;add_v_proj&quot;</span><span class="p">]):</span>
                    <span class="c1"># These keys should exist if the block `i` was part of the T2V LoRA.</span>
                    <span class="n">ref_key_lora_A</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_A.weight&quot;</span>
                    <span class="n">ref_key_lora_B</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.weight&quot;</span>

                    <span class="k">if</span> <span class="n">ref_key_lora_A</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span> <span class="ow">or</span> <span class="n">ref_key_lora_B</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_A.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_A.weight&quot;</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_B.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.weight&quot;</span><span class="p">]</span>
                    <span class="p">)</span>

                    <span class="c1"># If the original LoRA had biases (indicated by has_bias)</span>
                    <span class="c1"># AND the specific reference bias key exists for this block.</span>

                    <span class="n">ref_key_lora_B_bias</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.to_k.lora_B.bias&quot;</span>
                    <span class="k">if</span> <span class="n">has_bias</span> <span class="ow">and</span> <span class="n">ref_key_lora_B_bias</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
                        <span class="n">ref_lora_B_bias_tensor</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">ref_key_lora_B_bias</span><span class="p">]</span>
                        <span class="n">state_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;transformer.blocks.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.attn2.</span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">.lora_B.bias&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                            <span class="n">ref_lora_B_bias_tensor</span>
                        <span class="p">)</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.load_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">        `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">        [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># convert T2V LoRA to I2V LoRA (when loaded to Wan I2V) by adding zeros for the additional (missing) _img layers</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;SkyReelsV2Transformer3DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`SkyReelsV2Transformer3DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, ms.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `ms.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">            transformer_lora_adapter_metadata:</span>
<span class="sd">                LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
            <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.fuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        ).to(&quot;cuda&quot;)</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
            <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.unfuse_lora</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5323</span>
<span class="normal">5324</span>
<span class="normal">5325</span>
<span class="normal">5326</span>
<span class="normal">5327</span>
<span class="normal">5328</span>
<span class="normal">5329</span>
<span class="normal">5330</span>
<span class="normal">5331</span>
<span class="normal">5332</span>
<span class="normal">5333</span>
<span class="normal">5334</span>
<span class="normal">5335</span>
<span class="normal">5336</span>
<span class="normal">5337</span>
<span class="normal">5338</span>
<span class="normal">5339</span>
<span class="normal">5340</span>
<span class="normal">5341</span>
<span class="normal">5342</span>
<span class="normal">5343</span>
<span class="normal">5344</span>
<span class="normal">5345</span>
<span class="normal">5346</span>
<span class="normal">5347</span>
<span class="normal">5348</span>
<span class="normal">5349</span>
<span class="normal">5350</span>
<span class="normal">5351</span>
<span class="normal">5352</span>
<span class="normal">5353</span>
<span class="normal">5354</span>
<span class="normal">5355</span>
<span class="normal">5356</span>
<span class="normal">5357</span>
<span class="normal">5358</span>
<span class="normal">5359</span>
<span class="normal">5360</span>
<span class="normal">5361</span>
<span class="normal">5362</span>
<span class="normal">5363</span>
<span class="normal">5364</span>
<span class="normal">5365</span>
<span class="normal">5366</span>
<span class="normal">5367</span>
<span class="normal">5368</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    ).to(&quot;cuda&quot;)</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`SkyReelsV2Transformer3DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5224</span>
<span class="normal">5225</span>
<span class="normal">5226</span>
<span class="normal">5227</span>
<span class="normal">5228</span>
<span class="normal">5229</span>
<span class="normal">5230</span>
<span class="normal">5231</span>
<span class="normal">5232</span>
<span class="normal">5233</span>
<span class="normal">5234</span>
<span class="normal">5235</span>
<span class="normal">5236</span>
<span class="normal">5237</span>
<span class="normal">5238</span>
<span class="normal">5239</span>
<span class="normal">5240</span>
<span class="normal">5241</span>
<span class="normal">5242</span>
<span class="normal">5243</span>
<span class="normal">5244</span>
<span class="normal">5245</span>
<span class="normal">5246</span>
<span class="normal">5247</span>
<span class="normal">5248</span>
<span class="normal">5249</span>
<span class="normal">5250</span>
<span class="normal">5251</span>
<span class="normal">5252</span>
<span class="normal">5253</span>
<span class="normal">5254</span>
<span class="normal">5255</span>
<span class="normal">5256</span>
<span class="normal">5257</span>
<span class="normal">5258</span>
<span class="normal">5259</span>
<span class="normal">5260</span>
<span class="normal">5261</span>
<span class="normal">5262</span>
<span class="normal">5263</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer with SD3Transformer2DModel-&gt;SkyReelsV2Transformer3DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`SkyReelsV2Transformer3DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.transformer</code> and
<code>self.text_encoder</code>. All kwargs are forwarded to <code>self.lora_state_dict</code>. See
[<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is loaded.
See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kwargs</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5174</span>
<span class="normal">5175</span>
<span class="normal">5176</span>
<span class="normal">5177</span>
<span class="normal">5178</span>
<span class="normal">5179</span>
<span class="normal">5180</span>
<span class="normal">5181</span>
<span class="normal">5182</span>
<span class="normal">5183</span>
<span class="normal">5184</span>
<span class="normal">5185</span>
<span class="normal">5186</span>
<span class="normal">5187</span>
<span class="normal">5188</span>
<span class="normal">5189</span>
<span class="normal">5190</span>
<span class="normal">5191</span>
<span class="normal">5192</span>
<span class="normal">5193</span>
<span class="normal">5194</span>
<span class="normal">5195</span>
<span class="normal">5196</span>
<span class="normal">5197</span>
<span class="normal">5198</span>
<span class="normal">5199</span>
<span class="normal">5200</span>
<span class="normal">5201</span>
<span class="normal">5202</span>
<span class="normal">5203</span>
<span class="normal">5204</span>
<span class="normal">5205</span>
<span class="normal">5206</span>
<span class="normal">5207</span>
<span class="normal">5208</span>
<span class="normal">5209</span>
<span class="normal">5210</span>
<span class="normal">5211</span>
<span class="normal">5212</span>
<span class="normal">5213</span>
<span class="normal">5214</span>
<span class="normal">5215</span>
<span class="normal">5216</span>
<span class="normal">5217</span>
<span class="normal">5218</span>
<span class="normal">5219</span>
<span class="normal">5220</span>
<span class="normal">5221</span>
<span class="normal">5222</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer` and</span>
<span class="sd">    `self.text_encoder`. All kwargs are forwarded to `self.lora_state_dict`. See</span>
<span class="sd">    [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.</span>
<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># convert T2V LoRA to I2V LoRA (when loaded to Wan I2V) by adding zeros for the additional (missing) _img layers</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_expand_t2v_lora_for_i2v</span><span class="p">(</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>pretrained_model_name_or_path_or_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache_dir</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>force_download</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>proxies</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>local_files_only</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>token</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>revision</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>subfolder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_lora_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5026</span>
<span class="normal">5027</span>
<span class="normal">5028</span>
<span class="normal">5029</span>
<span class="normal">5030</span>
<span class="normal">5031</span>
<span class="normal">5032</span>
<span class="normal">5033</span>
<span class="normal">5034</span>
<span class="normal">5035</span>
<span class="normal">5036</span>
<span class="normal">5037</span>
<span class="normal">5038</span>
<span class="normal">5039</span>
<span class="normal">5040</span>
<span class="normal">5041</span>
<span class="normal">5042</span>
<span class="normal">5043</span>
<span class="normal">5044</span>
<span class="normal">5045</span>
<span class="normal">5046</span>
<span class="normal">5047</span>
<span class="normal">5048</span>
<span class="normal">5049</span>
<span class="normal">5050</span>
<span class="normal">5051</span>
<span class="normal">5052</span>
<span class="normal">5053</span>
<span class="normal">5054</span>
<span class="normal">5055</span>
<span class="normal">5056</span>
<span class="normal">5057</span>
<span class="normal">5058</span>
<span class="normal">5059</span>
<span class="normal">5060</span>
<span class="normal">5061</span>
<span class="normal">5062</span>
<span class="normal">5063</span>
<span class="normal">5064</span>
<span class="normal">5065</span>
<span class="normal">5066</span>
<span class="normal">5067</span>
<span class="normal">5068</span>
<span class="normal">5069</span>
<span class="normal">5070</span>
<span class="normal">5071</span>
<span class="normal">5072</span>
<span class="normal">5073</span>
<span class="normal">5074</span>
<span class="normal">5075</span>
<span class="normal">5076</span>
<span class="normal">5077</span>
<span class="normal">5078</span>
<span class="normal">5079</span>
<span class="normal">5080</span>
<span class="normal">5081</span>
<span class="normal">5082</span>
<span class="normal">5083</span>
<span class="normal">5084</span>
<span class="normal">5085</span>
<span class="normal">5086</span>
<span class="normal">5087</span>
<span class="normal">5088</span>
<span class="normal">5089</span>
<span class="normal">5090</span>
<span class="normal">5091</span>
<span class="normal">5092</span>
<span class="normal">5093</span>
<span class="normal">5094</span>
<span class="normal">5095</span>
<span class="normal">5096</span>
<span class="normal">5097</span>
<span class="normal">5098</span>
<span class="normal">5099</span>
<span class="normal">5100</span>
<span class="normal">5101</span>
<span class="normal">5102</span>
<span class="normal">5103</span>
<span class="normal">5104</span>
<span class="normal">5105</span>
<span class="normal">5106</span>
<span class="normal">5107</span>
<span class="normal">5108</span>
<span class="normal">5109</span>
<span class="normal">5110</span>
<span class="normal">5111</span>
<span class="normal">5112</span>
<span class="normal">5113</span>
<span class="normal">5114</span>
<span class="normal">5115</span>
<span class="normal">5116</span>
<span class="normal">5117</span>
<span class="normal">5118</span>
<span class="normal">5119</span>
<span class="normal">5120</span>
<span class="normal">5121</span>
<span class="normal">5122</span>
<span class="normal">5123</span>
<span class="normal">5124</span>
<span class="normal">5125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.WanLoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        return_lora_metadata (`bool`, *optional*, defaults to False):</span>
<span class="sd">            When enabled, additionally return the LoRA adapter metadata, typically found in the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">return_lora_metadata</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;return_lora_metadata&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span> <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">}</span>

    <span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;diffusion_model.&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_convert_musubi_wan_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span> <span class="k">if</span> <span class="n">return_lora_metadata</span> <span class="k">else</span> <span class="n">state_dict</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transformer_lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the transformer.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, ms.nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>ms.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer_lora_adapter_metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>LoRA adapter metadata associated with the transformer to be serialized with the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="dict">dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5265</span>
<span class="normal">5266</span>
<span class="normal">5267</span>
<span class="normal">5268</span>
<span class="normal">5269</span>
<span class="normal">5270</span>
<span class="normal">5271</span>
<span class="normal">5272</span>
<span class="normal">5273</span>
<span class="normal">5274</span>
<span class="normal">5275</span>
<span class="normal">5276</span>
<span class="normal">5277</span>
<span class="normal">5278</span>
<span class="normal">5279</span>
<span class="normal">5280</span>
<span class="normal">5281</span>
<span class="normal">5282</span>
<span class="normal">5283</span>
<span class="normal">5284</span>
<span class="normal">5285</span>
<span class="normal">5286</span>
<span class="normal">5287</span>
<span class="normal">5288</span>
<span class="normal">5289</span>
<span class="normal">5290</span>
<span class="normal">5291</span>
<span class="normal">5292</span>
<span class="normal">5293</span>
<span class="normal">5294</span>
<span class="normal">5295</span>
<span class="normal">5296</span>
<span class="normal">5297</span>
<span class="normal">5298</span>
<span class="normal">5299</span>
<span class="normal">5300</span>
<span class="normal">5301</span>
<span class="normal">5302</span>
<span class="normal">5303</span>
<span class="normal">5304</span>
<span class="normal">5305</span>
<span class="normal">5306</span>
<span class="normal">5307</span>
<span class="normal">5308</span>
<span class="normal">5309</span>
<span class="normal">5310</span>
<span class="normal">5311</span>
<span class="normal">5312</span>
<span class="normal">5313</span>
<span class="normal">5314</span>
<span class="normal">5315</span>
<span class="normal">5316</span>
<span class="normal">5317</span>
<span class="normal">5318</span>
<span class="normal">5319</span>
<span class="normal">5320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.CogVideoXLoraLoaderMixin.save_lora_weights</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">transformer_lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the transformer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, ms.nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `ms.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        transformer_lora_adapter_metadata:</span>
<span class="sd">            LoRA adapter metadata associated with the transformer to be serialized with the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lora_adapter_metadata</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass `transformer_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">transformer_lora_adapter_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">transformer_lora_adapter_metadata</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="n">lora_adapter_metadata</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SkyReelsV2LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SkyReelsV2LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">5371</span>
<span class="normal">5372</span>
<span class="normal">5373</span>
<span class="normal">5374</span>
<span class="normal">5375</span>
<span class="normal">5376</span>
<span class="normal">5377</span>
<span class="normal">5378</span>
<span class="normal">5379</span>
<span class="normal">5380</span>
<span class="normal">5381</span>
<span class="normal">5382</span>
<span class="normal">5383</span>
<span class="normal">5384</span>
<span class="normal">5385</span>
<span class="normal">5386</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_transformer (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin">StableDiffusionLoraLoaderMixin</a></code></p>









              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AmusedLoraLoaderMixin</span><span class="p">(</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="p">):</span>
    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer with FluxTransformer2DModel-&gt;UVit2DModel</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">transformer</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            transformer (`UVit2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            hotswap (`bool`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">            metadata (`dict`):</span>
<span class="sd">                Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">                from the state dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `transformer_lora_layers` or `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">AmusedLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prefix</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">AmusedLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hotswap</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>state_dict</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>network_alphas</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>transformer</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UVit2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_name</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hotswap</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>metadata</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional LoRA adapter metadata. When supplied, the <code>LoraConfig</code> arguments of <code>peft</code> won't be derived
from the state dict.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.FluxLoraLoaderMixin.load_lora_into_transformer with FluxTransformer2DModel-&gt;UVit2DModel</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hotswap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        transformer (`UVit2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        hotswap (`bool`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`].</span>
<span class="sd">        metadata (`dict`):</span>
<span class="sd">            Optional LoRA adapter metadata. When supplied, the `LoraConfig` arguments of `peft` won&#39;t be derived</span>
<span class="sd">            from the state dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="n">hotswap</span><span class="o">=</span><span class="n">hotswap</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">AmusedLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.AmusedLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>save_directory</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unet_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>text_encoder_lora_layers</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from ðŸ¤— Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>is_main_process</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_function</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_serialization</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from ðŸ¤— Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `transformer_lora_layers` or `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_base.LoraBaseMixin</code>


<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Utility class for handling LoRAs.</p>








              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LoraBaseMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility class for handling LoRAs.&quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">_merged_adapters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the lora scale which can be set at run time by the pipeline. # if `_lora_scale` has not been set,</span>
<span class="sd">        return 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_scale</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_lora_scale&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="mf">1.0</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_fused_loras</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the number of LoRAs that have been fused.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fused_loras</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns names of the LoRAs that have been fused.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`load_lora_weights()` is not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`save_lora_weights()` not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`lora_state_dict()` is not implemented.&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unloads the LoRA parameters.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">        &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_unet` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_unet&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_transformer` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_transformer&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_text_encoder` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_text_encoder&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

        <span class="c1"># Need to retrieve the names as `adapter_names` can be None. So we cannot directly use it</span>
        <span class="c1"># in `self._merged_adapters = self._merged_adapters | merged_adapter_names`.</span>
        <span class="n">merged_adapter_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># check if diffusers model</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                            <span class="n">merged_adapter_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">))</span>
                <span class="c1"># handle transformers models.</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">fuse_text_encoder_lora</span><span class="p">(</span>
                        <span class="n">model</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                            <span class="n">merged_adapter_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">|</span> <span class="n">merged_adapter_names</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_unet` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_unet&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_transformer` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_transformer&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_text_encoder` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_text_encoder&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">)):</span>
                    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                            <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">):</span>
                                <span class="k">if</span> <span class="n">adapter</span> <span class="ow">and</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span><span class="p">:</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">-</span> <span class="p">{</span><span class="n">adapter</span><span class="p">}</span>
                            <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_adapters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">adapter_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the currently active adapters for use in the pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_names (`List[str]` or `str`):</span>
<span class="sd">                The names of the adapters to use.</span>
<span class="sd">            adapter_weights (`Union[List[float], float]`, *optional*):</span>
<span class="sd">                The adapter(s) weights to use with the UNet. If `None`, the weights are set to `1.0` for all the</span>
<span class="sd">                adapters.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(</span>
<span class="sd">            &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.set_adapters([&quot;cinematic&quot;, &quot;pixel&quot;], adapter_weights=[0.5, 0.5])</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">components_passed</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">adapter_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">lora_components</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">)</span>

            <span class="n">invalid_components</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">components_passed</span> <span class="o">-</span> <span class="n">lora_components</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">invalid_components</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The following components in `adapter_weights` are not part of the pipeline: </span><span class="si">{</span><span class="n">invalid_components</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Available components that are LoRA-compatible: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">}</span><span class="s2">. So, weights belonging &quot;</span>
                    <span class="s2">&quot;to the invalid components will be removed and ignored.&quot;</span>
                <span class="p">)</span>
                <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">adapter_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invalid_components</span><span class="p">}</span>

        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>
        <span class="n">adapter_weights</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span>

        <span class="c1"># Expand weights into a list, one entry per adapter</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">list_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span>  <span class="c1"># eg {&quot;unet&quot;: [&quot;adapter1&quot;, &quot;adapter2&quot;], &quot;text_encoder&quot;: [&quot;adapter2&quot;]}</span>
        <span class="c1"># eg [&quot;adapter1&quot;, &quot;adapter2&quot;]</span>
        <span class="n">all_adapters</span> <span class="o">=</span> <span class="p">{</span><span class="n">adapter</span> <span class="k">for</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">}</span>
        <span class="n">missing_adapters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">-</span> <span class="n">all_adapters</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_adapters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter name(s) </span><span class="si">{</span><span class="n">missing_adapters</span><span class="si">}</span><span class="s2"> not in the list of present adapters: </span><span class="si">{</span><span class="n">all_adapters</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="c1"># eg {&quot;adapter1&quot;: [&quot;unet&quot;], &quot;adapter2&quot;: [&quot;unet&quot;, &quot;text_encoder&quot;]}</span>
        <span class="n">invert_list_adapters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">adapter</span><span class="p">:</span> <span class="p">[</span><span class="n">part</span> <span class="k">for</span> <span class="n">part</span><span class="p">,</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">all_adapters</span>
        <span class="p">}</span>

        <span class="c1"># Decompose weights into weights for denoiser and text encoders.</span>
        <span class="n">_component_adapter_weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">adapter_weights</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">component_adapter_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Lora weight dict for adapter &#39;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2">&#39; contains </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">,&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;but this will be ignored because </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not contain weights for </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">.&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;Valid parts for </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="p">)</span>
                        <span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span>

                <span class="n">_component_adapter_weights</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component_adapter_weights</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">disable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Disables the active LoRA layers of the pipeline.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(</span>
<span class="sd">            &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.disable_lora()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enables the active LoRA layers of the pipeline.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(</span>
<span class="sd">            &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.enable_lora()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete an adapter&#39;s LoRA layers from the pipeline.</span>

<span class="sd">        Args:</span>
<span class="sd">            adapter_names (`Union[List[str], str]`):</span>
<span class="sd">                The names of the adapters to delete.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">        import mindspore as ms</span>

<span class="sd">        pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(</span>
<span class="sd">            &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_names=&quot;cinematic&quot;</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.delete_adapters(&quot;cinematic&quot;)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
                        <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the list of the current active adapters.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">        pipeline.get_active_adapters()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
                        <span class="k">break</span>

        <span class="k">return</span> <span class="n">active_adapters</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">))</span>
                <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">set_adapters</span><span class="p">[</span><span class="n">component</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">set_adapters</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_lora_hotswap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hotswap adapters without triggering recompilation of a model or if the ranks of the loaded adapters are</span>
<span class="sd">        different.</span>

<span class="sd">        Args:</span>
<span class="sd">            target_rank (`int`):</span>
<span class="sd">                The highest rank among all the adapters that will be loaded.</span>
<span class="sd">            check_compiled (`str`, *optional*, defaults to `&quot;error&quot;`):</span>
<span class="sd">                How to handle a model that is already compiled. The check can return the following messages:</span>
<span class="sd">                  - &quot;error&quot; (default): raise an error</span>
<span class="sd">                  - &quot;warn&quot;: issue a warning</span>
<span class="sd">                  - &quot;ignore&quot;: do nothing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="s2">&quot;enable_lora_hotswap&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">):</span>
                <span class="n">component</span><span class="o">.</span><span class="n">enable_lora_hotswap</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">layers_weights</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
        <span class="k">return</span> <span class="n">_pack_dict_with_prefix</span><span class="p">(</span><span class="n">layers_weights</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Writes the state dict of the LoRA layers (optionally with metadata) to disk.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">lora_adapter_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">safe_serialization</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`lora_adapter_metadata` cannot be specified when not using `safe_serialization`.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lora_adapter_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lora_adapter_metadata</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;`lora_adapter_metadata` must be of type `dict`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">save_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">save_function</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
                    <span class="c1"># Inject framework format.</span>
                    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">}</span>
                    <span class="k">if</span> <span class="n">lora_adapter_metadata</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
                                <span class="n">lora_adapter_metadata</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                        <span class="n">metadata</span><span class="p">[</span><span class="n">LORA_ADAPTER_METADATA_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
                            <span class="n">lora_adapter_metadata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span>
                        <span class="p">)</span>

                    <span class="k">return</span> <span class="n">save_file</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">save_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
        <span class="n">save_function</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_state_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">deprecation_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Using the `_fetch_state_dict()` method from </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> has been deprecated and will be removed in a future version. Please use `from diffusers.loaders.lora_base import _fetch_state_dict`.&quot;</span>  <span class="c1"># noqa</span>
        <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;_fetch_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;0.35.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_fetch_state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_best_guess_weight_name</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">deprecation_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Using the `_best_guess_weight_name()` method from </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> has been deprecated and will be removed in a future version. Please use `from diffusers.loaders.lora_base import _best_guess_weight_name`.&quot;</span>  <span class="c1"># noqa</span>
        <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;_best_guess_weight_name&quot;</span><span class="p">,</span> <span class="s2">&quot;0.35.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_best_guess_weight_name</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.fused_loras" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">fused_loras</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fused_loras" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns names of the LoRAs that have been fused.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.lora_scale" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">lora_scale</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.lora_scale" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the lora scale which can be set at run time by the pipeline. # if <code>_lora_scale</code> has not been set,
return 1.</p>

    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.num_fused_loras" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">num_fused_loras</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.num_fused_loras" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Returns the number of LoRAs that have been fused.</p>

    </div>

</div>




<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Delete an adapter's LoRA layers from the pipeline.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The names of the adapters to delete.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[List[str], str]`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoPipelineForText2Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">AutoPipelineForText2Image</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span>
    <span class="s2">&quot;jbilcke-hf/sdxl-cinematic-1&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pytorch_lora_weights.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="s2">&quot;cinematic&quot;</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="s2">&quot;cinematic&quot;</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Delete an adapter&#39;s LoRA layers from the pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        adapter_names (`Union[List[str], str]`):</span>
<span class="sd">            The names of the adapters to delete.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(</span>
<span class="sd">        &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_names=&quot;cinematic&quot;</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.delete_adapters(&quot;cinematic&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
                    <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.disable_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.disable_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Disables the active LoRA layers of the pipeline.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoPipelineForText2Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">AutoPipelineForText2Image</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span>
    <span class="s2">&quot;jbilcke-hf/sdxl-cinematic-1&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pytorch_lora_weights.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;cinematic&quot;</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">disable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Disables the active LoRA layers of the pipeline.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(</span>
<span class="sd">        &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.disable_lora()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Enables the active LoRA layers of the pipeline.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoPipelineForText2Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">AutoPipelineForText2Image</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span>
    <span class="s2">&quot;jbilcke-hf/sdxl-cinematic-1&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pytorch_lora_weights.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;cinematic&quot;</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">enable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enables the active LoRA layers of the pipeline.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(</span>
<span class="sd">        &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.enable_lora()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">enable_lora_hotswap</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.enable_lora_hotswap" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Hotswap adapters without triggering recompilation of a model or if the ranks of the loaded adapters are
different.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>target_rank</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The highest rank among all the adapters that will be loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>check_compiled</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How to handle a model that is already compiled. The check can return the following messages:
  - "error" (default): raise an error
  - "warn": issue a warning
  - "ignore": do nothing</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;error&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">enable_lora_hotswap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hotswap adapters without triggering recompilation of a model or if the ranks of the loaded adapters are</span>
<span class="sd">    different.</span>

<span class="sd">    Args:</span>
<span class="sd">        target_rank (`int`):</span>
<span class="sd">            The highest rank among all the adapters that will be loaded.</span>
<span class="sd">        check_compiled (`str`, *optional*, defaults to `&quot;error&quot;`):</span>
<span class="sd">            How to handle a model that is already compiled. The check can return the following messages:</span>
<span class="sd">              - &quot;error&quot; (default): raise an error</span>
<span class="sd">              - &quot;warn&quot;: issue a warning</span>
<span class="sd">              - &quot;ignore&quot;: do nothing</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="s2">&quot;enable_lora_hotswap&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">):</span>
            <span class="n">component</span><span class="o">.</span><span class="n">enable_lora_hotswap</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>lora_scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>safe_fusing</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_unet` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_unet&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_transformer` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_transformer&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_text_encoder` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_text_encoder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

    <span class="c1"># Need to retrieve the names as `adapter_names` can be None. So we cannot directly use it</span>
    <span class="c1"># in `self._merged_adapters = self._merged_adapters | merged_adapter_names`.</span>
    <span class="n">merged_adapter_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># check if diffusers model</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="n">merged_adapter_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">))</span>
            <span class="c1"># handle transformers models.</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">fuse_text_encoder_lora</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="n">merged_adapter_names</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">|</span> <span class="n">merged_adapter_names</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the list of the current active adapters.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;CiroN2022/toy-face&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;toy_face_sdxl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;toy&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the list of the current active adapters.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">    pipeline.get_active_adapters()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                    <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
                    <span class="k">break</span>

    <span class="k">return</span> <span class="n">active_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the current list of all available adapters in the pipeline.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">))</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">set_adapters</span><span class="p">[</span><span class="n">component</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">set_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.set_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">adapter_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.set_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Set the currently active adapters for use in the pipeline.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>adapter_names</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The names of the adapters to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]` or `str`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>adapter_weights</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The adapter(s) weights to use with the UNet. If <code>None</code>, the weights are set to <code>1.0</code> for all the
adapters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[List[float], float]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoPipelineForText2Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ms</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">AutoPipelineForText2Image</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span>
    <span class="s2">&quot;jbilcke-hf/sdxl-cinematic-1&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pytorch_lora_weights.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;cinematic&quot;</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">([</span><span class="s2">&quot;cinematic&quot;</span><span class="p">,</span> <span class="s2">&quot;pixel&quot;</span><span class="p">],</span> <span class="n">adapter_weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_adapters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">adapter_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the currently active adapters for use in the pipeline.</span>

<span class="sd">    Args:</span>
<span class="sd">        adapter_names (`List[str]` or `str`):</span>
<span class="sd">            The names of the adapters to use.</span>
<span class="sd">        adapter_weights (`Union[List[float], float]`, *optional*):</span>
<span class="sd">            The adapter(s) weights to use with the UNet. If `None`, the weights are set to `1.0` for all the</span>
<span class="sd">            adapters.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import AutoPipelineForText2Image</span>
<span class="sd">    import mindspore as ms</span>

<span class="sd">    pipeline = AutoPipelineForText2Image.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=ms.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(</span>
<span class="sd">        &quot;jbilcke-hf/sdxl-cinematic-1&quot;, weight_name=&quot;pytorch_lora_weights.safetensors&quot;, adapter_name=&quot;cinematic&quot;</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.set_adapters([&quot;cinematic&quot;, &quot;pixel&quot;], adapter_weights=[0.5, 0.5])</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">components_passed</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">adapter_weights</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">lora_components</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">)</span>

        <span class="n">invalid_components</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">components_passed</span> <span class="o">-</span> <span class="n">lora_components</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">invalid_components</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following components in `adapter_weights` are not part of the pipeline: </span><span class="si">{</span><span class="n">invalid_components</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Available components that are LoRA-compatible: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">}</span><span class="s2">. So, weights belonging &quot;</span>
                <span class="s2">&quot;to the invalid components will be removed and ignored.&quot;</span>
            <span class="p">)</span>
            <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">adapter_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invalid_components</span><span class="p">}</span>

    <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>
    <span class="n">adapter_weights</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span>

    <span class="c1"># Expand weights into a list, one entry per adapter</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">list_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span>  <span class="c1"># eg {&quot;unet&quot;: [&quot;adapter1&quot;, &quot;adapter2&quot;], &quot;text_encoder&quot;: [&quot;adapter2&quot;]}</span>
    <span class="c1"># eg [&quot;adapter1&quot;, &quot;adapter2&quot;]</span>
    <span class="n">all_adapters</span> <span class="o">=</span> <span class="p">{</span><span class="n">adapter</span> <span class="k">for</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">}</span>
    <span class="n">missing_adapters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">-</span> <span class="n">all_adapters</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_adapters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter name(s) </span><span class="si">{</span><span class="n">missing_adapters</span><span class="si">}</span><span class="s2"> not in the list of present adapters: </span><span class="si">{</span><span class="n">all_adapters</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="c1"># eg {&quot;adapter1&quot;: [&quot;unet&quot;], &quot;adapter2&quot;: [&quot;unet&quot;, &quot;text_encoder&quot;]}</span>
    <span class="n">invert_list_adapters</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">adapter</span><span class="p">:</span> <span class="p">[</span><span class="n">part</span> <span class="k">for</span> <span class="n">part</span><span class="p">,</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">all_adapters</span>
    <span class="p">}</span>

    <span class="c1"># Decompose weights into weights for denoiser and text encoders.</span>
    <span class="n">_component_adapter_weights</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">adapter_weights</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">component_adapter_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Lora weight dict for adapter &#39;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2">&#39; contains </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">,&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but this will be ignored because </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not contain weights for </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;Valid parts for </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span>

            <span class="n">_component_adapter_weights</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="p">[])</span>
            <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component_adapter_weights</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>
        <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
            <span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>components</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_unet</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unfuse_text_encoder</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_unet` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_unet&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_transformer` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_transformer&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_text_encoder` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_text_encoder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">merged_adapters</span><span class="p">):</span>
                            <span class="k">if</span> <span class="n">adapter</span> <span class="ow">and</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span><span class="p">:</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_adapters</span> <span class="o">-</span> <span class="p">{</span><span class="n">adapter</span><span class="p">}</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Unloads the LoRA parameters.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="c1"># Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="o">...</span>
</code></pre></div>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unloads the LoRA parameters.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">    &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">    &gt;&gt;&gt; ...</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.write_lora_layers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">,</span> <span class="n">is_main_process</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">,</span> <span class="n">save_function</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="p">,</span> <span class="n">lora_adapter_metadata</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.write_lora_layers" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Writes the state dict of the LoRA layers (optionally with metadata) to disk.</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">write_lora_layers</span><span class="p">(</span>
    <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">lora_adapter_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Writes the state dict of the LoRA layers (optionally with metadata) to disk.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="n">lora_adapter_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">safe_serialization</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`lora_adapter_metadata` cannot be specified when not using `safe_serialization`.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">lora_adapter_metadata</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lora_adapter_metadata</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;`lora_adapter_metadata` must be of type `dict`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">save_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">save_function</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
                <span class="c1"># Inject framework format.</span>
                <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">}</span>
                <span class="k">if</span> <span class="n">lora_adapter_metadata</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">lora_adapter_metadata</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
                            <span class="n">lora_adapter_metadata</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="n">metadata</span><span class="p">[</span><span class="n">LORA_ADAPTER_METADATA_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
                        <span class="n">lora_adapter_metadata</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">)</span>

                <span class="k">return</span> <span class="n">save_file</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">save_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span>

    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
            <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME</span>

    <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
    <span class="n">save_function</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 29, 2025 10:21:55 UTC">August 29, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 1, 2024 02:56:38 UTC">November 1, 2024</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:77485245+wcrzlh@users.noreply.github.com">Chaoran Wei</a>, 
        <a href="mailto:73014084+cui-yshoho@users.noreply.github.com">Cui-yshoho</a>, 
        <a href="mailto:53842165+the-truthh@users.noreply.github.com">The-truthh</a>, 
        <a href="mailto:townwish2023@outlook.com">townwish4git</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../ip_adapter/" class="md-footer__link md-footer__link--prev" aria-label="Previous: IP-Adapter">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                IP-Adapter
              </div>
            </div>
          </a>
        
        
          
          <a href="../single_file/" class="md-footer__link md-footer__link--next" aria-label="Next: Single files">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Single files
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 MindSpore Lab
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindone" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M170.5 148.1v217.5h23.4l7.7 26.4 42-26.4h49.5V148.1H170.4zM268.3 342h-27.9l-27.9 17.5-5.1-17.5h-11.9V171.7h72.8zm-118.5-94.3H97.5c1.7-27.1 2.2-51.6 2.2-73.5h51.2s2-22.6-8.6-22.3H53.8c3.5-13.1 7.9-26.7 13.1-40.7 0 0-24.1 0-32.3 21.6-3.4 8.9-13.2 43.1-30.7 78.1 5.9-.6 25.4-1.2 36.8-22.2 2.1-5.9 2.5-6.7 5.1-14.5h28.9c0 10.5-1.2 66.9-1.7 73.4H20.7c-11.7 0-15.6 23.6-15.6 23.6h65.6c-4.4 49.9-28 91.9-70.8 125.1 20.5 5.9 40.9-.9 51-9.9 0 0 23-20.9 35.6-69.3l54 64.9s7.9-26.9-1.2-40c-7.6-8.9-28.1-33.1-36.8-41.8L87.9 312c4.4-14 7-27.6 7.9-40.7h61.6s-.1-23.6-7.6-23.6m412-1.6c20.8-25.6 45-58.6 45-58.6s-18.6-14.8-27.4-4.1c-6 8.2-36.8 48.2-36.8 48.2l19.2 14.4zm-150-59.1c-9-8.2-25.9 2.1-25.9 2.1s39.5 55 41.1 57.4l19.5-13.7s-25.7-37.6-34.7-45.9zM640 258.4c-19.8 0-130.9.9-131.1.9v-101q7.2 0 22.8-1.2c40.9-2.4 70.1-4 87.8-4.8 0 0 12.2-27.2-.6-33.4-3.1-1.2-23.2 4.6-23.2 4.6s-165.2 16.5-232.4 18c1.6 8.8 7.6 17.1 15.8 19.6 13.3 3.5 22.7 1.7 49.2.9 24.8-1.6 43.7-2.4 56.5-2.4v99.8H351.3s2.8 22.3 25.5 22.9h107.9v70.9c0 14-11.2 22-24.5 21.1-14.1.1-26.1-1.1-41.7-1.8 2 4 6.3 14.4 19.3 21.8 9.9 4.8 16.2 6.6 26 6.6 29.6 0 45.7-17.3 44.9-45.3v-73.3h122.4c9.7 0 8.7-23.8 8.7-23.8z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>