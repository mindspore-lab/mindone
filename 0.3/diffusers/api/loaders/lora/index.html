
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindone/0.3/diffusers/api/loaders/lora/">
      
      
        <link rel="prev" href="../ip_adapter/">
      
      
        <link rel="next" href="../single_file/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>LoRA - MindOne - One for All</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Source Sans Pro";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lora" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="MindOne - One for All" class="md-header__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOne - One for All
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LoRA
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../" class="md-tabs__link">
          
  
  
  Diffusers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../transformers/" class="md-tabs__link">
          
  
  
  Transformers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../peft/" class="md-tabs__link">
          
  
  
  PEFT

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="MindOne - One for All" class="md-nav__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOne - One for All
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Diffusers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    🧨 Diffusers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../quicktour/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quicktour
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Effective and efficient diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../limitations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Limitations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/tutorial_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/write_own_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding pipelines, models and schedulers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/autopipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/basic_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Train a diffusion model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorials/using_peft_for_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load LoRAs for inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Load pipelines and adapters
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Load pipelines and adapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/schedulers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load schedulers and models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/other-formats/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model files and layouts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/loading_adapters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load adapters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/push_to_hub/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Push files to the Hub
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Generative tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Generative tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/unconditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/conditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/text-img2vid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text or image-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Inference techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Inference techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/overview_techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/merge_loras/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Merge LoRAs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/scheduler_features/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler features
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/callback/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pipeline callbacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/reusing_seeds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reproducible pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Specific pipeline examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Specific pipeline examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/t2i_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/textual_inversion_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/shap-e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/inference_with_tcd_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trajectory Consistency Distillation-LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Video Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/marigold_usage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/create_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create a dataset for training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/adapt_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adapt a model to a new task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_4" >
        
          
          <label class="md-nav__link" for="__nav_2_7_4" id="__nav_2_7_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7_4">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/unconditional_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text2image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7_5" >
        
          
          <label class="md-nav__link" for="__nav_2_7_5" id="__nav_2_7_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_7_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7_5">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/text_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/dreambooth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DreamBooth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Accelerate inference and reduce memory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            Accelerate inference and reduce memory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/fp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speed up inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reduce memory usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../optimization/xformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    xFormers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
        
          
          <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Conceptual Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_9">
            <span class="md-nav__icon md-icon"></span>
            Conceptual Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../conceptual/philosophy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Philosophy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../using-diffusers/controlling_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Controlled generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" checked>
        
          
          <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_10">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_1" >
        
          
          <label class="md-nav__link" for="__nav_2_10_1" id="__nav_2_10_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Main Classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_1">
            <span class="md-nav__icon md-icon"></span>
            Main Classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../outputs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outputs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2_10_2" id="__nav_2_10_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Loaders
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_10_2">
            <span class="md-nav__icon md-icon"></span>
            Loaders
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SD3LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraBaseMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraBaseMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../single_file/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single files
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../textual_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_3" >
        
          
          <label class="md-nav__link" for="__nav_2_10_3" id="__nav_2_10_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_3_2" >
        
          
          <label class="md-nav__link" for="__nav_2_10_3_2" id="__nav_2_10_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ControlNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_3_2">
            <span class="md-nav__icon md-icon"></span>
            ControlNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_sparsectrl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SparseControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnionModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_3_3" >
        
          
          <label class="md-nav__link" for="__nav_2_10_3_3" id="__nav_2_10_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_3_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/allegro_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AllegroTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/aura_flow_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlowTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogvideox_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview3plus_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3PlusTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/cogview4_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/dit_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiTTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/easyanimate_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimateTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/flux_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/hunyuan_video_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/latte_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LatteTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina_nextdit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LuminaNextDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/lumina2_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina2Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/ltx_video_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/mochi_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MochiTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/pixart_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArtTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/prior_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PriorTransformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sd3_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/sana_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SanaTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_audio_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableAudioDiTModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/transformer_temporal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TransformerTemporalModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_3_4" >
        
          
          <label class="md-nav__link" for="__nav_2_10_3_4" id="__nav_2_10_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    UNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_3_4">
            <span class="md-nav__icon md-icon"></span>
            UNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/stable_cascade_unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableCascadeUNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet1DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet2d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet3d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet3DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/unet-motion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNetMotionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/uvit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UViT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_3_5" >
        
          
          <label class="md-nav__link" for="__nav_2_10_3_5" id="__nav_2_10_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VAEs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_3_5">
            <span class="md-nav__icon md-icon"></span>
            VAEs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLAllegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLCogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_kl_hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLHunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLLTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_magvit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMagvit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoderkl_mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/asymmetricautoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AsymmetricAutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/consistency_decoder_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderVAE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_oobleck/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Oobleck AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/autoencoder_tiny/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tiny AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/vq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_4" >
        
          
          <label class="md-nav__link" for="__nav_2_10_4" id="__nav_2_10_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pipelines
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_4">
            <span class="md-nav__icon md-icon"></span>
            Pipelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Allegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/animatediff/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AnimateDiff
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/audioldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/aura_flow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/auto_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/blip_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BLIP-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogview3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/cogview4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Flux.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/easyanimate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimate
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/control_flux_inpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlInpaint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnetxs_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dance_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dance Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/deepfloyd_if/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepFloyd IF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/dit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/i2vgenxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I2VGen-XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pix2pix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructPix2Pix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky_v22/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kandinsky3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/kolors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kolors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latent_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/latte/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latte
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/lumina2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina 2.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/lumina/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina-T2X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/marigold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/musicldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MusicLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/paint_by_example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paint by Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Personalized Image Animator (PIA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-α
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/pixart_sigma/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Σ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/sana_sprint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana Sprint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/self_attention_guidance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/semantic_stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/shap_e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_cascade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Cascade
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_4_58" >
        
          
          <label class="md-nav__link" for="__nav_2_10_4_58" id="__nav_2_10_4_58_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_10_4_58_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_4_58">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/text2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/image_variation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image variation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/stable_diffusion_xl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/latent_upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Super-resolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/ldm3d_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_diffusion/gligen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GLIGEN (Grounded Language-to-Image Generation)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/stable_unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/text_to_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/text_to_video_zero/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text2Video-Zero
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pipelines/wuerstchen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wuerstchen
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_5" >
        
          
          <label class="md-nav__link" for="__nav_2_10_5" id="__nav_2_10_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Schedulers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_5">
            <span class="md-nav__icon md-icon"></span>
            Schedulers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/cm_stochastic_iterative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMStochasticIterativeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/consistency_decoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMInverseScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/deis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DEISMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepInverse
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/singlestep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSinglestepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/edm_euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMEulerScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerAncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_euler_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchEulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/flow_match_heun_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchHeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/heun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/ipndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IPNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2AncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/dpm_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2DiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LCMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/lms_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LMSDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/pndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/repaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RePaintScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_ve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/score_sde_vp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVpScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/tcd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TCDScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/unipc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniPCMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../schedulers/vq_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQDiffusionScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10_6" >
        
          
          <label class="md-nav__link" for="__nav_2_10_6" id="__nav_2_10_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Internal classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_10_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10_6">
            <span class="md-nav__icon md-icon"></span>
            Internal classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../internal_classes_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attnprocessor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../activations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom activation functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom normalization layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../utilities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VAE Image Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../video_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video Processor
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    🤗 Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune a pretrained model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/finetune_distribute/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed training and mixed precision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../transformers/tutorials/generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generation with LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    🤗 PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      StableDiffusionXLLoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StableDiffusionXLLoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_unet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="md-nav__link">
    <span class="md-ellipsis">
      SD3LoraLoaderMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3LoraLoaderMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_text_encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_into_transformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      load_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="md-nav__link">
    <span class="md-ellipsis">
      lora_state_dict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      save_lora_weights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="md-nav__link">
    <span class="md-ellipsis">
      LoraBaseMixin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoraBaseMixin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      delete_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      fuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_active_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="md-nav__link">
    <span class="md-ellipsis">
      get_list_adapters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="md-nav__link">
    <span class="md-ellipsis">
      unfuse_lora
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="md-nav__link">
    <span class="md-ellipsis">
      unload_lora_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/mindspore-lab/mindone/edit/master/docs/diffusers/api/loaders/lora.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindone/raw/master/docs/diffusers/api/loaders/lora.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="lora">LoRA<a class="headerlink" href="#lora" title="Permanent link">&para;</a></h1>
<p>LoRA is a fast and lightweight training method that inserts and trains a significantly smaller number of parameters instead of all the model parameters. This produces a smaller file (~100 MBs) and makes it easier to quickly train a model to learn a new concept. LoRA weights are typically loaded into the denoiser, text encoder or both. The denoiser usually corresponds to a UNet (<code>UNet2DConditionModel</code>, for example) or a Transformer (<code>SD3Transformer2DModel</code>, for example). There are several classes for loading LoRA weights:</p>
<ul>
<li><code>StableDiffusionLoraLoaderMixin</code> provides functions for loading and unloading, fusing and unfusing, enabling and disabling, and more functions for managing LoRA weights. This class can be used with any model.</li>
<li><code>StableDiffusionXLLoraLoaderMixin</code> is a <a href="../../pipelines/stable_diffusion/stable_diffusion_xl/">Stable Diffusion (SDXL)</a> version of the <code>StableDiffusionLoraLoaderMixin</code> class for loading and saving LoRA weights. It can only be used with the SDXL model.</li>
<li><code>SD3LoraLoaderMixin</code> provides similar functions for <a href="../../pipelines/stable_diffusion/stable_diffusion_3/">Stable Diffusion 3</a></li>
<li><code>LoraBaseMixin</code> provides a base class with several utility methods to fuse, unfuse, unload, LoRAs and more.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To learn more about how to load LoRA weights, see the <a href="../../../using-diffusers/loading_adapters/#lora">LoRA</a> loading guide.</p>
</div>


<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into Stable Diffusion [<code>UNet2DConditionModel</code>] and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>.</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StableDiffusionLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into Stable Diffusion [`UNet2DConditionModel`] and</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">]</span>
    <span class="n">unet_name</span> <span class="o">=</span> <span class="n">UNET_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">        loaded into `self.unet`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">                Name of the serialized state dict file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># UNet and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
            <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Map SDXL blocks correctly.</span>
            <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># use unet config to remap block numbers</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
            <span class="c1"># Load the layers corresponding to UNet.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `unet_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_fusing</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prefix</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>Speed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>up model loading by only loading the pretrained LoRA weights and not initializing the random weights.</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>unet</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
        <span class="c1"># Load the layers corresponding to UNet.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is
loaded into <code>self.unet</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state
dict is loaded into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kwargs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">    loaded into `self.unet`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">unet</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet_name</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>cache_dir</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>force_download</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>proxies</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>local_files_only</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>token</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>revision</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>subfolder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>weight_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the serialized state dict file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">            Name of the serialized state dict file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># UNet and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
        <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="c1"># Map SDXL blocks correctly.</span>
        <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use unet config to remap block numbers</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>save_directory</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from 🤗 Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>is_main_process</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>save_function</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_serialization</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You must pass at least one of `unet_lora_layers` and `text_encoder_lora_layers`.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">))</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into Stable Diffusion XL [<code>UNet2DConditionModel</code>],
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>, and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection"><code>CLIPTextModelWithProjection</code></a>.</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StableDiffusionXLLoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into Stable Diffusion XL [`UNet2DConditionModel`],</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), and</span>
<span class="sd">    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span>
    <span class="n">unet_name</span> <span class="o">=</span> <span class="n">UNET_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">        loaded into `self.unet`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.text_encoder`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
        <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
        <span class="c1"># pipeline.</span>

        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span>
        <span class="p">)</span>
        <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">            weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">                Name of the serialized state dict file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># UNet and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
            <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="c1"># Map SDXL blocks correctly.</span>
            <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># use unet config to remap block numbers</span>
                <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            unet (`UNet2DConditionModel`):</span>
<span class="sd">                The UNet model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading only loading the pretrained LoRA weights and not initializing the random weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
        <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
        <span class="c1"># their prefixes.</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
            <span class="c1"># Load the layers corresponding to UNet.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">            text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers` or `text_encoder_2_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_fusing</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prefix</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>Speed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>up model loading by only loading the pretrained LoRA weights and not initializing the random weights.</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_into_unet" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>unet</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The UNet model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`UNet2DConditionModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_unet</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_unet</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `unet`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        unet (`UNet2DConditionModel`):</span>
<span class="sd">            The UNet model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading only loading the pretrained LoRA weights and not initializing the random weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If the serialization format is new (introduced in https://github.com/huggingface/diffusers/pull/2918),</span>
    <span class="c1"># then the `state_dict` keys should have `cls.unet_name` and/or `cls.text_encoder_name` as</span>
    <span class="c1"># their prefixes.</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">only_text_encoder</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">only_text_encoder</span><span class="p">:</span>
        <span class="c1"># Load the layers corresponding to UNet.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">unet</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">unet_name</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet</code>] for more details on how the state dict is
loaded into <code>self.unet</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</code>] for more details on how the state
dict is loaded into <code>self.text_encoder</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>Speed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>up model loading by only loading the pretrained LoRA weights and not initializing the random weights.</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kwargs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">adapter_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is</span>
<span class="sd">    loaded into `self.unet`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.text_encoder`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We could have accessed the unet config from `lora_state_dict()` too. We pass</span>
    <span class="c1"># it here explicitly to be able to tell that it&#39;s coming from an SDXL</span>
    <span class="c1"># pipeline.</span>

    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">unet_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_unet</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span> <span class="n">unet</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">unet</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span>
    <span class="p">)</span>
    <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>cache_dir</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>force_download</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>proxies</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>local_files_only</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>token</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>revision</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>subfolder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>weight_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Name of the serialized state dict file.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.lora_state_dict</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>
<span class="sd">        weight_name (`str`, *optional*, defaults to None):</span>
<span class="sd">            Name of the serialized state dict file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># UNet and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">unet_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;unet_config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
        <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="n">network_alphas</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># TODO: replace it with a method from `state_dict_utils`</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_unet_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te1_&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;lora_te2_&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">):</span>
        <span class="c1"># Map SDXL blocks correctly.</span>
        <span class="k">if</span> <span class="n">unet_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># use unet config to remap block numbers</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_maybe_map_sgm_blocks_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">unet_config</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span> <span class="o">=</span> <span class="n">_convert_non_diffusers_lora_to_diffusers</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">unet_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>save_directory</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unet_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>unet</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from 🤗 Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_2_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder_2</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from 🤗 Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>is_main_process</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>save_function</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_serialization</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">unet_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        unet_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `unet`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">        text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">unet_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `unet_lora_layers`, `text_encoder_lora_layers` or `text_encoder_2_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">unet_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">unet_lora_layers</span><span class="p">,</span> <span class="s2">&quot;unet&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">StableDiffusionXLLoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unet&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.StableDiffusionXLLoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;unet&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;unet&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin</code>


<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.loaders.lora_base.LoraBaseMixin" href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin">LoraBaseMixin</a></code></p>


        <p>Load LoRA layers into [<code>SD3Transformer2DModel</code>],
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel"><code>CLIPTextModel</code></a>, and
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection"><code>CLIPTextModelWithProjection</code></a>.</p>
<p>Specific to [<code>StableDiffusion3Pipeline</code>].</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SD3LoraLoaderMixin</span><span class="p">(</span><span class="n">LoraBaseMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA layers into [`SD3Transformer2DModel`],</span>
<span class="sd">    [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), and</span>
<span class="sd">    [`CLIPTextModelWithProjection`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection).</span>

<span class="sd">    Specific to [`StableDiffusion3Pipeline`].</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">]</span>
    <span class="n">transformer_name</span> <span class="o">=</span> <span class="n">TRANSFORMER_NAME</span>
    <span class="n">text_encoder_name</span> <span class="o">=</span> <span class="n">TEXT_ENCODER_NAME</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@validate_hf_hub_args</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return state dict for lora weights and the network alphas.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">        This function is experimental and might change in the future.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                      the Hub.</span>
<span class="sd">                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                      with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                    - A MindSpore state dict.</span>

<span class="sd">            cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">                is not used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">                cached versions if they exist.</span>

<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">                won&#39;t be downloaded from the Hub.</span>
<span class="sd">            token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">                `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">                allowed by Git.</span>
<span class="sd">            subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">                The subfolder location of a model file within a larger model repository on the Hub or locally.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
        <span class="c1"># transformer and text encoder or both.</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
            <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
            <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
            <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
            <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
            <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
            <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
            <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
            <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">        `self.text_encoder`.</span>

<span class="sd">        All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">        loaded.</span>

<span class="sd">        See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">        dict is loaded into `self.transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            kwargs (`dict`, *optional*):</span>
<span class="sd">                See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

        <span class="n">transformer_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;transformer.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
                <span class="n">state_dict</span><span class="p">,</span>
                <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
                <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
                <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
                <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
                <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
                <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
                <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">                into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">                encoder lora layers.</span>
<span class="sd">            transformer (`SD3Transformer2DModel`):</span>
<span class="sd">                The Transformer model to load the LoRA layers into.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load the layers corresponding to transformer.</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">        Parameters:</span>
<span class="sd">            state_dict (`dict`):</span>
<span class="sd">                A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">                additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">            network_alphas (`Dict[str, float]`):</span>
<span class="sd">                The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">            text_encoder (`CLIPTextModel`):</span>
<span class="sd">                The text encoder model to load the LoRA layers into.</span>
<span class="sd">            prefix (`str`):</span>
<span class="sd">                Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">            lora_scale (`float`):</span>
<span class="sd">                How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">                lora layer.</span>
<span class="sd">            adapter_name (`str`, *optional*):</span>
<span class="sd">                Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">                `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">            Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
            <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`):</span>
<span class="sd">                Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">            transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">            text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">            text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">                State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">                encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">            is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">                need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">                process to avoid race conditions.</span>
<span class="sd">            save_function (`Callable`):</span>
<span class="sd">                The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">                replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">                `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">            safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You must pass at least one of `transformer_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

        <span class="c1"># Save the model</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
            <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
            <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
            <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_fusing</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span>
        <span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">network_alphas</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_text_encoder" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>text_encoder</code></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The key should be prefixed with an
additional <code>text_encoder</code> to distinguish between unet lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>network_alphas</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The value of the network alpha used for stable learning and preventing underflow. This value has the
same meaning as the <code>--network_alpha</code> option in the kohya-ss trainer script. Refer to <a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">this
link</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, float]`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The text encoder model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`CLIPTextModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prefix</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Expected prefix of the <code>text_encoder</code> in the <code>state_dict</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>How much to scale the output of the lora linear layer before it is added with the output of the regular
lora layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>Speed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>up model loading by only loading the pretrained LoRA weights and not initializing the random weights.</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="c1"># Copied from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_text_encoder</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">state_dict</span><span class="p">,</span>
    <span class="n">network_alphas</span><span class="p">,</span>
    <span class="n">text_encoder</span><span class="p">,</span>
    <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `text_encoder`</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The key should be prefixed with an</span>
<span class="sd">            additional `text_encoder` to distinguish between unet lora layers.</span>
<span class="sd">        network_alphas (`Dict[str, float]`):</span>
<span class="sd">            The value of the network alpha used for stable learning and preventing underflow. This value has the</span>
<span class="sd">            same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this</span>
<span class="sd">            link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).</span>
<span class="sd">        text_encoder (`CLIPTextModel`):</span>
<span class="sd">            The text encoder model to load the LoRA layers into.</span>
<span class="sd">        prefix (`str`):</span>
<span class="sd">            Expected prefix of the `text_encoder` in the `state_dict`.</span>
<span class="sd">        lora_scale (`float`):</span>
<span class="sd">            How much to scale the output of the lora linear layer before it is added with the output of the regular</span>
<span class="sd">            lora layer.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_load_lora_into_text_encoder</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="n">network_alphas</span><span class="p">,</span>
        <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span>
        <span class="n">text_encoder</span><span class="o">=</span><span class="n">text_encoder</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">text_encoder_name</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">text_encoder_name</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_into_transformer" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This will load the LoRA layers specified in <code>state_dict</code> into <code>transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>state_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A standard state dict containing the lora layer parameters. The keys can either be indexed directly
into the unet or prefixed with an additional <code>unet</code> which can be used to distinguish between text
encoder lora layers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>transformer</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The Transformer model to load the LoRA layers into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`SD3Transformer2DModel`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>Speed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>up model loading by only loading the pretrained LoRA weights and not initializing the random weights.</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lora_into_transformer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">transformer</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_pipeline</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This will load the LoRA layers specified in `state_dict` into `transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        state_dict (`dict`):</span>
<span class="sd">            A standard state dict containing the lora layer parameters. The keys can either be indexed directly</span>
<span class="sd">            into the unet or prefixed with an additional `unet` which can be used to distinguish between text</span>
<span class="sd">            encoder lora layers.</span>
<span class="sd">        transformer (`SD3Transformer2DModel`):</span>
<span class="sd">            The Transformer model to load the LoRA layers into.</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        Speed up model loading by only loading the pretrained LoRA weights and not initializing the random weights.:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the layers corresponding to transformer.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">transformer</span><span class="o">.</span><span class="n">load_lora_adapter</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">,</span>
        <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
        <span class="n">_pipeline</span><span class="o">=</span><span class="n">_pipeline</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.load_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Load LoRA weights specified in <code>pretrained_model_name_or_path_or_dict</code> into <code>self.unet</code> and
<code>self.text_encoder</code>.</p>
<p>All kwargs are forwarded to <code>self.lora_state_dict</code>.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>] for more details on how the state dict is
loaded.</p>
<p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer</code>] for more details on how the state
dict is loaded into <code>self.transformer</code>.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter name to be used for referencing the loaded adapter model. If not specified, it will use
<code>default_{i}</code> where i is the total number of adapters being loaded.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kwargs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>See [<code>~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict</code>].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`dict`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>{}</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">adapter_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and</span>
<span class="sd">    `self.text_encoder`.</span>

<span class="sd">    All kwargs are forwarded to `self.lora_state_dict`.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is</span>
<span class="sd">    loaded.</span>

<span class="sd">    See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_transformer`] for more details on how the state</span>
<span class="sd">    dict is loaded into `self.transformer`.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">        adapter_name (`str`, *optional*):</span>
<span class="sd">            Adapter name to be used for referencing the loaded adapter model. If not specified, it will use</span>
<span class="sd">            `default_{i}` where i is the total number of adapters being loaded.</span>
<span class="sd">        kwargs (`dict`, *optional*):</span>
<span class="sd">            See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if a dict is passed, copy it instead of modifying it inplace</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># First, ensure that the checkpoint is a compatible one and can be successfully loaded.</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">is_correct_format</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="s2">&quot;lora&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_correct_format</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid LoRA checkpoint.&quot;</span><span class="p">)</span>

    <span class="n">transformer_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;transformer.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_transformer</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span>
            <span class="n">transformer</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;transformer&quot;</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">text_encoder_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">text_encoder_2_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;text_encoder_2.&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_encoder_2_state_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_lora_into_text_encoder</span><span class="p">(</span>
            <span class="n">text_encoder_2_state_dict</span><span class="p">,</span>
            <span class="n">network_alphas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">text_encoder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">text_encoder_2</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;text_encoder_2&quot;</span><span class="p">,</span>
            <span class="n">lora_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_scale</span><span class="p">,</span>
            <span class="n">adapter_name</span><span class="o">=</span><span class="n">adapter_name</span><span class="p">,</span>
            <span class="n">_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">lora_state_dict</span><span class="p">(</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.lora_state_dict" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Return state dict for lora weights and the network alphas.</p>
<p><Tip warning={true}></p>
<p>We support loading A1111 formatted LoRA checkpoints in a limited capacity.</p>
<p>This function is experimental and might change in the future.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>pretrained_model_name_or_path_or_dict</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Can be either:</p>
<div class="highlight"><pre><span></span><code>- A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
  the Hub.
- A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
  with [`ModelMixin.save_pretrained`].
- A MindSpore state dict.
</code></pre></div>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike` or `dict`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>cache_dir</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
is not used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Union[str, os.PathLike]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>force_download</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>proxies</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A dictionary of proxy servers to use by protocol or endpoint, for example, <code>{'http': 'foo.bar:3128',
'http://hostname': 'foo.bar:4012'}</code>. The proxies are used on each request.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, str]`, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>local_files_only</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to only load local model weights and configuration files or not. If set to <code>True</code>, the model
won't be downloaded from the Hub.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `False`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>token</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The token to use as HTTP bearer authorization for remote files. If <code>True</code>, the token generated from
<code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>) is used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or *bool*, *optional*</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>revision</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
allowed by Git.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;main&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>subfolder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The subfolder location of a model file within a larger model repository on the Hub or locally.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str`, *optional*, defaults to `&#34;&#34;`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="nd">@validate_hf_hub_args</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return state dict for lora weights and the network alphas.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    We support loading A1111 formatted LoRA checkpoints in a limited capacity.</span>

<span class="sd">    This function is experimental and might change in the future.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Parameters:</span>
<span class="sd">        pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):</span>
<span class="sd">            Can be either:</span>

<span class="sd">                - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on</span>
<span class="sd">                  the Hub.</span>
<span class="sd">                - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved</span>
<span class="sd">                  with [`ModelMixin.save_pretrained`].</span>
<span class="sd">                - A MindSpore state dict.</span>

<span class="sd">        cache_dir (`Union[str, os.PathLike]`, *optional*):</span>
<span class="sd">            Path to a directory where a downloaded pretrained model configuration is cached if the standard cache</span>
<span class="sd">            is not used.</span>
<span class="sd">        force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to force the (re-)download of the model weights and configuration files, overriding the</span>
<span class="sd">            cached versions if they exist.</span>

<span class="sd">        proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">            A dictionary of proxy servers to use by protocol or endpoint, for example, `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">            &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">        local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether to only load local model weights and configuration files or not. If set to `True`, the model</span>
<span class="sd">            won&#39;t be downloaded from the Hub.</span>
<span class="sd">        token (`str` or *bool*, *optional*):</span>
<span class="sd">            The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from</span>
<span class="sd">            `diffusers-cli login` (stored in `~/.huggingface`) is used.</span>
<span class="sd">        revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">            The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier</span>
<span class="sd">            allowed by Git.</span>
<span class="sd">        subfolder (`str`, *optional*, defaults to `&quot;&quot;`):</span>
<span class="sd">            The subfolder location of a model file within a larger model repository on the Hub or locally.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load the main state dict first which has the LoRA layers for either of</span>
    <span class="c1"># transformer and text encoder or both.</span>
    <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">weight_name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;weight_name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">use_safetensors</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_safetensors&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">use_safetensors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">use_safetensors</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;attn_procs_weights&quot;</span><span class="p">,</span>
        <span class="s2">&quot;framework&quot;</span><span class="p">:</span> <span class="s2">&quot;pytorch&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">_fetch_state_dict</span><span class="p">(</span>
        <span class="n">pretrained_model_name_or_path_or_dict</span><span class="o">=</span><span class="n">pretrained_model_name_or_path_or_dict</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">use_safetensors</span><span class="o">=</span><span class="n">use_safetensors</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
        <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
        <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
        <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
        <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
        <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
        <span class="n">allow_pickle</span><span class="o">=</span><span class="n">allow_pickle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">is_dora_scale_present</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="s2">&quot;dora_scale&quot;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_dora_scale_present</span><span class="p">:</span>
        <span class="n">warn_msg</span> <span class="o">=</span> <span class="s2">&quot;It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to &#39;dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">warn_msg</span><span class="p">)</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;dora_scale&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">save_lora_weights</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">transformer_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_encoder_2_lora_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_main_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.save_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Save the LoRA parameters corresponding to the UNet and text encoder.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>save_directory</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Directory to save LoRA parameters to. Will be created if it doesn't exist.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`str` or `os.PathLike`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>transformer_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>transformer</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from 🤗 Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>text_encoder_2_lora_layers</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>State dict of the LoRA layers corresponding to the <code>text_encoder_2</code>. Must explicitly pass the text
encoder LoRA state dict because it comes from 🤗 Transformers.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>is_main_process</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether the process calling this is the main process or not. Useful during distributed training and you
need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on the main
process to avoid race conditions.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>save_function</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The function to use to save the state dictionary. Useful during distributed training when you need to
replace <code>mindspore.save_checkpoint</code> with another method. Can be configured with the environment variable
<code>DIFFUSERS_SAVE_MODE</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_serialization</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to save the model using <code>safetensors</code> or the traditional MindSpore way.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, *optional*, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="n">transformer_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the LoRA parameters corresponding to the UNet and text encoder.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        save_directory (`str` or `os.PathLike`):</span>
<span class="sd">            Directory to save LoRA parameters to. Will be created if it doesn&#39;t exist.</span>
<span class="sd">        transformer_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `transformer`.</span>
<span class="sd">        text_encoder_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">        text_encoder_2_lora_layers (`Dict[str, nn.Cell]` or `Dict[str, ms.Tensor]`):</span>
<span class="sd">            State dict of the LoRA layers corresponding to the `text_encoder_2`. Must explicitly pass the text</span>
<span class="sd">            encoder LoRA state dict because it comes from 🤗 Transformers.</span>
<span class="sd">        is_main_process (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether the process calling this is the main process or not. Useful during distributed training and you</span>
<span class="sd">            need to call this function on all processes. In this case, set `is_main_process=True` only on the main</span>
<span class="sd">            process to avoid race conditions.</span>
<span class="sd">        save_function (`Callable`):</span>
<span class="sd">            The function to use to save the state dictionary. Useful during distributed training when you need to</span>
<span class="sd">            replace `mindspore.save_checkpoint` with another method. Can be configured with the environment variable</span>
<span class="sd">            `DIFFUSERS_SAVE_MODE`.</span>
<span class="sd">        safe_serialization (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">            Whether to save the model using `safetensors` or the traditional MindSpore way.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">transformer_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_lora_layers</span> <span class="ow">or</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You must pass at least one of `transformer_lora_layers`, `text_encoder_lora_layers`, `text_encoder_2_lora_layers`.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">transformer_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">transformer_lora_layers</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">transformer_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">text_encoder_2_lora_layers</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">pack_weights</span><span class="p">(</span><span class="n">text_encoder_2_lora_layers</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">))</span>

    <span class="c1"># Save the model</span>
    <span class="bp">cls</span><span class="o">.</span><span class="n">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="o">=</span><span class="n">is_main_process</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="o">=</span><span class="n">weight_name</span><span class="p">,</span>
        <span class="n">save_function</span><span class="o">=</span><span class="n">save_function</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="o">=</span><span class="n">safe_serialization</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_pipeline</span><span class="o">.</span><span class="n">SD3LoraLoaderMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;transformer&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder&#39;</span><span class="p">,</span> <span class="s1">&#39;text_encoder_2&#39;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_pipeline.SD3LoraLoaderMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[&#39;transformer&#39;, &#39;text_encoder&#39;, &#39;text_encoder_2&#39;]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_pipeline.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder&quot;</span><span class="p">,</span> <span class="s2">&quot;text_encoder_2&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="n">components</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="doc doc-heading">
            <code>mindone.diffusers.loaders.lora_base.LoraBaseMixin</code>


<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Utility class for handling LoRAs.</p>

              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LoraBaseMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility class for handling LoRAs.&quot;&quot;&quot;</span>

    <span class="n">_lora_loadable_modules</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_fused_loras</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`load_lora_weights()` is not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_lora_weights</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`save_lora_weights()` not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_state_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`lora_state_dict()` is not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_optionally_disable_offloading</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">_pipeline</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;`_optionally_disable_offloading()` is not implemented.&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_fetch_state_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">deprecation_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Using the `_fetch_state_dict()` method from </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> has been deprecated and will be removed in a future version. Please use `from diffusers.loaders.lora_base import _fetch_state_dict`.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;_fetch_state_dict&quot;</span><span class="p">,</span> <span class="s2">&quot;0.35.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_fetch_state_dict</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_best_guess_weight_name</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">deprecation_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Using the `_best_guess_weight_name()` method from </span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2"> has been deprecated and will be removed in a future version. Please use `from diffusers.loaders.lora_base import _best_guess_weight_name`.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;_best_guess_weight_name&quot;</span><span class="p">,</span> <span class="s2">&quot;0.35.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_best_guess_weight_name</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Unloads the LoRA parameters.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">        &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">        &gt;&gt;&gt; ...</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">            lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">                Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">            safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">            adapter_names (`List[str]`, *optional*):</span>
<span class="sd">                Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">        import mindspore</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">        pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_unet` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_unet&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_transformer` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_transformer&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;fuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_text_encoder` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;fuse_text_encoder&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># check if diffusers model</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
                <span class="c1"># handle transformers models.</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">fuse_text_encoder_lora</span><span class="p">(</span>
                        <span class="n">model</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
                    <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reverses the effect of</span>
<span class="sd">        [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This is an experimental API.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">            unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">            unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">                LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_unet` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_unet&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_transformer` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_transformer&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;unfuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_text_encoder` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
            <span class="n">deprecate</span><span class="p">(</span>
                <span class="s2">&quot;unfuse_text_encoder&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
                <span class="n">depr_message</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">)):</span>
                    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                            <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">-=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_adapters</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">adapter_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">adapter_names</span>

        <span class="n">adapter_weights</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span>

        <span class="c1"># Expand weights into a list, one entry per adapter</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">adapter_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_weights</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Length of adapter names </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span><span class="si">}</span><span class="s2"> is not equal to the length of the weights </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">adapter_weights</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">list_adapters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span>  <span class="c1"># eg {&quot;unet&quot;: [&quot;adapter1&quot;, &quot;adapter2&quot;], &quot;text_encoder&quot;: [&quot;adapter2&quot;]}</span>
        <span class="c1"># eg [&quot;adapter1&quot;, &quot;adapter2&quot;]</span>
        <span class="n">all_adapters</span> <span class="o">=</span> <span class="p">{</span><span class="n">adapter</span> <span class="k">for</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">}</span>
        <span class="n">missing_adapters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span> <span class="o">-</span> <span class="n">all_adapters</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">missing_adapters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adapter name(s) </span><span class="si">{</span><span class="n">missing_adapters</span><span class="si">}</span><span class="s2"> not in the list of present adapters: </span><span class="si">{</span><span class="n">all_adapters</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="c1"># eg {&quot;adapter1&quot;: [&quot;unet&quot;], &quot;adapter2&quot;: [&quot;unet&quot;, &quot;text_encoder&quot;]}</span>
        <span class="n">invert_list_adapters</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">adapter</span><span class="p">:</span> <span class="p">[</span><span class="n">part</span> <span class="k">for</span> <span class="n">part</span><span class="p">,</span> <span class="n">adapters</span> <span class="ow">in</span> <span class="n">list_adapters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">adapters</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">adapter</span> <span class="ow">in</span> <span class="n">all_adapters</span>
        <span class="p">}</span>

        <span class="c1"># Decompose weights into weights for denoiser and text encoders.</span>
        <span class="n">_component_adapter_weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">adapter_name</span><span class="p">,</span> <span class="n">weights</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">adapter_weights</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">component_adapter_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">):</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Lora weight dict contains </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2"> weights but will be ignored because pipeline does not have </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">.&quot;</span>
                        <span class="p">)</span>

                    <span class="k">if</span> <span class="n">component_adapter_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Lora weight dict for adapter &#39;</span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2">&#39; contains </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">,&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;but this will be ignored because </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> does not contain weights for </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">.&quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;Valid parts for </span><span class="si">{</span><span class="n">adapter_name</span><span class="si">}</span><span class="s2"> are: </span><span class="si">{</span><span class="n">invert_list_adapters</span><span class="p">[</span><span class="n">adapter_name</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
                            <span class="p">)</span>
                        <span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">component_adapter_weights</span> <span class="o">=</span> <span class="n">weights</span>

                <span class="n">_component_adapter_weights</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">component_adapter_weights</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">set_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">set_adapters_for_text_encoder</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">_component_adapter_weights</span><span class="p">[</span><span class="n">component</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">disable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">disable_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">disable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">enable_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">enable_lora</span><span class="p">()</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="n">enable_lora_for_text_encoder</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">        Deletes the LoRA layers of `adapter_name` for the unet and text-encoder(s).</span>
<span class="sd">            adapter_names (`Union[List[str], str]`):</span>
<span class="sd">                The names of the adapter to delete. Can be a single string or a list of strings</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                    <span class="n">model</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
                        <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the list of the current active adapters.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        from mindone.diffusers import DiffusionPipeline</span>

<span class="sd">        pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">            &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">        )</span>
<span class="sd">        pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">        pipeline.get_active_adapters()</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
                        <span class="k">break</span>

        <span class="k">return</span> <span class="n">active_adapters</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">))</span>
                <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">set_adapters</span><span class="p">[</span><span class="n">component</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">set_adapters</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">pack_weights</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">prefix</span><span class="p">):</span>
        <span class="n">layers_weights</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">parameters_dict</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span> <span class="k">else</span> <span class="n">layers</span>
        <span class="n">layers_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">module_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">layers_weights</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">return</span> <span class="n">layers_state_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">write_lora_layers</span><span class="p">(</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">is_main_process</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">safe_serialization</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">save_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">save_function</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">save_file</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;np&quot;</span><span class="p">})</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">save_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">safe_serialization</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME_SAFE</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weight_name</span> <span class="o">=</span> <span class="n">LORA_WEIGHT_NAME</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">()</span>
        <span class="n">save_function</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model weights saved in </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">lora_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="c1"># property function that returns the lora scale which can be set at run time by the pipeline.</span>
        <span class="c1"># if _lora_scale has not been set, return 1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_scale</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_lora_scale&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="mf">1.0</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.delete_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Deletes the LoRA layers of <code>adapter_name</code> for the unet and text-encoder(s).
    adapter_names (<code>Union[List[str], str]</code>):
        The names of the adapter to delete. Can be a single string or a list of strings</p>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">delete_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">    Deletes the LoRA layers of `adapter_name` for the unet and text-encoder(s).</span>
<span class="sd">        adapter_names (`Union[List[str], str]`):</span>
<span class="sd">            The names of the adapter to delete. Can be a single string or a list of strings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">adapter_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">adapter_names</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">delete_adapters</span><span class="p">(</span><span class="n">adapter_names</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">adapter_name</span> <span class="ow">in</span> <span class="n">adapter_names</span><span class="p">:</span>
                    <span class="n">delete_adapter_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adapter_name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[],</span> <span class="n">lora_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.fuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fuses the LoRA parameters into the original parameters of the corresponding blocks.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>(<code>List[str]</code>): List of LoRA-injectable components to fuse the LoRAs into.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>lora_scale</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls how much to influence the outputs with the LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safe_fusing</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `False`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>adapter_names</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mindspore</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span> <span class="n">mindspore_dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;nerijs/pixel-art-xl&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;pixel-art-xl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;pixel&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fuse_lora</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">lora_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">safe_fusing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">adapter_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fuses the LoRA parameters into the original parameters of the corresponding blocks.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.</span>
<span class="sd">        lora_scale (`float`, defaults to 1.0):</span>
<span class="sd">            Controls how much to influence the outputs with the LoRA parameters.</span>
<span class="sd">        safe_fusing (`bool`, defaults to `False`):</span>
<span class="sd">            Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.</span>
<span class="sd">        adapter_names (`List[str]`, *optional*):</span>
<span class="sd">            Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```py</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>
<span class="sd">    import mindspore</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, mindspore_dtype=mindspore.float16</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;nerijs/pixel-art-xl&quot;, weight_name=&quot;pixel-art-xl.safetensors&quot;, adapter_name=&quot;pixel&quot;)</span>
<span class="sd">    pipeline.fuse_lora(lora_scale=0.7)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_unet` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_unet&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_transformer` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_transformer&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;fuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `fuse_text_encoder` to `fuse_lora()` is deprecated and will be ignored. Please use the `components` argument and provide a list of the components whose LoRAs are to be fused. `fuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;fuse_text_encoder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># check if diffusers model</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">fuse_lora</span><span class="p">(</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span><span class="p">)</span>
            <span class="c1"># handle transformers models.</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">fuse_text_encoder_lora</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">lora_scale</span><span class="o">=</span><span class="n">lora_scale</span><span class="p">,</span> <span class="n">safe_fusing</span><span class="o">=</span><span class="n">safe_fusing</span><span class="p">,</span> <span class="n">adapter_names</span><span class="o">=</span><span class="n">adapter_names</span>
                <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_active_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the list of the current active adapters.</p>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">mindone.diffusers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiffusionPipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">DiffusionPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">load_lora_weights</span><span class="p">(</span><span class="s2">&quot;CiroN2022/toy-face&quot;</span><span class="p">,</span> <span class="n">weight_name</span><span class="o">=</span><span class="s2">&quot;toy_face_sdxl.safetensors&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;toy&quot;</span><span class="p">)</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">get_active_adapters</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_active_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the list of the current active adapters.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    from mindone.diffusers import DiffusionPipeline</span>

<span class="sd">    pipeline = DiffusionPipeline.from_pretrained(</span>
<span class="sd">        &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,</span>
<span class="sd">    )</span>
<span class="sd">    pipeline.load_lora_weights(&quot;CiroN2022/toy-face&quot;, weight_name=&quot;toy_face_sdxl.safetensors&quot;, adapter_name=&quot;toy&quot;)</span>
<span class="sd">    pipeline.get_active_adapters()</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">active_adapters</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                    <span class="n">active_adapters</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">active_adapters</span>
                    <span class="k">break</span>

    <span class="k">return</span> <span class="n">active_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">get_list_adapters</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.get_list_adapters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Gets the current list of all available adapters in the pipeline.</p>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_list_adapters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the current list of all available adapters in the pipeline.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">set_adapters</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">))</span>
            <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;peft_config&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">set_adapters</span><span class="p">[</span><span class="n">component</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">peft_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">set_adapters</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">unfuse_lora</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unfuse_lora" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Reverses the effect of
<a href="https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora"><code>pipe.fuse_lora()</code></a>.</p>
<p><Tip warning={true}></p>
<p>This is an experimental API.</p>
<p></Tip></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>components</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of LoRA-injectable components to unfuse LoRA from.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`List[str]`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>[]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_unet</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the UNet LoRA parameters.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>unfuse_text_encoder</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the
LoRA parameters then it won't have any effect.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfuse_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the effect of</span>
<span class="sd">    [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This is an experimental API.</span>

<span class="sd">    &lt;/Tip&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.</span>
<span class="sd">        unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.</span>
<span class="sd">        unfuse_text_encoder (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn&#39;t monkey-patched with the</span>
<span class="sd">            LoRA parameters then it won&#39;t have any effect.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_unet&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_unet` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_unet` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_unet&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_transformer&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_transformer` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_transformer` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_transformer&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;unfuse_text_encoder&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">depr_message</span> <span class="o">=</span> <span class="s2">&quot;Passing `unfuse_text_encoder` to `unfuse_lora()` is deprecated and will be ignored. Please use the `components` argument. `unfuse_text_encoder` will be removed in a future version.&quot;</span>  <span class="c1"># noqa: E501</span>
        <span class="n">deprecate</span><span class="p">(</span>
            <span class="s2">&quot;unfuse_text_encoder&quot;</span><span class="p">,</span>
            <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
            <span class="n">depr_message</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">components</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`components` cannot be an empty list.&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">fuse_component</span> <span class="ow">in</span> <span class="n">components</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">fuse_component</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fuse_component</span><span class="si">}</span><span class="s2"> is not found in </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="si">=}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fuse_component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="p">(</span><span class="n">ModelMixin</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseTunerLayer</span><span class="p">):</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">unmerge</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_fused_loras</span> <span class="o">-=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">loaders</span><span class="o">.</span><span class="n">lora_base</span><span class="o">.</span><span class="n">LoraBaseMixin</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span></code>

<a href="#mindone.diffusers.loaders.lora_base.LoraBaseMixin.unload_lora_weights" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Unloads the LoRA parameters.</p>
<p>Examples:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="c1"># Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">unload_lora_weights</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="o">...</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/loaders/lora_base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unload_lora_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unloads the LoRA parameters.</span>

<span class="sd">    Examples:</span>

<span class="sd">    ```python</span>
<span class="sd">    &gt;&gt;&gt; # Assuming `pipeline` is already loaded with the LoRA parameters.</span>
<span class="sd">    &gt;&gt;&gt; pipeline.unload_lora_weights()</span>
<span class="sd">    &gt;&gt;&gt; ...</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lora_loadable_modules</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">ModelMixin</span><span class="p">):</span>
                <span class="n">model</span><span class="o">.</span><span class="n">unload_lora</span><span class="p">()</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">MSPreTrainedModel</span><span class="p">):</span>
                <span class="n">_remove_text_encoder_monkey_patch</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 21, 2024 01:50:07 UTC">November 21, 2024</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 1, 2024 02:56:38 UTC">November 1, 2024</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:77485245+wcrzlh@users.noreply.github.com">Chaoran Wei</a>, 
        <a href="mailto:townwish2023@outlook.com">townwish4git</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../ip_adapter/" class="md-footer__link md-footer__link--prev" aria-label="Previous: IP-Adapter">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                IP-Adapter
              </div>
            </div>
          </a>
        
        
          
          <a href="../single_file/" class="md-footer__link md-footer__link--next" aria-label="Next: Single files">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Single files
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 MindSpore Lab
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindone" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>