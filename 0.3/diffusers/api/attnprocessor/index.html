
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindone/0.3/diffusers/api/attnprocessor/">
      
      
        <link rel="prev" href="../internal_classes_overview/">
      
      
        <link rel="next" href="../activations/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Attention Processor - MindOne - One for All</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Source Sans Pro";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#attention-processor" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="MindOne - One for All" class="md-header__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOne - One for All
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Attention Processor
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
  Diffusers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../transformers/" class="md-tabs__link">
          
  
  
  Transformers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../peft/" class="md-tabs__link">
          
  
  
  PEFT

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="MindOne - One for All" class="md-nav__button md-logo" aria-label="MindOne - One for All" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOne - One for All
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindone" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindone
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Diffusers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Diffusers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ§¨ Diffusers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quicktour/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quicktour
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Effective and efficient diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../limitations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Limitations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/tutorial_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/write_own_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding pipelines, models and schedulers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/autopipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/basic_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Train a diffusion model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/using_peft_for_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load LoRAs for inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Load pipelines and adapters
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Load pipelines and adapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/loading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/schedulers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load schedulers and models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/other-formats/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model files and layouts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/loading_adapters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Load adapters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/push_to_hub/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Push files to the Hub
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Generative tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Generative tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/unconditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/conditional_image_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/text-img2vid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Inference techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Inference techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/overview_techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/merge_loras/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Merge LoRAs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/scheduler_features/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler features
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/callback/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pipeline callbacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/reusing_seeds/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reproducible pipelines
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Advanced inference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Advanced inference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../advanced_inference/outpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outpainting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_7" >
        
          
          <label class="md-nav__link" for="__nav_2_7" id="__nav_2_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Specific pipeline examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_7">
            <span class="md-nav__icon md-icon"></span>
            Specific pipeline examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/t2i_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/inference_with_lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/textual_inversion_inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/shap-e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/inference_with_tcd_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trajectory Consistency Distillation-LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Video Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/marigold_usage/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold Computer Vision
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/create_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Create a dataset for training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/adapt_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adapt a model to a new task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_4" >
        
          
          <label class="md-nav__link" for="__nav_2_8_4" id="__nav_2_8_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_4">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/unconditional_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unconditional image generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/text2image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8_5" >
        
          
          <label class="md-nav__link" for="__nav_2_8_5" id="__nav_2_8_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8_5">
            <span class="md-nav__icon md-icon"></span>
            Methods
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/text_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/dreambooth/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DreamBooth
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_9" >
        
          
          <label class="md-nav__link" for="__nav_2_9" id="__nav_2_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Accelerate inference and reduce memory
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_9">
            <span class="md-nav__icon md-icon"></span>
            Accelerate inference and reduce memory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/fp16/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speed up inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reduce memory usage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../optimization/xformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    xFormers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_10" >
        
          
          <label class="md-nav__link" for="__nav_2_10" id="__nav_2_10_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Conceptual Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_10">
            <span class="md-nav__icon md-icon"></span>
            Conceptual Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../conceptual/philosophy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Philosophy
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../using-diffusers/controlling_generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Controlled generation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11" id="__nav_2_11_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_11_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_1" >
        
          
          <label class="md-nav__link" for="__nav_2_11_1" id="__nav_2_11_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Main Classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_1">
            <span class="md-nav__icon md-icon"></span>
            Main Classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../outputs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Outputs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_2" >
        
          
          <label class="md-nav__link" for="__nav_2_11_2" id="__nav_2_11_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Loaders
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_2">
            <span class="md-nav__icon md-icon"></span>
            Loaders
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/ip_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IP-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LoRA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/single_file/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single files
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/textual_inversion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Textual Inversion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/transformer_sd3.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2D
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loaders/peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3" id="__nav_2_11_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/auto_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_3" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_3" id="__nav_2_11_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ControlNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_3">
            <span class="md-nav__icon md-icon"></span>
            ControlNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3ControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet_sparsectrl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SparseControlNetModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnionModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_4" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_4" id="__nav_2_11_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_4">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/allegro_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AllegroTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/aura_flow_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlowTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cogvideox_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/consisid_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisIDTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cogview3plus_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3PlusTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/cogview4_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/dit_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiTTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/easyanimate_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimateTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/flux_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/hunyuan_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/hunyuan_video_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/latte_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LatteTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/lumina_nextdit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LuminaNextDiT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/lumina2_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina2Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/ltx_video_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideoTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/mochi_transformer3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MochiTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/omnigen_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGenTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/pixart_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArtTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/prior_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PriorTransformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/sd3_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SD3Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/sana_transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SanaTransformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/stable_audio_transformer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableAudioDiTModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/transformer2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformer2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/transformer_temporal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TransformerTemporalModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/wan_transformer_3d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WanTransformer3DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_5" id="__nav_2_11_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    UNets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_5">
            <span class="md-nav__icon md-icon"></span>
            UNets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/stable_cascade_unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    StableCascadeUNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet1DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/unet2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/unet2d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet2DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/unet3d-cond/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNet3DConditionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/unet-motion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UNetMotionModel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/uvit2d/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UViT2DModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_3_6" >
        
          
          <label class="md-nav__link" for="__nav_2_11_3_6" id="__nav_2_11_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VAEs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_3_6">
            <span class="md-nav__icon md-icon"></span>
            VAEs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl_allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLAllegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLCogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoder_kl_hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLHunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl_ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLLTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl_magvit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMagvit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoderkl_mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLMochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoder_kl_wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderKLWan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/asymmetricautoencoderkl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AsymmetricAutoencoderKL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoder_dc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoencoderDC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/consistency_decoder_vae/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderVAE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoder_oobleck/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Oobleck AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/autoencoder_tiny/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tiny AutoEncoder
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/vq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQModel
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4" >
        
          
          <label class="md-nav__link" for="__nav_2_11_4" id="__nav_2_11_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pipelines
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_4">
            <span class="md-nav__icon md-icon"></span>
            Pipelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/allegro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Allegro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/amused/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    aMUSEd
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/animatediff/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AnimateDiff
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/attend_and_excite/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attend-and-Excite
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/audioldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/audioldm2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AudioLDM 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/aura_flow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AuraFlow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/auto_pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoPipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/blip_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BLIP-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/cogview3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/cogview4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogView4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/consisid/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsisID
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet_flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Flux.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet_hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet_sd3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnetxs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnetxs_sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNet-XS with Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/controlnet_union/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ControlNetUnion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/dance_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dance Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/deepfloyd_if/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepFloyd IF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/diffedit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiffEdit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/dit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/easyanimate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EasyAnimate
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/flux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Flux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/control_flux_inpaint.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FluxControlInpaint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/hunyuandit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hunyuan-DiT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/hunyuan_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HunyuanVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/i2vgenxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I2VGen-XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/pix2pix/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructPix2Pix
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/kandinsky/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/kandinsky_v22/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 2.2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/kandinsky3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kandinsky 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/kolors/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kolors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/latent_consistency_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Consistency Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/latent_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/latte/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latte
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/ledits_pp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LEDITS++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/ltx_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LTXVideo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/lumina2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina 2.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/lumina/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lumina-T2X
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/marigold/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Marigold
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/mochi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mochi
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/panorama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MultiDiffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/musicldm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MusicLDM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/omnigen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OmniGen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/pag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PAG
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/paint_by_example/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paint by Example
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/pia/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Personalized Image Animator (PIA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/pixart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î±
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/pixart_sigma/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PixArt-Î£
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/sana/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/sana_sprint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sana Sprint
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/self_attention_guidance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self-Attention Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/semantic_stable_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Guidance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/shap_e/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shap-E
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Audio
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_cascade/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Cascade
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_4_65" >
        
          
          <label class="md-nav__link" for="__nav_2_11_4_65" id="__nav_2_11_4_65_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_2_11_4_65_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_4_65">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/text2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/img2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/svd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/inpaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inpainting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/depth2img/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Depth-to-image
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/image_variation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image variation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/stable_diffusion_safe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Safe Stable Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/stable_diffusion_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/stable_diffusion_3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion 3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/stable_diffusion_xl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Diffusion XL
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/sdxl_turbo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SDXL Turbo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/latent_upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Latent upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/upscale/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Super-resolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/k_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Diffusion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/ldm3d_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    T2I-Adapter
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_diffusion/gligen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GLIGEN (Grounded Language-to-Image Generation)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/stable_unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/text_to_video/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text-to-video
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/text_to_video_zero/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text2Video-Zero
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/unclip/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    unCLIP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/unidiffuser/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniDiffuser
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/value_guided_sampling.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Value-guided sampling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/wan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wan
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pipelines/wuerstchen/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Wuerstchen
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_5" >
        
          
          <label class="md-nav__link" for="__nav_2_11_5" id="__nav_2_11_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Schedulers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_11_5">
            <span class="md-nav__icon md-icon"></span>
            Schedulers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/cm_stochastic_iterative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMStochasticIterativeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/ddim_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/multistep_dpm_solver_cogvideox/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CogVideoXDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/consistency_decoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ConsistencyDecoderScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/cosine_dpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CosineDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/ddim_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMInverseScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/ddim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDIMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/ddpm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DDPMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/deis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DEISMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/multistep_dpm_solver_inverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepInverse
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/dpm_sde/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSDEScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/singlestep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPMSolverSinglestepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/edm_multistep_dpm_solver/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMDPMSolverMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/edm_euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EDMEulerScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/euler_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerAncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/euler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/flow_match_euler_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchEulerDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/flow_match_heun_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FlowMatchHeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/heun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HeunDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/ipndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IPNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/stochastic_karras_ve.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KarrasVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/dpm_discrete_ancestral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2AncestralDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/dpm_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KDPM2DiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/lcm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LCMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/lms_discrete/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LMSDiscreteScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/pndm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PNDMScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/repaint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RePaintScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/score_sde_ve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVeScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/score_sde_vp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ScoreSdeVpScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/tcd/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TCDScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/unipc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UniPCMultistepScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedulers/vq_diffusion/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VQDiffusionScheduler
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_11_6" checked>
        
          
          <label class="md-nav__link" for="__nav_2_11_6" id="__nav_2_11_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Internal classes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_11_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_11_6">
            <span class="md-nav__icon md-icon"></span>
            Internal classes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../internal_classes_overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Attention Processor
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Attention Processor
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#attnprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnAddedKVProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnAddedKVProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AllegroAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AllegroAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AuraFlowAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AuraFlowAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedAuraFlowAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedAuraFlowAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      CogVideoXAttnProcessor2_0
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CogVideoXAttnProcessor2_0">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0.apply_rotary_emb_for_image_part" class="md-nav__link">
    <span class="md-ellipsis">
      apply_rotary_emb_for_image_part
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedCogVideoXAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedCogVideoXAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.CrossFrameAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      CrossFrameAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CustomDiffusionAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      CustomDiffusionAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedFluxAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedFluxAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxSingleAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxSingleAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.HunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      HunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGHunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGHunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGHunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGHunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGIdentitySelfAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGIdentitySelfAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGIdentitySelfAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGIdentitySelfAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      IPAdapterAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      IPAdapterAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SD3IPAdapterJointAttnProcessor2_0
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3IPAdapterJointAttnProcessor2_0">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0.construct" class="md-nav__link">
    <span class="md-ellipsis">
      construct
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.JointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      JointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.LuminaAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      LuminaAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.MochiAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      MochiAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.MochiVaeAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      MochiVaeAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SanaMultiscaleAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SanaMultiscaleAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGSanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGSanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGIdentitySanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGIdentitySanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.StableAudioAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      StableAudioAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.XFormersAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      XFormersAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxIPAdapterJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxIPAdapterJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom activation functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Custom normalization layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utilities/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VAE Image Processor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../video_processor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Video Processor
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tutorials/finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tune a pretrained model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tutorials/finetune_distribute/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed training and mixed precision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../transformers/tutorials/generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generation with LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    PEFT
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            PEFT
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Get started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Get started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../peft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ðŸ¤— PEFT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#attnprocessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AttnAddedKVProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      AttnAddedKVProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AllegroAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AllegroAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.AuraFlowAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      AuraFlowAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedAuraFlowAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedAuraFlowAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      CogVideoXAttnProcessor2_0
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CogVideoXAttnProcessor2_0">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0.apply_rotary_emb_for_image_part" class="md-nav__link">
    <span class="md-ellipsis">
      apply_rotary_emb_for_image_part
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedCogVideoXAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedCogVideoXAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.CrossFrameAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      CrossFrameAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.CustomDiffusionAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      CustomDiffusionAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedFluxAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedFluxAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxSingleAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxSingleAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.HunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      HunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGHunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGHunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGHunyuanAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGHunyuanAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGIdentitySelfAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGIdentitySelfAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGIdentitySelfAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGIdentitySelfAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      IPAdapterAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      IPAdapterAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SD3IPAdapterJointAttnProcessor2_0
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SD3IPAdapterJointAttnProcessor2_0">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0.construct" class="md-nav__link">
    <span class="md-ellipsis">
      construct
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.JointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      JointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FusedJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FusedJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.LuminaAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      LuminaAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.MochiAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      MochiAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.MochiVaeAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      MochiVaeAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.SanaMultiscaleAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      SanaMultiscaleAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGCFGSanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGCFGSanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.PAGIdentitySanaLinearAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      PAGIdentitySanaLinearAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.StableAudioAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      StableAudioAttnProcessor2_0
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.XFormersAttnProcessor" class="md-nav__link">
    <span class="md-ellipsis">
      XFormersAttnProcessor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mindone.diffusers.models.attention_processor.FluxIPAdapterJointAttnProcessor2_0" class="md-nav__link">
    <span class="md-ellipsis">
      FluxIPAdapterJointAttnProcessor2_0
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/mindspore-lab/mindone/edit/master/docs/diffusers/api/attnprocessor.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindone/raw/master/docs/diffusers/api/attnprocessor.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="attention-processor">Attention Processor<a class="headerlink" href="#attention-processor" title="Permanent link">&para;</a></h1>
<p>An attention processor is a class for applying different types of attention mechanisms.</p>
<h2 id="attnprocessor">AttnProcessor<a class="headerlink" href="#attnprocessor" title="Permanent link">&para;</a></h2>


<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.AttnProcessor" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.AttnProcessor</code>


<a href="#mindone.diffusers.models.attention_processor.AttnProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Default processor for performing attention-related computations.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AttnProcessor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Default processor for performing attention-related computations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.AttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.AttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.AttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span>
<span class="normal">2756</span>
<span class="normal">2757</span>
<span class="normal">2758</span>
<span class="normal">2759</span>
<span class="normal">2760</span>
<span class="normal">2761</span>
<span class="normal">2762</span>
<span class="normal">2763</span>
<span class="normal">2764</span>
<span class="normal">2765</span>
<span class="normal">2766</span>
<span class="normal">2767</span>
<span class="normal">2768</span>
<span class="normal">2769</span>
<span class="normal">2770</span>
<span class="normal">2771</span>
<span class="normal">2772</span>
<span class="normal">2773</span>
<span class="normal">2774</span>
<span class="normal">2775</span>
<span class="normal">2776</span>
<span class="normal">2777</span>
<span class="normal">2778</span>
<span class="normal">2779</span>
<span class="normal">2780</span>
<span class="normal">2781</span>
<span class="normal">2782</span>
<span class="normal">2783</span>
<span class="normal">2784</span>
<span class="normal">2785</span>
<span class="normal">2786</span>
<span class="normal">2787</span>
<span class="normal">2788</span>
<span class="normal">2789</span>
<span class="normal">2790</span>
<span class="normal">2791</span>
<span class="normal">2792</span>
<span class="normal">2793</span>
<span class="normal">2794</span>
<span class="normal">2795</span>
<span class="normal">2796</span>
<span class="normal">2797</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.AttnAddedKVProcessor" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.AttnAddedKVProcessor</code>


<a href="#mindone.diffusers.models.attention_processor.AttnAddedKVProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for performing attention-related computations with extra learnable key and value matrices for the text
encoder.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AttnAddedKVProcessor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for performing attention-related computations with extra learnable key and value matrices for the text</span>
<span class="sd">    encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

        <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">only_cross_attention</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FusedAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FusedAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FusedAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). It uses
fused projection layers. For self-attention modules, all projection matrices (i.e., query, key, value) are fused.
For cross-attention modules, key and value projection matrices are fused.</p>
<p><Tip warning={true}></p>
<p>This API is currently ðŸ§ª experimental in nature and can change in future.</p>
<p></Tip></p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4091</span>
<span class="normal">4092</span>
<span class="normal">4093</span>
<span class="normal">4094</span>
<span class="normal">4095</span>
<span class="normal">4096</span>
<span class="normal">4097</span>
<span class="normal">4098</span>
<span class="normal">4099</span>
<span class="normal">4100</span>
<span class="normal">4101</span>
<span class="normal">4102</span>
<span class="normal">4103</span>
<span class="normal">4104</span>
<span class="normal">4105</span>
<span class="normal">4106</span>
<span class="normal">4107</span>
<span class="normal">4108</span>
<span class="normal">4109</span>
<span class="normal">4110</span>
<span class="normal">4111</span>
<span class="normal">4112</span>
<span class="normal">4113</span>
<span class="normal">4114</span>
<span class="normal">4115</span>
<span class="normal">4116</span>
<span class="normal">4117</span>
<span class="normal">4118</span>
<span class="normal">4119</span>
<span class="normal">4120</span>
<span class="normal">4121</span>
<span class="normal">4122</span>
<span class="normal">4123</span>
<span class="normal">4124</span>
<span class="normal">4125</span>
<span class="normal">4126</span>
<span class="normal">4127</span>
<span class="normal">4128</span>
<span class="normal">4129</span>
<span class="normal">4130</span>
<span class="normal">4131</span>
<span class="normal">4132</span>
<span class="normal">4133</span>
<span class="normal">4134</span>
<span class="normal">4135</span>
<span class="normal">4136</span>
<span class="normal">4137</span>
<span class="normal">4138</span>
<span class="normal">4139</span>
<span class="normal">4140</span>
<span class="normal">4141</span>
<span class="normal">4142</span>
<span class="normal">4143</span>
<span class="normal">4144</span>
<span class="normal">4145</span>
<span class="normal">4146</span>
<span class="normal">4147</span>
<span class="normal">4148</span>
<span class="normal">4149</span>
<span class="normal">4150</span>
<span class="normal">4151</span>
<span class="normal">4152</span>
<span class="normal">4153</span>
<span class="normal">4154</span>
<span class="normal">4155</span>
<span class="normal">4156</span>
<span class="normal">4157</span>
<span class="normal">4158</span>
<span class="normal">4159</span>
<span class="normal">4160</span>
<span class="normal">4161</span>
<span class="normal">4162</span>
<span class="normal">4163</span>
<span class="normal">4164</span>
<span class="normal">4165</span>
<span class="normal">4166</span>
<span class="normal">4167</span>
<span class="normal">4168</span>
<span class="normal">4169</span>
<span class="normal">4170</span>
<span class="normal">4171</span>
<span class="normal">4172</span>
<span class="normal">4173</span>
<span class="normal">4174</span>
<span class="normal">4175</span>
<span class="normal">4176</span>
<span class="normal">4177</span>
<span class="normal">4178</span>
<span class="normal">4179</span>
<span class="normal">4180</span>
<span class="normal">4181</span>
<span class="normal">4182</span>
<span class="normal">4183</span>
<span class="normal">4184</span>
<span class="normal">4185</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). It uses</span>
<span class="sd">    fused projection layers. For self-attention modules, all projection matrices (i.e., query, key, value) are fused.</span>
<span class="sd">    For cross-attention modules, key and value projection matrices are fused.</span>

<span class="sd">    &lt;Tip warning={true}&gt;</span>

<span class="sd">    This API is currently ðŸ§ª experimental in nature and can change in future.</span>

<span class="sd">    &lt;/Tip&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">split_size</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
                <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="n">kv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">split_size</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.AllegroAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.AllegroAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.AllegroAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the Allegro model. It applies a normalization layer and rotary embedding on the query and key vector.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span>
<span class="normal">1842</span>
<span class="normal">1843</span>
<span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AllegroAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the Allegro model. It applies a normalization layer and rotary embedding on the query and key vector.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb_allegro</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb_allegro</span> <span class="o">=</span> <span class="n">apply_rotary_emb_allegro</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb_allegro</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_rotary_emb</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb_allegro</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_rotary_emb</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.AuraFlowAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.AuraFlowAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.AuraFlowAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing Aura Flow.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span>
<span class="normal">1961</span>
<span class="normal">1962</span>
<span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span>
<span class="normal">2014</span>
<span class="normal">2015</span>
<span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AuraFlowAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing Aura Flow.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Reshape.</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Apply QK norm.</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Concatenate the projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Attention.</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Split the attention outputs.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FusedAuraFlowAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FusedAuraFlowAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FusedAuraFlowAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing Aura Flow with fused projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span>
<span class="normal">2032</span>
<span class="normal">2033</span>
<span class="normal">2034</span>
<span class="normal">2035</span>
<span class="normal">2036</span>
<span class="normal">2037</span>
<span class="normal">2038</span>
<span class="normal">2039</span>
<span class="normal">2040</span>
<span class="normal">2041</span>
<span class="normal">2042</span>
<span class="normal">2043</span>
<span class="normal">2044</span>
<span class="normal">2045</span>
<span class="normal">2046</span>
<span class="normal">2047</span>
<span class="normal">2048</span>
<span class="normal">2049</span>
<span class="normal">2050</span>
<span class="normal">2051</span>
<span class="normal">2052</span>
<span class="normal">2053</span>
<span class="normal">2054</span>
<span class="normal">2055</span>
<span class="normal">2056</span>
<span class="normal">2057</span>
<span class="normal">2058</span>
<span class="normal">2059</span>
<span class="normal">2060</span>
<span class="normal">2061</span>
<span class="normal">2062</span>
<span class="normal">2063</span>
<span class="normal">2064</span>
<span class="normal">2065</span>
<span class="normal">2066</span>
<span class="normal">2067</span>
<span class="normal">2068</span>
<span class="normal">2069</span>
<span class="normal">2070</span>
<span class="normal">2071</span>
<span class="normal">2072</span>
<span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span>
<span class="normal">2103</span>
<span class="normal">2104</span>
<span class="normal">2105</span>
<span class="normal">2106</span>
<span class="normal">2107</span>
<span class="normal">2108</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedAuraFlowAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing Aura Flow with fused projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_added_qkv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">split_size</span> <span class="o">=</span> <span class="n">encoder_qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
            <span class="p">(</span>
                <span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">encoder_qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Reshape.</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Apply QK norm.</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Concatenate the projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Attention.</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Split the attention outputs.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on
query and key vectors, but does not include spatial normalization.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span>
<span class="normal">2494</span>
<span class="normal">2495</span>
<span class="normal">2496</span>
<span class="normal">2497</span>
<span class="normal">2498</span>
<span class="normal">2499</span>
<span class="normal">2500</span>
<span class="normal">2501</span>
<span class="normal">2502</span>
<span class="normal">2503</span>
<span class="normal">2504</span>
<span class="normal">2505</span>
<span class="normal">2506</span>
<span class="normal">2507</span>
<span class="normal">2508</span>
<span class="normal">2509</span>
<span class="normal">2510</span>
<span class="normal">2511</span>
<span class="normal">2512</span>
<span class="normal">2513</span>
<span class="normal">2514</span>
<span class="normal">2515</span>
<span class="normal">2516</span>
<span class="normal">2517</span>
<span class="normal">2518</span>
<span class="normal">2519</span>
<span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CogVideoXAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on</span>
<span class="sd">    query and key vectors, but does not include spatial normalization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_emb_for_image_part</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_state</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">start_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Equivalence of expression(when axis=2):</span>
<span class="sd">            `hidden_state[:, :, start_index:] = self.apply_rotary_emb(hidden_state[:, :, start_index:], image_rotary_emb)`</span>

<span class="sd">        Rewrite it since implement above might call ops.ScatterNdUpdate which is super slow!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hidden_state_text</span><span class="p">,</span> <span class="n">hidden_state_image</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">hidden_state</span><span class="p">,</span> <span class="p">(</span><span class="n">start_index</span><span class="p">,</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_index</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span>
        <span class="p">)</span>
        <span class="n">hidden_state_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">hidden_state_image</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_state_text</span><span class="p">,</span> <span class="n">hidden_state_image</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_state</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">text_seq_length</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="c1"># rewrite the implement for performance, refer to `self.apply_rotary_emb_for_image_part`</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb_for_image_part</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">,</span> <span class="n">text_seq_length</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb_for_image_part</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">,</span> <span class="n">text_seq_length</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="p">[</span><span class="n">text_seq_length</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">text_seq_length</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0.apply_rotary_emb_for_image_part" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">attention_processor</span><span class="o">.</span><span class="n">CogVideoXAttnProcessor2_0</span><span class="o">.</span><span class="n">apply_rotary_emb_for_image_part</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code>

<a href="#mindone.diffusers.models.attention_processor.CogVideoXAttnProcessor2_0.apply_rotary_emb_for_image_part" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Equivalence of expression(when axis=2):
    <code>hidden_state[:, :, start_index:] = self.apply_rotary_emb(hidden_state[:, :, start_index:], image_rotary_emb)</code></p>
<p>Rewrite it since implement above might call ops.ScatterNdUpdate which is super slow!</p>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_emb_for_image_part</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_state</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">start_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Equivalence of expression(when axis=2):</span>
<span class="sd">        `hidden_state[:, :, start_index:] = self.apply_rotary_emb(hidden_state[:, :, start_index:], image_rotary_emb)`</span>

<span class="sd">    Rewrite it since implement above might call ops.ScatterNdUpdate which is super slow!</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hidden_state_text</span><span class="p">,</span> <span class="n">hidden_state_image</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="n">hidden_state</span><span class="p">,</span> <span class="p">(</span><span class="n">start_index</span><span class="p">,</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_index</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span>
    <span class="p">)</span>
    <span class="n">hidden_state_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">hidden_state_image</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_state_text</span><span class="p">,</span> <span class="n">hidden_state_image</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_state</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FusedCogVideoXAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FusedCogVideoXAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FusedCogVideoXAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on
query and key vectors, but does not include spatial normalization.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span>
<span class="normal">2559</span>
<span class="normal">2560</span>
<span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span>
<span class="normal">2569</span>
<span class="normal">2570</span>
<span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span>
<span class="normal">2584</span>
<span class="normal">2585</span>
<span class="normal">2586</span>
<span class="normal">2587</span>
<span class="normal">2588</span>
<span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedCogVideoXAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on</span>
<span class="sd">    query and key vectors, but does not include spatial normalization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">text_seq_length</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">text_seq_length</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">text_seq_length</span><span class="p">:],</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">text_seq_length</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">text_seq_length</span><span class="p">:],</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="p">[</span><span class="n">text_seq_length</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">text_seq_length</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.CrossFrameAttnProcessor" class="doc doc-heading">
            <code>mindone.diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.CrossFrameAttnProcessor</code>


<a href="#mindone.diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero.CrossFrameAttnProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Cross frame attention processor. Each frame attends the first frame.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number that represents actual batch size, other than the frames.
For example, calling unet with a single prompt and num_images_per_prompt=1, batch_size should be equal to
2, due to classifier-free guidance.</p>
              </div>
              <p>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>2</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CrossFrameAttnProcessor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cross frame attention processor. Each frame attends the first frame.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size: The number that represents actual batch size, other than the frames.</span>
<span class="sd">            For example, calling unet with a single prompt and num_images_per_prompt=1, batch_size should be equal to</span>
<span class="sd">            2, due to classifier-free guidance.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="c1"># Cross Frame Attention</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cross_attention</span><span class="p">:</span>
            <span class="n">video_length</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">first_frame_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">video_length</span>

            <span class="c1"># rearrange keys to have batch and frames in the 1st and 2nd dims respectively</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">rearrange_3</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">video_length</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:,</span> <span class="n">first_frame_index</span><span class="p">]</span>
            <span class="c1"># rearrange values to have batch and frames in the 1st and 2nd dims respectively</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">rearrange_3</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">video_length</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:,</span> <span class="n">first_frame_index</span><span class="p">]</span>

            <span class="c1"># rearrange back to original shape</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">rearrange_4</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">rearrange_4</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.CustomDiffusionAttnProcessor" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.CustomDiffusionAttnProcessor</code>


<a href="#mindone.diffusers.models.attention_processor.CustomDiffusionAttnProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindspore.nn.Cell">Cell</span></code></p>


        <p>Processor for implementing attention for the Custom Diffusion method.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>train_kv</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to newly train the key and value matrices corresponding to the text features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>train_q_out</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to newly train query matrices corresponding to the latent image features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden size of the attention layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of channels in the <code>encoder_hidden_states</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_bias</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to include the bias parameter in <code>train_q_out</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`bool`, defaults to `True`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dropout</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The dropout probability to use.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, *optional*, defaults to 0.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CustomDiffusionAttnProcessor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing attention for the Custom Diffusion method.</span>

<span class="sd">    Args:</span>
<span class="sd">        train_kv (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to newly train the key and value matrices corresponding to the text features.</span>
<span class="sd">        train_q_out (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to newly train query matrices corresponding to the latent image features.</span>
<span class="sd">        hidden_size (`int`, *optional*, defaults to `None`):</span>
<span class="sd">            The hidden size of the attention layer.</span>
<span class="sd">        cross_attention_dim (`int`, *optional*, defaults to `None`):</span>
<span class="sd">            The number of channels in the `encoder_hidden_states`.</span>
<span class="sd">        out_bias (`bool`, defaults to `True`):</span>
<span class="sd">            Whether to include the bias parameter in `train_q_out`.</span>
<span class="sd">        dropout (`float`, *optional*, defaults to 0.0):</span>
<span class="sd">            The dropout probability to use.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_kv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">train_q_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cross_attention_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_kv</span> <span class="o">=</span> <span class="n">train_kv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_q_out</span> <span class="o">=</span> <span class="n">train_q_out</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_dim</span> <span class="o">=</span> <span class="n">cross_attention_dim</span>

        <span class="c1"># `_custom_diffusion` id for easy serialization and loading.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_kv</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_k_custom_diffusion</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span> <span class="ow">or</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_v_custom_diffusion</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span> <span class="ow">or</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_q_out</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_q_custom_diffusion</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">out_bias</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_q_out</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_q_custom_diffusion</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">crossattn</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">crossattn</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
                <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_kv</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_custom_diffusion</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_k_custom_diffusion</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_custom_diffusion</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_v_custom_diffusion</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">crossattn</span><span class="p">:</span>
            <span class="n">detach</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">detach</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">detach</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="mf">0.0</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">detach</span> <span class="o">*</span> <span class="n">key</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">detach</span><span class="p">)</span> <span class="o">*</span> <span class="n">key</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">detach</span> <span class="o">*</span> <span class="n">value</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">detach</span><span class="p">)</span> <span class="o">*</span> <span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_q_out</span><span class="p">:</span>
            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_out_custom_diffusion</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2111</span>
<span class="normal">2112</span>
<span class="normal">2113</span>
<span class="normal">2114</span>
<span class="normal">2115</span>
<span class="normal">2116</span>
<span class="normal">2117</span>
<span class="normal">2118</span>
<span class="normal">2119</span>
<span class="normal">2120</span>
<span class="normal">2121</span>
<span class="normal">2122</span>
<span class="normal">2123</span>
<span class="normal">2124</span>
<span class="normal">2125</span>
<span class="normal">2126</span>
<span class="normal">2127</span>
<span class="normal">2128</span>
<span class="normal">2129</span>
<span class="normal">2130</span>
<span class="normal">2131</span>
<span class="normal">2132</span>
<span class="normal">2133</span>
<span class="normal">2134</span>
<span class="normal">2135</span>
<span class="normal">2136</span>
<span class="normal">2137</span>
<span class="normal">2138</span>
<span class="normal">2139</span>
<span class="normal">2140</span>
<span class="normal">2141</span>
<span class="normal">2142</span>
<span class="normal">2143</span>
<span class="normal">2144</span>
<span class="normal">2145</span>
<span class="normal">2146</span>
<span class="normal">2147</span>
<span class="normal">2148</span>
<span class="normal">2149</span>
<span class="normal">2150</span>
<span class="normal">2151</span>
<span class="normal">2152</span>
<span class="normal">2153</span>
<span class="normal">2154</span>
<span class="normal">2155</span>
<span class="normal">2156</span>
<span class="normal">2157</span>
<span class="normal">2158</span>
<span class="normal">2159</span>
<span class="normal">2160</span>
<span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span>
<span class="normal">2198</span>
<span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FluxAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the attention in FluxSingleTransformerBlock does not use `encoder_hidden_states`</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># `context` projections.</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="c1"># attention</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            encoder_hidden_states, hidden_states = (</span>
<span class="sd">                hidden_states[:, : encoder_hidden_states.shape[1]],</span>
<span class="sd">                hidden_states[:, encoder_hidden_states.shape[1] :],</span>
<span class="sd">            )</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="p">[</span><span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FusedFluxAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FusedFluxAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FusedFluxAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span>
<span class="normal">2276</span>
<span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedFluxAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the attention in FluxSingleTransformerBlock does not use `encoder_hidden_states`</span>
        <span class="c1"># `context` projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_added_qkv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">split_size</span> <span class="o">=</span> <span class="n">encoder_qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
            <span class="p">(</span>
                <span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span>
                <span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">encoder_qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="c1"># attention</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span> <span class="n">query</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
            <span class="p">)</span>

            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FluxSingleAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FluxSingleAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FluxSingleAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0" href="#mindone.diffusers.models.attention_processor.FluxAttnProcessor2_0">FluxAttnProcessor2_0</a></code></p>


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4444</span>
<span class="normal">4445</span>
<span class="normal">4446</span>
<span class="normal">4447</span>
<span class="normal">4448</span>
<span class="normal">4449</span>
<span class="normal">4450</span>
<span class="normal">4451</span>
<span class="normal">4452</span>
<span class="normal">4453</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FluxSingleAttnProcessor2_0</span><span class="p">(</span><span class="n">FluxAttnProcessor2_0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">deprecation_message</span> <span class="o">=</span> <span class="s2">&quot;`FluxSingleAttnProcessor2_0` is deprecated and will be removed in a future version. Please use `FluxAttnProcessor2_0` instead.&quot;</span>
        <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;FluxSingleAttnProcessor2_0&quot;</span><span class="p">,</span> <span class="s2">&quot;0.32.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.HunyuanAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.HunyuanAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.HunyuanAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the HunyuanDiT model. It applies a s normalization layer and rotary embedding on query and key vector.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2885</span>
<span class="normal">2886</span>
<span class="normal">2887</span>
<span class="normal">2888</span>
<span class="normal">2889</span>
<span class="normal">2890</span>
<span class="normal">2891</span>
<span class="normal">2892</span>
<span class="normal">2893</span>
<span class="normal">2894</span>
<span class="normal">2895</span>
<span class="normal">2896</span>
<span class="normal">2897</span>
<span class="normal">2898</span>
<span class="normal">2899</span>
<span class="normal">2900</span>
<span class="normal">2901</span>
<span class="normal">2902</span>
<span class="normal">2903</span>
<span class="normal">2904</span>
<span class="normal">2905</span>
<span class="normal">2906</span>
<span class="normal">2907</span>
<span class="normal">2908</span>
<span class="normal">2909</span>
<span class="normal">2910</span>
<span class="normal">2911</span>
<span class="normal">2912</span>
<span class="normal">2913</span>
<span class="normal">2914</span>
<span class="normal">2915</span>
<span class="normal">2916</span>
<span class="normal">2917</span>
<span class="normal">2918</span>
<span class="normal">2919</span>
<span class="normal">2920</span>
<span class="normal">2921</span>
<span class="normal">2922</span>
<span class="normal">2923</span>
<span class="normal">2924</span>
<span class="normal">2925</span>
<span class="normal">2926</span>
<span class="normal">2927</span>
<span class="normal">2928</span>
<span class="normal">2929</span>
<span class="normal">2930</span>
<span class="normal">2931</span>
<span class="normal">2932</span>
<span class="normal">2933</span>
<span class="normal">2934</span>
<span class="normal">2935</span>
<span class="normal">2936</span>
<span class="normal">2937</span>
<span class="normal">2938</span>
<span class="normal">2939</span>
<span class="normal">2940</span>
<span class="normal">2941</span>
<span class="normal">2942</span>
<span class="normal">2943</span>
<span class="normal">2944</span>
<span class="normal">2945</span>
<span class="normal">2946</span>
<span class="normal">2947</span>
<span class="normal">2948</span>
<span class="normal">2949</span>
<span class="normal">2950</span>
<span class="normal">2951</span>
<span class="normal">2952</span>
<span class="normal">2953</span>
<span class="normal">2954</span>
<span class="normal">2955</span>
<span class="normal">2956</span>
<span class="normal">2957</span>
<span class="normal">2958</span>
<span class="normal">2959</span>
<span class="normal">2960</span>
<span class="normal">2961</span>
<span class="normal">2962</span>
<span class="normal">2963</span>
<span class="normal">2964</span>
<span class="normal">2965</span>
<span class="normal">2966</span>
<span class="normal">2967</span>
<span class="normal">2968</span>
<span class="normal">2969</span>
<span class="normal">2970</span>
<span class="normal">2971</span>
<span class="normal">2972</span>
<span class="normal">2973</span>
<span class="normal">2974</span>
<span class="normal">2975</span>
<span class="normal">2976</span>
<span class="normal">2977</span>
<span class="normal">2978</span>
<span class="normal">2979</span>
<span class="normal">2980</span>
<span class="normal">2981</span>
<span class="normal">2982</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">HunyuanAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the HunyuanDiT model. It applies a s normalization layer and rotary embedding on query and key vector.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="c1"># # the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># # TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGHunyuanAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGHunyuanAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGHunyuanAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This
variant of the processor employs <a href="https://arxiv.org/abs/2403.17377">Pertubed Attention Guidance</a>.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3743</span>
<span class="normal">3744</span>
<span class="normal">3745</span>
<span class="normal">3746</span>
<span class="normal">3747</span>
<span class="normal">3748</span>
<span class="normal">3749</span>
<span class="normal">3750</span>
<span class="normal">3751</span>
<span class="normal">3752</span>
<span class="normal">3753</span>
<span class="normal">3754</span>
<span class="normal">3755</span>
<span class="normal">3756</span>
<span class="normal">3757</span>
<span class="normal">3758</span>
<span class="normal">3759</span>
<span class="normal">3760</span>
<span class="normal">3761</span>
<span class="normal">3762</span>
<span class="normal">3763</span>
<span class="normal">3764</span>
<span class="normal">3765</span>
<span class="normal">3766</span>
<span class="normal">3767</span>
<span class="normal">3768</span>
<span class="normal">3769</span>
<span class="normal">3770</span>
<span class="normal">3771</span>
<span class="normal">3772</span>
<span class="normal">3773</span>
<span class="normal">3774</span>
<span class="normal">3775</span>
<span class="normal">3776</span>
<span class="normal">3777</span>
<span class="normal">3778</span>
<span class="normal">3779</span>
<span class="normal">3780</span>
<span class="normal">3781</span>
<span class="normal">3782</span>
<span class="normal">3783</span>
<span class="normal">3784</span>
<span class="normal">3785</span>
<span class="normal">3786</span>
<span class="normal">3787</span>
<span class="normal">3788</span>
<span class="normal">3789</span>
<span class="normal">3790</span>
<span class="normal">3791</span>
<span class="normal">3792</span>
<span class="normal">3793</span>
<span class="normal">3794</span>
<span class="normal">3795</span>
<span class="normal">3796</span>
<span class="normal">3797</span>
<span class="normal">3798</span>
<span class="normal">3799</span>
<span class="normal">3800</span>
<span class="normal">3801</span>
<span class="normal">3802</span>
<span class="normal">3803</span>
<span class="normal">3804</span>
<span class="normal">3805</span>
<span class="normal">3806</span>
<span class="normal">3807</span>
<span class="normal">3808</span>
<span class="normal">3809</span>
<span class="normal">3810</span>
<span class="normal">3811</span>
<span class="normal">3812</span>
<span class="normal">3813</span>
<span class="normal">3814</span>
<span class="normal">3815</span>
<span class="normal">3816</span>
<span class="normal">3817</span>
<span class="normal">3818</span>
<span class="normal">3819</span>
<span class="normal">3820</span>
<span class="normal">3821</span>
<span class="normal">3822</span>
<span class="normal">3823</span>
<span class="normal">3824</span>
<span class="normal">3825</span>
<span class="normal">3826</span>
<span class="normal">3827</span>
<span class="normal">3828</span>
<span class="normal">3829</span>
<span class="normal">3830</span>
<span class="normal">3831</span>
<span class="normal">3832</span>
<span class="normal">3833</span>
<span class="normal">3834</span>
<span class="normal">3835</span>
<span class="normal">3836</span>
<span class="normal">3837</span>
<span class="normal">3838</span>
<span class="normal">3839</span>
<span class="normal">3840</span>
<span class="normal">3841</span>
<span class="normal">3842</span>
<span class="normal">3843</span>
<span class="normal">3844</span>
<span class="normal">3845</span>
<span class="normal">3846</span>
<span class="normal">3847</span>
<span class="normal">3848</span>
<span class="normal">3849</span>
<span class="normal">3850</span>
<span class="normal">3851</span>
<span class="normal">3852</span>
<span class="normal">3853</span>
<span class="normal">3854</span>
<span class="normal">3855</span>
<span class="normal">3856</span>
<span class="normal">3857</span>
<span class="normal">3858</span>
<span class="normal">3859</span>
<span class="normal">3860</span>
<span class="normal">3861</span>
<span class="normal">3862</span>
<span class="normal">3863</span>
<span class="normal">3864</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGHunyuanAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This</span>
<span class="sd">    variant of the processor employs [Pertubed Attention Guidance](https://arxiv.org/abs/2403.17377).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># 1. Original Path</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states_org</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># 2. Perturbed Path</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># cat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGCFGHunyuanAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGCFGHunyuanAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGCFGHunyuanAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This
variant of the processor employs <a href="https://arxiv.org/abs/2403.17377">Pertubed Attention Guidance</a>.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3867</span>
<span class="normal">3868</span>
<span class="normal">3869</span>
<span class="normal">3870</span>
<span class="normal">3871</span>
<span class="normal">3872</span>
<span class="normal">3873</span>
<span class="normal">3874</span>
<span class="normal">3875</span>
<span class="normal">3876</span>
<span class="normal">3877</span>
<span class="normal">3878</span>
<span class="normal">3879</span>
<span class="normal">3880</span>
<span class="normal">3881</span>
<span class="normal">3882</span>
<span class="normal">3883</span>
<span class="normal">3884</span>
<span class="normal">3885</span>
<span class="normal">3886</span>
<span class="normal">3887</span>
<span class="normal">3888</span>
<span class="normal">3889</span>
<span class="normal">3890</span>
<span class="normal">3891</span>
<span class="normal">3892</span>
<span class="normal">3893</span>
<span class="normal">3894</span>
<span class="normal">3895</span>
<span class="normal">3896</span>
<span class="normal">3897</span>
<span class="normal">3898</span>
<span class="normal">3899</span>
<span class="normal">3900</span>
<span class="normal">3901</span>
<span class="normal">3902</span>
<span class="normal">3903</span>
<span class="normal">3904</span>
<span class="normal">3905</span>
<span class="normal">3906</span>
<span class="normal">3907</span>
<span class="normal">3908</span>
<span class="normal">3909</span>
<span class="normal">3910</span>
<span class="normal">3911</span>
<span class="normal">3912</span>
<span class="normal">3913</span>
<span class="normal">3914</span>
<span class="normal">3915</span>
<span class="normal">3916</span>
<span class="normal">3917</span>
<span class="normal">3918</span>
<span class="normal">3919</span>
<span class="normal">3920</span>
<span class="normal">3921</span>
<span class="normal">3922</span>
<span class="normal">3923</span>
<span class="normal">3924</span>
<span class="normal">3925</span>
<span class="normal">3926</span>
<span class="normal">3927</span>
<span class="normal">3928</span>
<span class="normal">3929</span>
<span class="normal">3930</span>
<span class="normal">3931</span>
<span class="normal">3932</span>
<span class="normal">3933</span>
<span class="normal">3934</span>
<span class="normal">3935</span>
<span class="normal">3936</span>
<span class="normal">3937</span>
<span class="normal">3938</span>
<span class="normal">3939</span>
<span class="normal">3940</span>
<span class="normal">3941</span>
<span class="normal">3942</span>
<span class="normal">3943</span>
<span class="normal">3944</span>
<span class="normal">3945</span>
<span class="normal">3946</span>
<span class="normal">3947</span>
<span class="normal">3948</span>
<span class="normal">3949</span>
<span class="normal">3950</span>
<span class="normal">3951</span>
<span class="normal">3952</span>
<span class="normal">3953</span>
<span class="normal">3954</span>
<span class="normal">3955</span>
<span class="normal">3956</span>
<span class="normal">3957</span>
<span class="normal">3958</span>
<span class="normal">3959</span>
<span class="normal">3960</span>
<span class="normal">3961</span>
<span class="normal">3962</span>
<span class="normal">3963</span>
<span class="normal">3964</span>
<span class="normal">3965</span>
<span class="normal">3966</span>
<span class="normal">3967</span>
<span class="normal">3968</span>
<span class="normal">3969</span>
<span class="normal">3970</span>
<span class="normal">3971</span>
<span class="normal">3972</span>
<span class="normal">3973</span>
<span class="normal">3974</span>
<span class="normal">3975</span>
<span class="normal">3976</span>
<span class="normal">3977</span>
<span class="normal">3978</span>
<span class="normal">3979</span>
<span class="normal">3980</span>
<span class="normal">3981</span>
<span class="normal">3982</span>
<span class="normal">3983</span>
<span class="normal">3984</span>
<span class="normal">3985</span>
<span class="normal">3986</span>
<span class="normal">3987</span>
<span class="normal">3988</span>
<span class="normal">3989</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGCFGHunyuanAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This</span>
<span class="sd">    variant of the processor employs [Pertubed Attention Guidance](https://arxiv.org/abs/2403.17377).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">])</span>

        <span class="c1"># 1. Original Path</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states_org</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># 2. Perturbed Path</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># cat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGIdentitySelfAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGIdentitySelfAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGIdentitySelfAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing PAG using scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
PAG reference: https://arxiv.org/abs/2403.17377</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4188</span>
<span class="normal">4189</span>
<span class="normal">4190</span>
<span class="normal">4191</span>
<span class="normal">4192</span>
<span class="normal">4193</span>
<span class="normal">4194</span>
<span class="normal">4195</span>
<span class="normal">4196</span>
<span class="normal">4197</span>
<span class="normal">4198</span>
<span class="normal">4199</span>
<span class="normal">4200</span>
<span class="normal">4201</span>
<span class="normal">4202</span>
<span class="normal">4203</span>
<span class="normal">4204</span>
<span class="normal">4205</span>
<span class="normal">4206</span>
<span class="normal">4207</span>
<span class="normal">4208</span>
<span class="normal">4209</span>
<span class="normal">4210</span>
<span class="normal">4211</span>
<span class="normal">4212</span>
<span class="normal">4213</span>
<span class="normal">4214</span>
<span class="normal">4215</span>
<span class="normal">4216</span>
<span class="normal">4217</span>
<span class="normal">4218</span>
<span class="normal">4219</span>
<span class="normal">4220</span>
<span class="normal">4221</span>
<span class="normal">4222</span>
<span class="normal">4223</span>
<span class="normal">4224</span>
<span class="normal">4225</span>
<span class="normal">4226</span>
<span class="normal">4227</span>
<span class="normal">4228</span>
<span class="normal">4229</span>
<span class="normal">4230</span>
<span class="normal">4231</span>
<span class="normal">4232</span>
<span class="normal">4233</span>
<span class="normal">4234</span>
<span class="normal">4235</span>
<span class="normal">4236</span>
<span class="normal">4237</span>
<span class="normal">4238</span>
<span class="normal">4239</span>
<span class="normal">4240</span>
<span class="normal">4241</span>
<span class="normal">4242</span>
<span class="normal">4243</span>
<span class="normal">4244</span>
<span class="normal">4245</span>
<span class="normal">4246</span>
<span class="normal">4247</span>
<span class="normal">4248</span>
<span class="normal">4249</span>
<span class="normal">4250</span>
<span class="normal">4251</span>
<span class="normal">4252</span>
<span class="normal">4253</span>
<span class="normal">4254</span>
<span class="normal">4255</span>
<span class="normal">4256</span>
<span class="normal">4257</span>
<span class="normal">4258</span>
<span class="normal">4259</span>
<span class="normal">4260</span>
<span class="normal">4261</span>
<span class="normal">4262</span>
<span class="normal">4263</span>
<span class="normal">4264</span>
<span class="normal">4265</span>
<span class="normal">4266</span>
<span class="normal">4267</span>
<span class="normal">4268</span>
<span class="normal">4269</span>
<span class="normal">4270</span>
<span class="normal">4271</span>
<span class="normal">4272</span>
<span class="normal">4273</span>
<span class="normal">4274</span>
<span class="normal">4275</span>
<span class="normal">4276</span>
<span class="normal">4277</span>
<span class="normal">4278</span>
<span class="normal">4279</span>
<span class="normal">4280</span>
<span class="normal">4281</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGIdentitySelfAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing PAG using scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0).</span>
<span class="sd">    PAG reference: https://arxiv.org/abs/2403.17377</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># original path</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># perturbed path (identity attention)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># cat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGCFGIdentitySelfAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGCFGIdentitySelfAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGCFGIdentitySelfAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing PAG using scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
PAG reference: https://arxiv.org/abs/2403.17377</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4284</span>
<span class="normal">4285</span>
<span class="normal">4286</span>
<span class="normal">4287</span>
<span class="normal">4288</span>
<span class="normal">4289</span>
<span class="normal">4290</span>
<span class="normal">4291</span>
<span class="normal">4292</span>
<span class="normal">4293</span>
<span class="normal">4294</span>
<span class="normal">4295</span>
<span class="normal">4296</span>
<span class="normal">4297</span>
<span class="normal">4298</span>
<span class="normal">4299</span>
<span class="normal">4300</span>
<span class="normal">4301</span>
<span class="normal">4302</span>
<span class="normal">4303</span>
<span class="normal">4304</span>
<span class="normal">4305</span>
<span class="normal">4306</span>
<span class="normal">4307</span>
<span class="normal">4308</span>
<span class="normal">4309</span>
<span class="normal">4310</span>
<span class="normal">4311</span>
<span class="normal">4312</span>
<span class="normal">4313</span>
<span class="normal">4314</span>
<span class="normal">4315</span>
<span class="normal">4316</span>
<span class="normal">4317</span>
<span class="normal">4318</span>
<span class="normal">4319</span>
<span class="normal">4320</span>
<span class="normal">4321</span>
<span class="normal">4322</span>
<span class="normal">4323</span>
<span class="normal">4324</span>
<span class="normal">4325</span>
<span class="normal">4326</span>
<span class="normal">4327</span>
<span class="normal">4328</span>
<span class="normal">4329</span>
<span class="normal">4330</span>
<span class="normal">4331</span>
<span class="normal">4332</span>
<span class="normal">4333</span>
<span class="normal">4334</span>
<span class="normal">4335</span>
<span class="normal">4336</span>
<span class="normal">4337</span>
<span class="normal">4338</span>
<span class="normal">4339</span>
<span class="normal">4340</span>
<span class="normal">4341</span>
<span class="normal">4342</span>
<span class="normal">4343</span>
<span class="normal">4344</span>
<span class="normal">4345</span>
<span class="normal">4346</span>
<span class="normal">4347</span>
<span class="normal">4348</span>
<span class="normal">4349</span>
<span class="normal">4350</span>
<span class="normal">4351</span>
<span class="normal">4352</span>
<span class="normal">4353</span>
<span class="normal">4354</span>
<span class="normal">4355</span>
<span class="normal">4356</span>
<span class="normal">4357</span>
<span class="normal">4358</span>
<span class="normal">4359</span>
<span class="normal">4360</span>
<span class="normal">4361</span>
<span class="normal">4362</span>
<span class="normal">4363</span>
<span class="normal">4364</span>
<span class="normal">4365</span>
<span class="normal">4366</span>
<span class="normal">4367</span>
<span class="normal">4368</span>
<span class="normal">4369</span>
<span class="normal">4370</span>
<span class="normal">4371</span>
<span class="normal">4372</span>
<span class="normal">4373</span>
<span class="normal">4374</span>
<span class="normal">4375</span>
<span class="normal">4376</span>
<span class="normal">4377</span>
<span class="normal">4378</span>
<span class="normal">4379</span>
<span class="normal">4380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGCFGIdentitySelfAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing PAG using scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0).</span>
<span class="sd">    PAG reference: https://arxiv.org/abs/2403.17377</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">])</span>

        <span class="c1"># original path</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># perturbed path (identity attention)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="c1"># cat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor</code>


<a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindspore.nn.Cell">Cell</span></code></p>


        <p>Attention processor for Multiple IP-Adapters.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden size of the attention layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of channels in the <code>encoder_hidden_states</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The context length of the image features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(4,)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>the weight scale of image prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float` or List[`float`], defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span>
<span class="normal">3192</span>
<span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span>
<span class="normal">3261</span>
<span class="normal">3262</span>
<span class="normal">3263</span>
<span class="normal">3264</span>
<span class="normal">3265</span>
<span class="normal">3266</span>
<span class="normal">3267</span>
<span class="normal">3268</span>
<span class="normal">3269</span>
<span class="normal">3270</span>
<span class="normal">3271</span>
<span class="normal">3272</span>
<span class="normal">3273</span>
<span class="normal">3274</span>
<span class="normal">3275</span>
<span class="normal">3276</span>
<span class="normal">3277</span>
<span class="normal">3278</span>
<span class="normal">3279</span>
<span class="normal">3280</span>
<span class="normal">3281</span>
<span class="normal">3282</span>
<span class="normal">3283</span>
<span class="normal">3284</span>
<span class="normal">3285</span>
<span class="normal">3286</span>
<span class="normal">3287</span>
<span class="normal">3288</span>
<span class="normal">3289</span>
<span class="normal">3290</span>
<span class="normal">3291</span>
<span class="normal">3292</span>
<span class="normal">3293</span>
<span class="normal">3294</span>
<span class="normal">3295</span>
<span class="normal">3296</span>
<span class="normal">3297</span>
<span class="normal">3298</span>
<span class="normal">3299</span>
<span class="normal">3300</span>
<span class="normal">3301</span>
<span class="normal">3302</span>
<span class="normal">3303</span>
<span class="normal">3304</span>
<span class="normal">3305</span>
<span class="normal">3306</span>
<span class="normal">3307</span>
<span class="normal">3308</span>
<span class="normal">3309</span>
<span class="normal">3310</span>
<span class="normal">3311</span>
<span class="normal">3312</span>
<span class="normal">3313</span>
<span class="normal">3314</span>
<span class="normal">3315</span>
<span class="normal">3316</span>
<span class="normal">3317</span>
<span class="normal">3318</span>
<span class="normal">3319</span>
<span class="normal">3320</span>
<span class="normal">3321</span>
<span class="normal">3322</span>
<span class="normal">3323</span>
<span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">IPAdapterAttnProcessor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention processor for Multiple IP-Adapters.</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (`int`):</span>
<span class="sd">            The hidden size of the attention layer.</span>
<span class="sd">        cross_attention_dim (`int`):</span>
<span class="sd">            The number of channels in the `encoder_hidden_states`.</span>
<span class="sd">        num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):</span>
<span class="sd">            The context length of the image features.</span>
<span class="sd">        scale (`float` or List[`float`], defaults to 1.0):</span>
<span class="sd">            the weight scale of image prompt.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">cross_attention_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_dim</span> <span class="o">=</span> <span class="n">cross_attention_dim</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">num_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_tokens</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="n">num_tokens</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`scale` should be a list of integers with the same length as `num_tokens`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">ip_adapter_masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># separate ip_hidden_states from encoder_hidden_states</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">end_pos</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">encoder_hidden_states</span><span class="p">[:,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">,</span> <span class="p">:],</span>
                    <span class="p">[</span><span class="n">encoder_hidden_states</span><span class="p">[:,</span> <span class="n">end_pos</span><span class="p">:,</span> <span class="p">:]],</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ip_adapter_masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
                <span class="c1"># for backward compatibility, we accept `ip_adapter_mask` as a tensor of shape [num_ip_adapter, 1, height, width]</span>
                <span class="n">ip_adapter_masks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Length of ip_adapter_masks array (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">)</span><span class="si">}</span><span class="s2">) must match &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;length of self.scale array (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span><span class="si">}</span><span class="s2">) and number of ip_hidden_states &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">ip_state</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">ip_hidden_states</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Each element of the ip_adapter_masks array should be a tensor with shape &quot;</span>
                            <span class="s2">&quot;[1, num_images_for_ip_adapter, height, width].&quot;</span>
                            <span class="s2">&quot; Please use `IPAdapterMaskProcessor` to preprocess your mask&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ip_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Number of masks (</span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) does not match &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;number of ip images (</span><span class="si">{</span><span class="n">ip_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) at index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Number of masks (</span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) does not match &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;number of scales (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span><span class="si">}</span><span class="s2">) at index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ip_adapter_masks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>

        <span class="c1"># for ip-adapter</span>
        <span class="k">for</span> <span class="n">current_ip_hidden_states</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">to_k_ip</span><span class="p">,</span> <span class="n">to_v_ip</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">ip_hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span><span class="p">,</span> <span class="n">ip_adapter_masks</span>
        <span class="p">):</span>
            <span class="n">skip</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">s</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scale</span><span class="p">):</span>
                    <span class="n">skip</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">scale</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">skip</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">skip</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                        <span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                    <span class="n">current_num_images</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">current_num_images</span><span class="p">):</span>
                        <span class="n">ip_key</span> <span class="o">=</span> <span class="n">to_k_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
                        <span class="n">ip_value</span> <span class="o">=</span> <span class="n">to_v_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>

                        <span class="n">ip_key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">ip_key</span><span class="p">)</span>
                        <span class="n">ip_value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">ip_value</span><span class="p">)</span>

                        <span class="n">ip_attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="n">_current_ip_hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">ip_attention_probs</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">)</span>
                        <span class="n">_current_ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">_current_ip_hidden_states</span><span class="p">)</span>

                        <span class="n">mask_downsample</span> <span class="o">=</span> <span class="n">IPAdapterMaskProcessor</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span>
                            <span class="n">mask</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
                            <span class="n">batch_size</span><span class="p">,</span>
                            <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                        <span class="p">)</span>

                        <span class="n">mask_downsample</span> <span class="o">=</span> <span class="n">mask_downsample</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">scale</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">_current_ip_hidden_states</span> <span class="o">*</span> <span class="n">mask_downsample</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ip_key</span> <span class="o">=</span> <span class="n">to_k_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>
                    <span class="n">ip_value</span> <span class="o">=</span> <span class="n">to_v_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>

                    <span class="n">ip_key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">ip_key</span><span class="p">)</span>
                    <span class="n">ip_value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">ip_value</span><span class="p">)</span>

                    <span class="n">ip_attention_probs</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">get_attention_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">ip_attention_probs</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">)</span>
                    <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>

                    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">current_ip_hidden_states</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.IPAdapterAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindspore.nn.Cell">Cell</span></code></p>


        <p>Attention processor for IP-Adapter for PyTorch 2.0.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The hidden size of the attention layer.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cross_attention_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of channels in the <code>encoder_hidden_states</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_tokens</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The context length of the image features.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>(4,)</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>the weight scale of image prompt.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float` or `List[float]`, defaults to 1.0</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1.0</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span>
<span class="normal">3355</span>
<span class="normal">3356</span>
<span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span>
<span class="normal">3508</span>
<span class="normal">3509</span>
<span class="normal">3510</span>
<span class="normal">3511</span>
<span class="normal">3512</span>
<span class="normal">3513</span>
<span class="normal">3514</span>
<span class="normal">3515</span>
<span class="normal">3516</span>
<span class="normal">3517</span>
<span class="normal">3518</span>
<span class="normal">3519</span>
<span class="normal">3520</span>
<span class="normal">3521</span>
<span class="normal">3522</span>
<span class="normal">3523</span>
<span class="normal">3524</span>
<span class="normal">3525</span>
<span class="normal">3526</span>
<span class="normal">3527</span>
<span class="normal">3528</span>
<span class="normal">3529</span>
<span class="normal">3530</span>
<span class="normal">3531</span>
<span class="normal">3532</span>
<span class="normal">3533</span>
<span class="normal">3534</span>
<span class="normal">3535</span>
<span class="normal">3536</span>
<span class="normal">3537</span>
<span class="normal">3538</span>
<span class="normal">3539</span>
<span class="normal">3540</span>
<span class="normal">3541</span>
<span class="normal">3542</span>
<span class="normal">3543</span>
<span class="normal">3544</span>
<span class="normal">3545</span>
<span class="normal">3546</span>
<span class="normal">3547</span>
<span class="normal">3548</span>
<span class="normal">3549</span>
<span class="normal">3550</span>
<span class="normal">3551</span>
<span class="normal">3552</span>
<span class="normal">3553</span>
<span class="normal">3554</span>
<span class="normal">3555</span>
<span class="normal">3556</span>
<span class="normal">3557</span>
<span class="normal">3558</span>
<span class="normal">3559</span>
<span class="normal">3560</span>
<span class="normal">3561</span>
<span class="normal">3562</span>
<span class="normal">3563</span>
<span class="normal">3564</span>
<span class="normal">3565</span>
<span class="normal">3566</span>
<span class="normal">3567</span>
<span class="normal">3568</span>
<span class="normal">3569</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">IPAdapterAttnProcessor2_0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention processor for IP-Adapter for PyTorch 2.0.</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (`int`):</span>
<span class="sd">            The hidden size of the attention layer.</span>
<span class="sd">        cross_attention_dim (`int`):</span>
<span class="sd">            The number of channels in the `encoder_hidden_states`.</span>
<span class="sd">        num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):</span>
<span class="sd">            The context length of the image features.</span>
<span class="sd">        scale (`float` or `List[float]`, defaults to 1.0):</span>
<span class="sd">            the weight scale of image prompt.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">cross_attention_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_dim</span> <span class="o">=</span> <span class="n">cross_attention_dim</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">num_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_tokens</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="n">num_tokens</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`scale` should be a list of integers with the same length as `num_tokens`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">ip_adapter_masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="c1"># separate ip_hidden_states from encoder_hidden_states</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">deprecation_message</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="s2">&quot;You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release.&quot;</span>
                    <span class="s2">&quot; Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning.&quot;</span>
                <span class="p">)</span>
                <span class="n">deprecate</span><span class="p">(</span><span class="s2">&quot;encoder_hidden_states not a tuple&quot;</span><span class="p">,</span> <span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span> <span class="n">deprecation_message</span><span class="p">,</span> <span class="n">standard_warn</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">end_pos</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">encoder_hidden_states</span><span class="p">[:,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">,</span> <span class="p">:],</span>
                    <span class="p">[</span><span class="n">encoder_hidden_states</span><span class="p">[:,</span> <span class="n">end_pos</span><span class="p">:,</span> <span class="p">:]],</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ip_adapter_masks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">,</span> <span class="n">List</span><span class="p">):</span>
                <span class="c1"># for backward compatibility, we accept `ip_adapter_mask` as a tensor of shape [num_ip_adapter, 1, height, width]</span>
                <span class="n">ip_adapter_masks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Length of ip_adapter_masks array (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">)</span><span class="si">}</span><span class="s2">) must match &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;length of self.scale array (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span><span class="si">}</span><span class="s2">) and number of ip_hidden_states &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">ip_state</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">ip_adapter_masks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">ip_hidden_states</span><span class="p">)):</span>
                    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Each element of the ip_adapter_masks array should be a tensor with shape &quot;</span>
                            <span class="s2">&quot;[1, num_images_for_ip_adapter, height, width].&quot;</span>
                            <span class="s2">&quot; Please use `IPAdapterMaskProcessor` to preprocess your mask&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ip_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Number of masks (</span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) does not match &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;number of ip images (</span><span class="si">{</span><span class="n">ip_state</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) at index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Number of masks (</span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">) does not match &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;number of scales (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span><span class="si">}</span><span class="s2">) at index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ip_adapter_masks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>

        <span class="c1"># for ip-adapter</span>
        <span class="k">for</span> <span class="n">current_ip_hidden_states</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">to_k_ip</span><span class="p">,</span> <span class="n">to_v_ip</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">ip_hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span><span class="p">,</span> <span class="n">ip_adapter_masks</span>
        <span class="p">):</span>
            <span class="n">skip</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">s</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scale</span><span class="p">):</span>
                    <span class="n">skip</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="n">scale</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">skip</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">skip</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                        <span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                    <span class="n">current_num_images</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">current_num_images</span><span class="p">):</span>
                        <span class="n">ip_key</span> <span class="o">=</span> <span class="n">to_k_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
                        <span class="n">ip_value</span> <span class="o">=</span> <span class="n">to_v_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>

                        <span class="n">ip_key</span> <span class="o">=</span> <span class="n">ip_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                        <span class="n">ip_value</span> <span class="o">=</span> <span class="n">ip_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

                        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
                        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
                        <span class="n">_current_ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
                            <span class="n">query</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
                        <span class="p">)</span>

                        <span class="n">_current_ip_hidden_states</span> <span class="o">=</span> <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span>
                        <span class="p">)</span>
                        <span class="n">_current_ip_hidden_states</span> <span class="o">=</span> <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                        <span class="n">mask_downsample</span> <span class="o">=</span> <span class="n">IPAdapterMaskProcessor</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span>
                            <span class="n">mask</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
                            <span class="n">batch_size</span><span class="p">,</span>
                            <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">_current_ip_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                        <span class="p">)</span>

                        <span class="n">mask_downsample</span> <span class="o">=</span> <span class="n">mask_downsample</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">scale</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">_current_ip_hidden_states</span> <span class="o">*</span> <span class="n">mask_downsample</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ip_key</span> <span class="o">=</span> <span class="n">to_k_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>
                    <span class="n">ip_value</span> <span class="o">=</span> <span class="n">to_v_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>

                    <span class="n">ip_key</span> <span class="o">=</span> <span class="n">ip_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                    <span class="n">ip_value</span> <span class="o">=</span> <span class="n">ip_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

                    <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
                    <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
                    <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
                        <span class="n">query</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
                    <span class="p">)</span>

                    <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">current_ip_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                        <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span>
                    <span class="p">)</span>
                    <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">current_ip_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

                    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">current_ip_hidden_states</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindspore.nn.Cell">Cell</span></code></p>


        <p>Attention processor for IP-Adapter used typically in processing the SD3-like self-attention projections, with
additional image-based information and timestep embeddings.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hidden_size</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of hidden channels.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ip_hidden_states_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The image feature dimension.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of head channels.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>timesteps_emb_dim</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of input channels for timestep embedding.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`int`, defaults to 1280</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>1280</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>scale</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>IP-Adapter scale.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`float`, defaults to 0.5</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>0.5</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3572</span>
<span class="normal">3573</span>
<span class="normal">3574</span>
<span class="normal">3575</span>
<span class="normal">3576</span>
<span class="normal">3577</span>
<span class="normal">3578</span>
<span class="normal">3579</span>
<span class="normal">3580</span>
<span class="normal">3581</span>
<span class="normal">3582</span>
<span class="normal">3583</span>
<span class="normal">3584</span>
<span class="normal">3585</span>
<span class="normal">3586</span>
<span class="normal">3587</span>
<span class="normal">3588</span>
<span class="normal">3589</span>
<span class="normal">3590</span>
<span class="normal">3591</span>
<span class="normal">3592</span>
<span class="normal">3593</span>
<span class="normal">3594</span>
<span class="normal">3595</span>
<span class="normal">3596</span>
<span class="normal">3597</span>
<span class="normal">3598</span>
<span class="normal">3599</span>
<span class="normal">3600</span>
<span class="normal">3601</span>
<span class="normal">3602</span>
<span class="normal">3603</span>
<span class="normal">3604</span>
<span class="normal">3605</span>
<span class="normal">3606</span>
<span class="normal">3607</span>
<span class="normal">3608</span>
<span class="normal">3609</span>
<span class="normal">3610</span>
<span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span>
<span class="normal">3689</span>
<span class="normal">3690</span>
<span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SD3IPAdapterJointAttnProcessor2_0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention processor for IP-Adapter used typically in processing the SD3-like self-attention projections, with</span>
<span class="sd">    additional image-based information and timestep embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (`int`):</span>
<span class="sd">            The number of hidden channels.</span>
<span class="sd">        ip_hidden_states_dim (`int`):</span>
<span class="sd">            The image feature dimension.</span>
<span class="sd">        head_dim (`int`):</span>
<span class="sd">            The number of head channels.</span>
<span class="sd">        timesteps_emb_dim (`int`, defaults to 1280):</span>
<span class="sd">            The number of input channels for timestep embedding.</span>
<span class="sd">        scale (`float`, defaults to 0.5):</span>
<span class="sd">            IP-Adapter scale.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">ip_hidden_states_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">timesteps_emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1280</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># To prevent circular import</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.normalization</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdaLayerNorm</span><span class="p">,</span> <span class="n">RMSNorm</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip</span> <span class="o">=</span> <span class="n">AdaLayerNorm</span><span class="p">(</span><span class="n">timesteps_emb_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">ip_hidden_states_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">norm_eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">chunk_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ip_hidden_states_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ip_hidden_states_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_q</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_k</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip_k</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ip_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform the attention computation, integrating image features (if provided) and timestep embeddings.</span>

<span class="sd">        If `ip_hidden_states` is `None`, this is equivalent to using JointAttnProcessor2_0.</span>

<span class="sd">        Args:</span>
<span class="sd">            attn (`Attention`):</span>
<span class="sd">                Attention instance.</span>
<span class="sd">            hidden_states (`ms.Tensor`):</span>
<span class="sd">                Input `hidden_states`.</span>
<span class="sd">            encoder_hidden_states (`ms.Tensor`, *optional*):</span>
<span class="sd">                The encoder hidden states.</span>
<span class="sd">            attention_mask (`ms.Tensor`, *optional*):</span>
<span class="sd">                Attention mask.</span>
<span class="sd">            ip_hidden_states (`ms.Tensor`, *optional*):</span>
<span class="sd">                Image embeddings.</span>
<span class="sd">            temb (`ms.Tensor`, *optional*):</span>
<span class="sd">                Timestep embeddings.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `ms.Tensor`: Output hidden states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">img_query</span> <span class="o">=</span> <span class="n">query</span>
        <span class="n">img_key</span> <span class="o">=</span> <span class="n">key</span>
        <span class="n">img_value</span> <span class="o">=</span> <span class="n">value</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_hidden_states_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">encoder_hidden_states_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">encoder_hidden_states_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Split the attention outputs.</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
                <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="c1"># IP Adapter</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ip_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Norm image features</span>
            <span class="n">norm_ip_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>

            <span class="c1"># To k and v</span>
            <span class="n">ip_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span><span class="p">(</span><span class="n">norm_ip_hidden_states</span><span class="p">)</span>
            <span class="n">ip_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span><span class="p">(</span><span class="n">norm_ip_hidden_states</span><span class="p">)</span>

            <span class="c1"># Reshape</span>
            <span class="n">ip_key</span> <span class="o">=</span> <span class="n">ip_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">ip_value</span> <span class="o">=</span> <span class="n">ip_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Norm</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">img_query</span><span class="p">)</span>
            <span class="n">img_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">img_key</span><span class="p">)</span>
            <span class="n">ip_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip_k</span><span class="p">(</span><span class="n">ip_key</span><span class="p">)</span>

            <span class="c1"># cat img</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">img_key</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">img_value</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">ip_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
            <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">ip_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">ip_hidden_states</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0.construct" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mindone</span><span class="o">.</span><span class="n">diffusers</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">attention_processor</span><span class="o">.</span><span class="n">SD3IPAdapterJointAttnProcessor2_0</span><span class="o">.</span><span class="n">construct</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ip_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

<a href="#mindone.diffusers.models.attention_processor.SD3IPAdapterJointAttnProcessor2_0.construct" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Perform the attention computation, integrating image features (if provided) and timestep embeddings.</p>
<p>If <code>ip_hidden_states</code> is <code>None</code>, this is equivalent to using JointAttnProcessor2_0.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>attn</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Attention instance.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Attention`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Input <code>hidden_states</code>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`ms.Tensor`</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>encoder_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The encoder hidden states.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`ms.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attention_mask</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Attention mask.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`ms.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ip_hidden_states</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Image embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`ms.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>temb</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Timestep embeddings.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`ms.Tensor`, *optional*</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="mindspore.Tensor">Tensor</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p><code>ms.Tensor</code>: Output hidden states.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


            <details class="quote">
              <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3611</span>
<span class="normal">3612</span>
<span class="normal">3613</span>
<span class="normal">3614</span>
<span class="normal">3615</span>
<span class="normal">3616</span>
<span class="normal">3617</span>
<span class="normal">3618</span>
<span class="normal">3619</span>
<span class="normal">3620</span>
<span class="normal">3621</span>
<span class="normal">3622</span>
<span class="normal">3623</span>
<span class="normal">3624</span>
<span class="normal">3625</span>
<span class="normal">3626</span>
<span class="normal">3627</span>
<span class="normal">3628</span>
<span class="normal">3629</span>
<span class="normal">3630</span>
<span class="normal">3631</span>
<span class="normal">3632</span>
<span class="normal">3633</span>
<span class="normal">3634</span>
<span class="normal">3635</span>
<span class="normal">3636</span>
<span class="normal">3637</span>
<span class="normal">3638</span>
<span class="normal">3639</span>
<span class="normal">3640</span>
<span class="normal">3641</span>
<span class="normal">3642</span>
<span class="normal">3643</span>
<span class="normal">3644</span>
<span class="normal">3645</span>
<span class="normal">3646</span>
<span class="normal">3647</span>
<span class="normal">3648</span>
<span class="normal">3649</span>
<span class="normal">3650</span>
<span class="normal">3651</span>
<span class="normal">3652</span>
<span class="normal">3653</span>
<span class="normal">3654</span>
<span class="normal">3655</span>
<span class="normal">3656</span>
<span class="normal">3657</span>
<span class="normal">3658</span>
<span class="normal">3659</span>
<span class="normal">3660</span>
<span class="normal">3661</span>
<span class="normal">3662</span>
<span class="normal">3663</span>
<span class="normal">3664</span>
<span class="normal">3665</span>
<span class="normal">3666</span>
<span class="normal">3667</span>
<span class="normal">3668</span>
<span class="normal">3669</span>
<span class="normal">3670</span>
<span class="normal">3671</span>
<span class="normal">3672</span>
<span class="normal">3673</span>
<span class="normal">3674</span>
<span class="normal">3675</span>
<span class="normal">3676</span>
<span class="normal">3677</span>
<span class="normal">3678</span>
<span class="normal">3679</span>
<span class="normal">3680</span>
<span class="normal">3681</span>
<span class="normal">3682</span>
<span class="normal">3683</span>
<span class="normal">3684</span>
<span class="normal">3685</span>
<span class="normal">3686</span>
<span class="normal">3687</span>
<span class="normal">3688</span>
<span class="normal">3689</span>
<span class="normal">3690</span>
<span class="normal">3691</span>
<span class="normal">3692</span>
<span class="normal">3693</span>
<span class="normal">3694</span>
<span class="normal">3695</span>
<span class="normal">3696</span>
<span class="normal">3697</span>
<span class="normal">3698</span>
<span class="normal">3699</span>
<span class="normal">3700</span>
<span class="normal">3701</span>
<span class="normal">3702</span>
<span class="normal">3703</span>
<span class="normal">3704</span>
<span class="normal">3705</span>
<span class="normal">3706</span>
<span class="normal">3707</span>
<span class="normal">3708</span>
<span class="normal">3709</span>
<span class="normal">3710</span>
<span class="normal">3711</span>
<span class="normal">3712</span>
<span class="normal">3713</span>
<span class="normal">3714</span>
<span class="normal">3715</span>
<span class="normal">3716</span>
<span class="normal">3717</span>
<span class="normal">3718</span>
<span class="normal">3719</span>
<span class="normal">3720</span>
<span class="normal">3721</span>
<span class="normal">3722</span>
<span class="normal">3723</span>
<span class="normal">3724</span>
<span class="normal">3725</span>
<span class="normal">3726</span>
<span class="normal">3727</span>
<span class="normal">3728</span>
<span class="normal">3729</span>
<span class="normal">3730</span>
<span class="normal">3731</span>
<span class="normal">3732</span>
<span class="normal">3733</span>
<span class="normal">3734</span>
<span class="normal">3735</span>
<span class="normal">3736</span>
<span class="normal">3737</span>
<span class="normal">3738</span>
<span class="normal">3739</span>
<span class="normal">3740</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ip_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temb</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform the attention computation, integrating image features (if provided) and timestep embeddings.</span>

<span class="sd">    If `ip_hidden_states` is `None`, this is equivalent to using JointAttnProcessor2_0.</span>

<span class="sd">    Args:</span>
<span class="sd">        attn (`Attention`):</span>
<span class="sd">            Attention instance.</span>
<span class="sd">        hidden_states (`ms.Tensor`):</span>
<span class="sd">            Input `hidden_states`.</span>
<span class="sd">        encoder_hidden_states (`ms.Tensor`, *optional*):</span>
<span class="sd">            The encoder hidden states.</span>
<span class="sd">        attention_mask (`ms.Tensor`, *optional*):</span>
<span class="sd">            Attention mask.</span>
<span class="sd">        ip_hidden_states (`ms.Tensor`, *optional*):</span>
<span class="sd">            Image embeddings.</span>
<span class="sd">        temb (`ms.Tensor`, *optional*):</span>
<span class="sd">            Timestep embeddings.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `ms.Tensor`: Output hidden states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># `sample` projections.</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">img_query</span> <span class="o">=</span> <span class="n">query</span>
    <span class="n">img_key</span> <span class="o">=</span> <span class="n">key</span>
    <span class="n">img_value</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="c1"># `context` projections.</span>
    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_hidden_states_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">encoder_hidden_states_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">encoder_hidden_states_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Split the attention outputs.</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

    <span class="c1"># IP Adapter</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ip_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Norm image features</span>
        <span class="n">norm_ip_hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip</span><span class="p">(</span><span class="n">ip_hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="o">=</span><span class="n">temb</span><span class="p">)</span>

        <span class="c1"># To k and v</span>
        <span class="n">ip_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span><span class="p">(</span><span class="n">norm_ip_hidden_states</span><span class="p">)</span>
        <span class="n">ip_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span><span class="p">(</span><span class="n">norm_ip_hidden_states</span><span class="p">)</span>

        <span class="c1"># Reshape</span>
        <span class="n">ip_key</span> <span class="o">=</span> <span class="n">ip_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">ip_value</span> <span class="o">=</span> <span class="n">ip_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Norm</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">img_query</span><span class="p">)</span>
        <span class="n">img_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">img_key</span><span class="p">)</span>
        <span class="n">ip_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_ip_k</span><span class="p">(</span><span class="n">ip_key</span><span class="p">)</span>

        <span class="c1"># cat img</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">img_key</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">img_value</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">ip_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">ip_hidden_states</span> <span class="o">=</span> <span class="n">ip_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">ip_hidden_states</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

    <span class="c1"># linear proj</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="c1"># dropout</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.JointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.JointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.JointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">JointAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_hidden_states_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">encoder_hidden_states_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">encoder_hidden_states_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Split the attention outputs.</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
                <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGJointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGJointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGJointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGJointAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">context_input_ndim</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># store the length of image patch sequences to create a mask that prevents interaction between patches</span>
        <span class="c1"># similar to making the self-attention map an identity matrix</span>
        <span class="n">identity_block_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># original path</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_hidden_states_org</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="n">encoder_hidden_states_org_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key_org</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">query_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">key_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">value_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query_org</span><span class="p">,</span> <span class="n">key_org</span><span class="p">,</span> <span class="n">value_org</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_org</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Split the attention outputs.</span>
        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="n">encoder_hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span>
            <span class="p">)</span>

        <span class="c1"># perturbed path</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_hidden_states_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="n">encoder_hidden_states_ptb_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_ptb_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_ptb_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">query_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">key_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">value_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># create a full mask with all entries set to 0</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">query_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">full_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_ptb</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># set the attention value between image patches to -inf</span>
        <span class="n">full_mask</span><span class="p">[:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>

        <span class="c1"># set the diagonal of the attention value between image patches to 0</span>
        <span class="n">full_mask</span><span class="p">[:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_mask</span><span class="p">[</span>
            <span class="p">:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span>
        <span class="p">]</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># expand the mask to match the attention weights shape</span>
        <span class="n">full_mask</span> <span class="o">=</span> <span class="n">full_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch and num_heads dimensions</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query_ptb</span><span class="p">,</span> <span class="n">key_ptb</span><span class="p">,</span> <span class="n">value_ptb</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_ptb</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># split the attention outputs.</span>
        <span class="n">hidden_states_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_ptb</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states_ptb</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="n">encoder_hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span>
            <span class="p">)</span>

        <span class="c1"># concat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGCFGJointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGCFGJointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGCFGJointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGCFGJointAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">context_input_ndim</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">identity_block_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span>
            <span class="mi">1</span>
        <span class="p">]</span>  <span class="c1"># patch embeddings width * height (correspond to self-attention map width or height)</span>

        <span class="c1"># chunk</span>
        <span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">])</span>

        <span class="p">(</span>
            <span class="n">encoder_hidden_states_uncond</span><span class="p">,</span>
            <span class="n">encoder_hidden_states_org</span><span class="p">,</span>
            <span class="n">encoder_hidden_states_ptb</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_uncond</span><span class="p">,</span> <span class="n">encoder_hidden_states_org</span><span class="p">])</span>

        <span class="c1"># original path</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_hidden_states_org</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="n">encoder_hidden_states_org_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_org_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key_org</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query_org</span> <span class="o">=</span> <span class="n">query_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_org</span> <span class="o">=</span> <span class="n">key_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_org</span> <span class="o">=</span> <span class="n">value_org</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query_org</span><span class="p">,</span> <span class="n">key_org</span><span class="p">,</span> <span class="n">value_org</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_org</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Split the attention outputs.</span>
        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states_org</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_org</span> <span class="o">=</span> <span class="n">encoder_hidden_states_org</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span>
            <span class="p">)</span>

        <span class="c1"># perturbed path</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_hidden_states_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="n">encoder_hidden_states_ptb_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_ptb_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>
        <span class="n">encoder_hidden_states_ptb_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query_ptb</span> <span class="o">=</span> <span class="n">query_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_ptb</span> <span class="o">=</span> <span class="n">key_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_ptb</span> <span class="o">=</span> <span class="n">value_ptb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># create a full mask with all entries set to 0</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">query_ptb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">full_mask</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">query_ptb</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># set the attention value between image patches to -inf</span>
        <span class="n">full_mask</span><span class="p">[:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>

        <span class="c1"># set the diagonal of the attention value between image patches to 0</span>
        <span class="n">full_mask</span><span class="p">[:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">full_mask</span><span class="p">[</span>
            <span class="p">:</span><span class="n">identity_block_size</span><span class="p">,</span> <span class="p">:</span><span class="n">identity_block_size</span>
        <span class="p">]</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># expand the mask to match the attention weights shape</span>
        <span class="n">full_mask</span> <span class="o">=</span> <span class="n">full_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch and num_heads dimensions</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query_ptb</span><span class="p">,</span> <span class="n">key_ptb</span><span class="p">,</span> <span class="n">value_ptb</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_ptb</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># split the attention outputs.</span>
        <span class="n">hidden_states_ptb</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states_ptb</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states_ptb</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states_ptb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">encoder_hidden_states_ptb</span> <span class="o">=</span> <span class="n">encoder_hidden_states_ptb</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span>
            <span class="p">)</span>

        <span class="c1"># concat</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_org</span><span class="p">,</span> <span class="n">encoder_hidden_states_ptb</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FusedJointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FusedJointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FusedJointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used typically in processing the SD3-like self-attention projections.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span>
<span class="normal">1776</span>
<span class="normal">1777</span>
<span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FusedJointAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">context_input_ndim</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># `context` projections.</span>
        <span class="n">encoder_qkv</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_added_qkv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="n">encoder_qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="p">(</span>
            <span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span>
            <span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span>
            <span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">encoder_qkv</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># attention</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">,</span> <span class="n">encoder_hidden_states_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">encoder_hidden_states_key_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">encoder_hidden_states_value_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Split the attention outputs.</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">context_pre_only</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context_input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.LuminaAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.LuminaAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.LuminaAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the LuminaNextDiT model. It applies a s normalization layer and rotary embedding on query and key vector.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3992</span>
<span class="normal">3993</span>
<span class="normal">3994</span>
<span class="normal">3995</span>
<span class="normal">3996</span>
<span class="normal">3997</span>
<span class="normal">3998</span>
<span class="normal">3999</span>
<span class="normal">4000</span>
<span class="normal">4001</span>
<span class="normal">4002</span>
<span class="normal">4003</span>
<span class="normal">4004</span>
<span class="normal">4005</span>
<span class="normal">4006</span>
<span class="normal">4007</span>
<span class="normal">4008</span>
<span class="normal">4009</span>
<span class="normal">4010</span>
<span class="normal">4011</span>
<span class="normal">4012</span>
<span class="normal">4013</span>
<span class="normal">4014</span>
<span class="normal">4015</span>
<span class="normal">4016</span>
<span class="normal">4017</span>
<span class="normal">4018</span>
<span class="normal">4019</span>
<span class="normal">4020</span>
<span class="normal">4021</span>
<span class="normal">4022</span>
<span class="normal">4023</span>
<span class="normal">4024</span>
<span class="normal">4025</span>
<span class="normal">4026</span>
<span class="normal">4027</span>
<span class="normal">4028</span>
<span class="normal">4029</span>
<span class="normal">4030</span>
<span class="normal">4031</span>
<span class="normal">4032</span>
<span class="normal">4033</span>
<span class="normal">4034</span>
<span class="normal">4035</span>
<span class="normal">4036</span>
<span class="normal">4037</span>
<span class="normal">4038</span>
<span class="normal">4039</span>
<span class="normal">4040</span>
<span class="normal">4041</span>
<span class="normal">4042</span>
<span class="normal">4043</span>
<span class="normal">4044</span>
<span class="normal">4045</span>
<span class="normal">4046</span>
<span class="normal">4047</span>
<span class="normal">4048</span>
<span class="normal">4049</span>
<span class="normal">4050</span>
<span class="normal">4051</span>
<span class="normal">4052</span>
<span class="normal">4053</span>
<span class="normal">4054</span>
<span class="normal">4055</span>
<span class="normal">4056</span>
<span class="normal">4057</span>
<span class="normal">4058</span>
<span class="normal">4059</span>
<span class="normal">4060</span>
<span class="normal">4061</span>
<span class="normal">4062</span>
<span class="normal">4063</span>
<span class="normal">4064</span>
<span class="normal">4065</span>
<span class="normal">4066</span>
<span class="normal">4067</span>
<span class="normal">4068</span>
<span class="normal">4069</span>
<span class="normal">4070</span>
<span class="normal">4071</span>
<span class="normal">4072</span>
<span class="normal">4073</span>
<span class="normal">4074</span>
<span class="normal">4075</span>
<span class="normal">4076</span>
<span class="normal">4077</span>
<span class="normal">4078</span>
<span class="normal">4079</span>
<span class="normal">4080</span>
<span class="normal">4081</span>
<span class="normal">4082</span>
<span class="normal">4083</span>
<span class="normal">4084</span>
<span class="normal">4085</span>
<span class="normal">4086</span>
<span class="normal">4087</span>
<span class="normal">4088</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LuminaAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the LuminaNextDiT model. It applies a s normalization layer and rotary embedding on query and key vector.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">query_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">base_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Get Query-Key-Value Pair</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">query_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">query_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span>

        <span class="c1"># Get key-value heads</span>
        <span class="n">kv_heads</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">head_dim</span>

        <span class="c1"># Apply Query-Key Norm if needed</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">query_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">query_rotary_emb</span><span class="p">,</span> <span class="n">use_real</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">key_rotary_emb</span><span class="p">,</span> <span class="n">use_real</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">key</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">softmax_scale</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Apply proportional attention if true</span>
        <span class="k">if</span> <span class="n">key_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">base_sequence_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">base_sequence_length</span><span class="p">))</span> <span class="o">*</span> <span class="n">attn</span><span class="o">.</span><span class="n">scale</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scale</span>

        <span class="c1"># perform Grouped-qurey Attention (GQA)</span>
        <span class="n">n_rep</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">//</span> <span class="n">kv_heads</span>
        <span class="k">if</span> <span class="n">n_rep</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
        <span class="c1"># (batch, heads, source_length, target_length)</span>
        <span class="n">target_length</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">target_length</span><span class="p">))</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">softmax_scale</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.MochiAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.MochiAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.MochiAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used in Mochi.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MochiAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attention processor used in Mochi.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">freqs_cos</span><span class="p">,</span> <span class="n">freqs_sin</span><span class="p">):</span>
        <span class="n">x_even</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">x_odd</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">cos</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_even</span> <span class="o">*</span> <span class="n">freqs_cos</span> <span class="o">-</span> <span class="n">x_odd</span> <span class="o">*</span> <span class="n">freqs_sin</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">sin</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_even</span> <span class="o">*</span> <span class="n">freqs_sin</span> <span class="o">+</span> <span class="n">x_odd</span> <span class="o">*</span> <span class="n">freqs_cos</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mint</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="s2">&quot;MochiAttention&quot;</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="n">encoder_query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">encoder_value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">encoder_query</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">encoder_query</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">encoder_key</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">encoder_key</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">encoder_value</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">encoder_value</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_key</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="o">*</span><span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="o">*</span><span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">encoder_query</span><span class="p">,</span> <span class="n">encoder_key</span><span class="p">,</span> <span class="n">encoder_value</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">encoder_query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">encoder_key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">encoder_value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">encoder_sequence_length</span> <span class="o">=</span> <span class="n">encoder_query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">total_length</span> <span class="o">=</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="n">encoder_sequence_length</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">attn_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">valid_prompt_token_indices</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">as_tuple</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="n">valid_encoder_query</span> <span class="o">=</span> <span class="n">encoder_query</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="n">valid_prompt_token_indices</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">valid_encoder_key</span> <span class="o">=</span> <span class="n">encoder_key</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="n">valid_prompt_token_indices</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">valid_encoder_value</span> <span class="o">=</span> <span class="n">encoder_value</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="n">valid_prompt_token_indices</span><span class="p">,</span> <span class="p">:]</span>

            <span class="n">valid_query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">valid_encoder_query</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">valid_key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">key</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">valid_encoder_key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">valid_value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">value</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">valid_encoder_value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
                <span class="n">valid_query</span><span class="p">,</span> <span class="n">valid_key</span><span class="p">,</span> <span class="n">valid_value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">valid_sequence_length</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">total_length</span> <span class="o">-</span> <span class="n">valid_sequence_length</span><span class="p">))</span>
            <span class="n">attn_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attn_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># hidden_states, encoder_hidden_states = hidden_states.split_with_sizes(</span>
        <span class="c1">#     (sequence_length, encoder_sequence_length), dim=1</span>
        <span class="c1"># )</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span><span class="n">sequence_length</span><span class="p">,</span> <span class="p">:],</span>
            <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">sequence_length</span><span class="p">:,</span> <span class="p">:],</span>
        <span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="s2">&quot;to_add_out&quot;</span><span class="p">):</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.MochiVaeAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.MochiVaeAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.MochiVaeAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Attention processor used in Mochi VAE.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2800</span>
<span class="normal">2801</span>
<span class="normal">2802</span>
<span class="normal">2803</span>
<span class="normal">2804</span>
<span class="normal">2805</span>
<span class="normal">2806</span>
<span class="normal">2807</span>
<span class="normal">2808</span>
<span class="normal">2809</span>
<span class="normal">2810</span>
<span class="normal">2811</span>
<span class="normal">2812</span>
<span class="normal">2813</span>
<span class="normal">2814</span>
<span class="normal">2815</span>
<span class="normal">2816</span>
<span class="normal">2817</span>
<span class="normal">2818</span>
<span class="normal">2819</span>
<span class="normal">2820</span>
<span class="normal">2821</span>
<span class="normal">2822</span>
<span class="normal">2823</span>
<span class="normal">2824</span>
<span class="normal">2825</span>
<span class="normal">2826</span>
<span class="normal">2827</span>
<span class="normal">2828</span>
<span class="normal">2829</span>
<span class="normal">2830</span>
<span class="normal">2831</span>
<span class="normal">2832</span>
<span class="normal">2833</span>
<span class="normal">2834</span>
<span class="normal">2835</span>
<span class="normal">2836</span>
<span class="normal">2837</span>
<span class="normal">2838</span>
<span class="normal">2839</span>
<span class="normal">2840</span>
<span class="normal">2841</span>
<span class="normal">2842</span>
<span class="normal">2843</span>
<span class="normal">2844</span>
<span class="normal">2845</span>
<span class="normal">2846</span>
<span class="normal">2847</span>
<span class="normal">2848</span>
<span class="normal">2849</span>
<span class="normal">2850</span>
<span class="normal">2851</span>
<span class="normal">2852</span>
<span class="normal">2853</span>
<span class="normal">2854</span>
<span class="normal">2855</span>
<span class="normal">2856</span>
<span class="normal">2857</span>
<span class="normal">2858</span>
<span class="normal">2859</span>
<span class="normal">2860</span>
<span class="normal">2861</span>
<span class="normal">2862</span>
<span class="normal">2863</span>
<span class="normal">2864</span>
<span class="normal">2865</span>
<span class="normal">2866</span>
<span class="normal">2867</span>
<span class="normal">2868</span>
<span class="normal">2869</span>
<span class="normal">2870</span>
<span class="normal">2871</span>
<span class="normal">2872</span>
<span class="normal">2873</span>
<span class="normal">2874</span>
<span class="normal">2875</span>
<span class="normal">2876</span>
<span class="normal">2877</span>
<span class="normal">2878</span>
<span class="normal">2879</span>
<span class="normal">2880</span>
<span class="normal">2881</span>
<span class="normal">2882</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MochiVaeAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention processor used in Mochi VAE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">is_single_frame</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">is_single_frame</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
            <span class="k">return</span> <span class="n">hidden_states</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">is_causal</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.SanaLinearAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.SanaLinearAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.SanaLinearAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product linear attention.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4456</span>
<span class="normal">4457</span>
<span class="normal">4458</span>
<span class="normal">4459</span>
<span class="normal">4460</span>
<span class="normal">4461</span>
<span class="normal">4462</span>
<span class="normal">4463</span>
<span class="normal">4464</span>
<span class="normal">4465</span>
<span class="normal">4466</span>
<span class="normal">4467</span>
<span class="normal">4468</span>
<span class="normal">4469</span>
<span class="normal">4470</span>
<span class="normal">4471</span>
<span class="normal">4472</span>
<span class="normal">4473</span>
<span class="normal">4474</span>
<span class="normal">4475</span>
<span class="normal">4476</span>
<span class="normal">4477</span>
<span class="normal">4478</span>
<span class="normal">4479</span>
<span class="normal">4480</span>
<span class="normal">4481</span>
<span class="normal">4482</span>
<span class="normal">4483</span>
<span class="normal">4484</span>
<span class="normal">4485</span>
<span class="normal">4486</span>
<span class="normal">4487</span>
<span class="normal">4488</span>
<span class="normal">4489</span>
<span class="normal">4490</span>
<span class="normal">4491</span>
<span class="normal">4492</span>
<span class="normal">4493</span>
<span class="normal">4494</span>
<span class="normal">4495</span>
<span class="normal">4496</span>
<span class="normal">4497</span>
<span class="normal">4498</span>
<span class="normal">4499</span>
<span class="normal">4500</span>
<span class="normal">4501</span>
<span class="normal">4502</span>
<span class="normal">4503</span>
<span class="normal">4504</span>
<span class="normal">4505</span>
<span class="normal">4506</span>
<span class="normal">4507</span>
<span class="normal">4508</span>
<span class="normal">4509</span>
<span class="normal">4510</span>
<span class="normal">4511</span>
<span class="normal">4512</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SanaLinearAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product linear attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># query = query.transpose(1, 2).unflatten(1, (attn.heads, -1))</span>
        <span class="c1"># key = key.transpose(1, 2).unflatten(1, (attn.heads, -1)).transpose(2, 3)</span>
        <span class="c1"># value = value.transpose(1, 2).unflatten(1, (attn.heads, -1))</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">key</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">value</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">original_dtype</span> <span class="o">==</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="mi">65504</span><span class="p">,</span> <span class="mi">65504</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.SanaMultiscaleAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.SanaMultiscaleAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.SanaMultiscaleAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing multiscale quadratic attention.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4383</span>
<span class="normal">4384</span>
<span class="normal">4385</span>
<span class="normal">4386</span>
<span class="normal">4387</span>
<span class="normal">4388</span>
<span class="normal">4389</span>
<span class="normal">4390</span>
<span class="normal">4391</span>
<span class="normal">4392</span>
<span class="normal">4393</span>
<span class="normal">4394</span>
<span class="normal">4395</span>
<span class="normal">4396</span>
<span class="normal">4397</span>
<span class="normal">4398</span>
<span class="normal">4399</span>
<span class="normal">4400</span>
<span class="normal">4401</span>
<span class="normal">4402</span>
<span class="normal">4403</span>
<span class="normal">4404</span>
<span class="normal">4405</span>
<span class="normal">4406</span>
<span class="normal">4407</span>
<span class="normal">4408</span>
<span class="normal">4409</span>
<span class="normal">4410</span>
<span class="normal">4411</span>
<span class="normal">4412</span>
<span class="normal">4413</span>
<span class="normal">4414</span>
<span class="normal">4415</span>
<span class="normal">4416</span>
<span class="normal">4417</span>
<span class="normal">4418</span>
<span class="normal">4419</span>
<span class="normal">4420</span>
<span class="normal">4421</span>
<span class="normal">4422</span>
<span class="normal">4423</span>
<span class="normal">4424</span>
<span class="normal">4425</span>
<span class="normal">4426</span>
<span class="normal">4427</span>
<span class="normal">4428</span>
<span class="normal">4429</span>
<span class="normal">4430</span>
<span class="normal">4431</span>
<span class="normal">4432</span>
<span class="normal">4433</span>
<span class="normal">4434</span>
<span class="normal">4435</span>
<span class="normal">4436</span>
<span class="normal">4437</span>
<span class="normal">4438</span>
<span class="normal">4439</span>
<span class="normal">4440</span>
<span class="normal">4441</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SanaMultiscaleAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing multiscale quadratic attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn</span><span class="p">:</span> <span class="n">SanaMultiscaleLinearAttention</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span> <span class="o">&gt;</span> <span class="n">attn</span><span class="o">.</span><span class="n">attention_head_dim</span><span class="p">:</span>
            <span class="n">use_linear_attention</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_linear_attention</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">multi_scale_qkv</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_states</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_qkv_multiscale</span><span class="p">:</span>
            <span class="n">multi_scale_qkv</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">multi_scale_qkv</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_linear_attention</span><span class="p">:</span>
            <span class="c1"># for linear attention upcast hidden_states to float32</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">attn</span><span class="o">.</span><span class="n">attention_head_dim</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">nonlinearity</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_linear_attention</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">apply_linear_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">original_dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">apply_quadratic_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;rms_norm&quot;</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">movedim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_out</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGCFGSanaLinearAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGCFGSanaLinearAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGCFGSanaLinearAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product linear attention.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4515</span>
<span class="normal">4516</span>
<span class="normal">4517</span>
<span class="normal">4518</span>
<span class="normal">4519</span>
<span class="normal">4520</span>
<span class="normal">4521</span>
<span class="normal">4522</span>
<span class="normal">4523</span>
<span class="normal">4524</span>
<span class="normal">4525</span>
<span class="normal">4526</span>
<span class="normal">4527</span>
<span class="normal">4528</span>
<span class="normal">4529</span>
<span class="normal">4530</span>
<span class="normal">4531</span>
<span class="normal">4532</span>
<span class="normal">4533</span>
<span class="normal">4534</span>
<span class="normal">4535</span>
<span class="normal">4536</span>
<span class="normal">4537</span>
<span class="normal">4538</span>
<span class="normal">4539</span>
<span class="normal">4540</span>
<span class="normal">4541</span>
<span class="normal">4542</span>
<span class="normal">4543</span>
<span class="normal">4544</span>
<span class="normal">4545</span>
<span class="normal">4546</span>
<span class="normal">4547</span>
<span class="normal">4548</span>
<span class="normal">4549</span>
<span class="normal">4550</span>
<span class="normal">4551</span>
<span class="normal">4552</span>
<span class="normal">4553</span>
<span class="normal">4554</span>
<span class="normal">4555</span>
<span class="normal">4556</span>
<span class="normal">4557</span>
<span class="normal">4558</span>
<span class="normal">4559</span>
<span class="normal">4560</span>
<span class="normal">4561</span>
<span class="normal">4562</span>
<span class="normal">4563</span>
<span class="normal">4564</span>
<span class="normal">4565</span>
<span class="normal">4566</span>
<span class="normal">4567</span>
<span class="normal">4568</span>
<span class="normal">4569</span>
<span class="normal">4570</span>
<span class="normal">4571</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGCFGSanaLinearAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product linear attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_uncond</span><span class="p">,</span> <span class="n">hidden_states_org</span><span class="p">])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">key</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">value</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="c1"># perturbed path (identity attention)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">original_dtype</span> <span class="o">==</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="mi">65504</span><span class="p">,</span> <span class="mi">65504</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.PAGIdentitySanaLinearAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.PAGIdentitySanaLinearAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.PAGIdentitySanaLinearAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product linear attention.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">4574</span>
<span class="normal">4575</span>
<span class="normal">4576</span>
<span class="normal">4577</span>
<span class="normal">4578</span>
<span class="normal">4579</span>
<span class="normal">4580</span>
<span class="normal">4581</span>
<span class="normal">4582</span>
<span class="normal">4583</span>
<span class="normal">4584</span>
<span class="normal">4585</span>
<span class="normal">4586</span>
<span class="normal">4587</span>
<span class="normal">4588</span>
<span class="normal">4589</span>
<span class="normal">4590</span>
<span class="normal">4591</span>
<span class="normal">4592</span>
<span class="normal">4593</span>
<span class="normal">4594</span>
<span class="normal">4595</span>
<span class="normal">4596</span>
<span class="normal">4597</span>
<span class="normal">4598</span>
<span class="normal">4599</span>
<span class="normal">4600</span>
<span class="normal">4601</span>
<span class="normal">4602</span>
<span class="normal">4603</span>
<span class="normal">4604</span>
<span class="normal">4605</span>
<span class="normal">4606</span>
<span class="normal">4607</span>
<span class="normal">4608</span>
<span class="normal">4609</span>
<span class="normal">4610</span>
<span class="normal">4611</span>
<span class="normal">4612</span>
<span class="normal">4613</span>
<span class="normal">4614</span>
<span class="normal">4615</span>
<span class="normal">4616</span>
<span class="normal">4617</span>
<span class="normal">4618</span>
<span class="normal">4619</span>
<span class="normal">4620</span>
<span class="normal">4621</span>
<span class="normal">4622</span>
<span class="normal">4623</span>
<span class="normal">4624</span>
<span class="normal">4625</span>
<span class="normal">4626</span>
<span class="normal">4627</span>
<span class="normal">4628</span>
<span class="normal">4629</span>
<span class="normal">4630</span>
<span class="normal">4631</span>
<span class="normal">4632</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PAGIdentitySanaLinearAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product linear attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">key</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">value</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]:</span>
            <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">hidden_states_org</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="mf">1e-15</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">hidden_states_org</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>
        <span class="n">hidden_states_org</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_org</span><span class="p">)</span>

        <span class="c1"># perturbed path (identity attention)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states_ptb</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>
        <span class="n">hidden_states_ptb</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states_ptb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">hidden_states_org</span><span class="p">,</span> <span class="n">hidden_states_ptb</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">original_dtype</span> <span class="o">==</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="mi">65504</span><span class="p">,</span> <span class="mi">65504</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.StableAudioAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.StableAudioAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.StableAudioAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
used in the Stable Audio model. It applies rotary embedding on query and key vector, and allows MHA, GQA or MQA.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2985</span>
<span class="normal">2986</span>
<span class="normal">2987</span>
<span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span>
<span class="normal">3054</span>
<span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StableAudioAttnProcessor2_0</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing scaled dot-product attention (enabled by default if you&#39;re using PyTorch 2.0). This is</span>
<span class="sd">    used in the Stable Audio model. It applies rotary embedding on query and key vector, and allows MHA, GQA or MQA.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># move importing from __call__ to __init__ as it is not supported in construct()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_partial_rotary_emb</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">rot_dim</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">x_to_rotate</span><span class="p">,</span> <span class="n">x_unrotated</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">rot_dim</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">rot_dim</span><span class="p">:]</span>

        <span class="n">x_rotated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">x_to_rotate</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">,</span> <span class="n">use_real</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_real_unbind_dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x_rotated</span><span class="p">,</span> <span class="n">x_unrotated</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="c1"># scaled_dot_product_attention expects attention_mask shape to be</span>
            <span class="c1"># (batch, heads, source_length, target_length)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">kv_heads</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">head_dim</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">kv_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">kv_heads</span> <span class="o">!=</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">:</span>
            <span class="c1"># if GQA or MQA, repeat the key/value heads to reach the number of query heads.</span>
            <span class="n">heads_per_kv_head</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">//</span> <span class="n">kv_heads</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">heads_per_kv_head</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">heads_per_kv_head</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
                <span class="n">value</span><span class="p">,</span> <span class="n">heads_per_kv_head</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">heads_per_kv_head</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply RoPE if needed</span>
        <span class="k">if</span> <span class="n">rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query_dtype</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">key_dtype</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

            <span class="n">rot_dim</span> <span class="o">=</span> <span class="n">rotary_emb</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">query_to_rotate</span><span class="p">,</span> <span class="n">query_unrotated</span> <span class="o">=</span> <span class="n">query</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">rot_dim</span><span class="p">],</span> <span class="n">query</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">rot_dim</span><span class="p">:]</span>
            <span class="n">query_rotated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query_to_rotate</span><span class="p">,</span> <span class="n">rotary_emb</span><span class="p">,</span> <span class="n">use_real</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_real_unbind_dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">query_rotated</span><span class="p">,</span> <span class="n">query_unrotated</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">attn</span><span class="o">.</span><span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">key_to_rotate</span><span class="p">,</span> <span class="n">key_unrotated</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">rot_dim</span><span class="p">],</span> <span class="n">key</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">rot_dim</span><span class="p">:]</span>
                <span class="n">key_rotated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key_to_rotate</span><span class="p">,</span> <span class="n">rotary_emb</span><span class="p">,</span> <span class="n">use_real</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_real_unbind_dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

                <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">key_rotated</span><span class="p">,</span> <span class="n">key_unrotated</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_dtype</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">key_dtype</span><span class="p">)</span>

        <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
        <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="c1"># attention_probs = attn.get_attention_scores(query, key, attention_mask)</span>
        <span class="c1"># hidden_states = mint.bmm(attention_probs, value)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.XFormersAttnProcessor" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.XFormersAttnProcessor</code>


<a href="#mindone.diffusers.models.attention_processor.XFormersAttnProcessor" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">


        <p>Processor for implementing memory efficient attention using xFormers-like interface.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>attention_op</code>
            </td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The base
<a href="https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase">operator</a> to
use as the attention operator. It is recommended to set to <code>None</code>, and allow xFormers to choose the best
operator.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>`Callable`, *optional*, defaults to `None`</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span>
<span class="normal">2629</span>
<span class="normal">2630</span>
<span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span>
<span class="normal">2695</span>
<span class="normal">2696</span>
<span class="normal">2697</span>
<span class="normal">2698</span>
<span class="normal">2699</span>
<span class="normal">2700</span>
<span class="normal">2701</span>
<span class="normal">2702</span>
<span class="normal">2703</span>
<span class="normal">2704</span>
<span class="normal">2705</span>
<span class="normal">2706</span>
<span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@ms</span><span class="o">.</span><span class="n">jit_class</span>
<span class="k">class</span><span class="w"> </span><span class="nc">XFormersAttnProcessor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Processor for implementing memory efficient attention using xFormers-like interface.</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_op (`Callable`, *optional*, defaults to `None`):</span>
<span class="sd">            The base</span>
<span class="sd">            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to</span>
<span class="sd">            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best</span>
<span class="sd">            operator.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_op</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">attention_op</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;Memory efficient attention on mindspore uses flash attention under the hoods. &quot;</span>
            <span class="s2">&quot;There is no other implementation for now. Please do not set `attention_op`.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_op</span> <span class="o">=</span> <span class="n">attention_op</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">spatial_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">temb</span><span class="p">)</span>

        <span class="n">input_ndim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">ndim</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">key_tokens</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">)</span>

        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">prepare_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_tokens</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># expand our mask&#39;s singleton query_tokens dimension:</span>
            <span class="c1">#   [batch*heads,            1, key_tokens] -&gt;</span>
            <span class="c1">#   [batch*heads, query_tokens, key_tokens]</span>
            <span class="c1"># so that it can be added as a bias onto the attention scores that xformers computes:</span>
            <span class="c1">#   [batch*heads, query_tokens, key_tokens]</span>
            <span class="c1"># we do this explicitly because xformers doesn&#39;t broadcast the singleton dimension for us.</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">query_tokens</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">query_tokens</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="k">elif</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_cross</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_encoder_hidden_states</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_to_batch_dim</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Memory efficient attention on mindspore uses flash attention under the hoods.</span>
        <span class="c1"># Flash attention implementation is called `FlashAttentionScore`</span>
        <span class="c1"># which is an experimental api with the following limitations:</span>
        <span class="c1"># 1. Sequence length of query must be divisible by 16 and in range of [1, 32768].</span>
        <span class="c1"># 2. Head dimensions must be one of [64, 80, 96, 120, 128, 256].</span>
        <span class="c1"># 3. The input dtype must be float16 or bfloat16.</span>
        <span class="c1"># Sequence length of query must be checked in runtime.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">query_tokens</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">query_tokens</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Sequence length of query must be divisible by 16, but got </span><span class="si">{</span><span class="n">query_tokens</span><span class="si">=}</span><span class="s2">.&quot;</span>
        <span class="c1"># Head dimension is checked in Attention.set_use_memory_efficient_attention_xformers. We maybe pad on head_dim.</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">query_padded</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim_padding</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
            <span class="n">key_padded</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim_padding</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
            <span class="n">value_padded</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim_padding</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">query_padded</span><span class="p">,</span> <span class="n">key_padded</span><span class="p">,</span> <span class="n">value_padded</span> <span class="o">=</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span>
        <span class="n">flash_attn</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">operations</span><span class="o">.</span><span class="n">nn_ops</span><span class="o">.</span><span class="n">FlashAttentionScore</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="n">attn</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">hidden_states_padded</span> <span class="o">=</span> <span class="n">flash_attn</span><span class="p">(</span><span class="n">query_padded</span><span class="p">,</span> <span class="n">key_padded</span><span class="p">,</span> <span class="n">value_padded</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span>
        <span class="c1"># If we did padding before calculate attention, undo it!</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim_padding</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states_padded</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">attn</span><span class="o">.</span><span class="n">head_dim</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states_padded</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">batch_to_head_dim</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># linear proj</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="c1"># dropout</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>

        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="mindone.diffusers.models.attention_processor.FluxIPAdapterJointAttnProcessor2_0" class="doc doc-heading">
            <code>mindone.diffusers.models.attention_processor.FluxIPAdapterJointAttnProcessor2_0</code>


<a href="#mindone.diffusers.models.attention_processor.FluxIPAdapterJointAttnProcessor2_0" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="mindspore.nn.Cell">Cell</span></code></p>


        <p>Flux Attention processor for IP-Adapter.</p>







              <details class="quote">
                <summary>Source code in <code>mindone/diffusers/models/attention_processor.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span>
<span class="normal">2340</span>
<span class="normal">2341</span>
<span class="normal">2342</span>
<span class="normal">2343</span>
<span class="normal">2344</span>
<span class="normal">2345</span>
<span class="normal">2346</span>
<span class="normal">2347</span>
<span class="normal">2348</span>
<span class="normal">2349</span>
<span class="normal">2350</span>
<span class="normal">2351</span>
<span class="normal">2352</span>
<span class="normal">2353</span>
<span class="normal">2354</span>
<span class="normal">2355</span>
<span class="normal">2356</span>
<span class="normal">2357</span>
<span class="normal">2358</span>
<span class="normal">2359</span>
<span class="normal">2360</span>
<span class="normal">2361</span>
<span class="normal">2362</span>
<span class="normal">2363</span>
<span class="normal">2364</span>
<span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span>
<span class="normal">2414</span>
<span class="normal">2415</span>
<span class="normal">2416</span>
<span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">FluxIPAdapterJointAttnProcessor2_0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flux Attention processor for IP-Adapter.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">cross_attention_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_tokens</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_dim</span> <span class="o">=</span> <span class="n">cross_attention_dim</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="n">num_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">num_tokens</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`scale` should be a list of integers with the same length as `num_tokens`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">mint</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cross_attention_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">))]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attn</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">image_rotary_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ip_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ip_adapter_masks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span> <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># `sample` projections.</span>
        <span class="n">hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">hidden_states_query_proj</span> <span class="o">=</span> <span class="n">hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_q</span><span class="p">(</span><span class="n">hidden_states_query_proj</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_k</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># the attention in FluxSingleTransformerBlock does not use `encoder_hidden_states`</span>
        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># `context` projections.</span>
            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_q_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_k_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">add_v_proj</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_query_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_key_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">encoder_hidden_states_value_proj</span> <span class="o">=</span> <span class="n">encoder_hidden_states_value_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_query_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_q</span><span class="p">(</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_hidden_states_key_proj</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">norm_added_k</span><span class="p">(</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">)</span>

            <span class="c1"># attention</span>
            <span class="n">query</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_query_proj</span><span class="p">,</span> <span class="n">hidden_states_query_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_key_proj</span><span class="p">,</span> <span class="n">key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">encoder_hidden_states_value_proj</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">image_rotary_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">image_rotary_emb</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                <span class="n">hidden_states</span><span class="p">[:,</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">:],</span>
            <span class="p">)</span>

            <span class="c1"># linear proj</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="c1"># dropout</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_add_out</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>

            <span class="c1"># IP-adapter</span>
            <span class="n">ip_query</span> <span class="o">=</span> <span class="n">hidden_states_query_proj</span>
            <span class="n">ip_attn_output</span> <span class="o">=</span> <span class="n">mint</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">current_ip_hidden_states</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">to_k_ip</span><span class="p">,</span> <span class="n">to_v_ip</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="n">ip_hidden_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k_ip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v_ip</span>
            <span class="p">):</span>
                <span class="n">ip_key</span> <span class="o">=</span> <span class="n">to_k_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>
                <span class="n">ip_value</span> <span class="o">=</span> <span class="n">to_v_ip</span><span class="p">(</span><span class="n">current_ip_hidden_states</span><span class="p">)</span>

                <span class="n">ip_key</span> <span class="o">=</span> <span class="n">ip_key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                <span class="n">ip_value</span> <span class="o">=</span> <span class="n">ip_value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
                <span class="c1"># the output of sdp = (batch, num_heads, seq_len, head_dim)</span>
                <span class="c1"># TODO: add support for attn.scale when we move to Torch 2.1</span>
                <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
                    <span class="n">ip_query</span><span class="p">,</span> <span class="n">ip_key</span><span class="p">,</span> <span class="n">ip_value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
                <span class="p">)</span>
                <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">current_ip_hidden_states</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span>
                <span class="p">)</span>
                <span class="n">current_ip_hidden_states</span> <span class="o">=</span> <span class="n">current_ip_hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ip_query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">ip_attn_output</span> <span class="o">+=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">current_ip_hidden_states</span>

            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">ip_attn_output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="July 1, 2025 08:00:24 UTC">July 1, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 1, 2024 02:56:38 UTC">November 1, 2024</span>
  </span>

    
    
      
  
  <span class="md-source-file__fact">
    <span class="md-icon" title="Contributors">
      
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 5.5A3.5 3.5 0 0 1 15.5 9a3.5 3.5 0 0 1-3.5 3.5A3.5 3.5 0 0 1 8.5 9 3.5 3.5 0 0 1 12 5.5M5 8c.56 0 1.08.15 1.53.42-.15 1.43.27 2.85 1.13 3.96C7.16 13.34 6.16 14 5 14a3 3 0 0 1-3-3 3 3 0 0 1 3-3m14 0a3 3 0 0 1 3 3 3 3 0 0 1-3 3c-1.16 0-2.16-.66-2.66-1.62a5.54 5.54 0 0 0 1.13-3.96c.45-.27.97-.42 1.53-.42M5.5 18.25c0-2.07 2.91-3.75 6.5-3.75s6.5 1.68 6.5 3.75V20h-13zM0 20v-1.5c0-1.39 1.89-2.56 4.45-2.9-.59.68-.95 1.62-.95 2.65V20zm24 0h-3.5v-1.75c0-1.03-.36-1.97-.95-2.65 2.56.34 4.45 1.51 4.45 2.9z"/></svg>
      
    </span>
    <nav>
      
        <a href="mailto:77485245+wcrzlh@users.noreply.github.com">Chaoran Wei</a>, 
        <a href="mailto:73014084+cui-yshoho@users.noreply.github.com">Cui-yshoho</a>
    </nav>
  </span>

    
    
  </aside>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../internal_classes_overview/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Overview">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Overview
              </div>
            </div>
          </a>
        
        
          
          <a href="../activations/" class="md-footer__link md-footer__link--next" aria-label="Next: Custom activation functions">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Custom activation functions
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 MindSpore Lab
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindone" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>