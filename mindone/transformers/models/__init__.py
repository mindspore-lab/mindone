# Copyright 2020 The HuggingFace Team. All rights reserved.
#
# This code is adapted from https://github.com/huggingface/transformers
# with modifications to run transformers on mindspore.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import transformers
from packaging import version

from . import (
    albert,
    aria,
    auto,
    bamba,
    bark,
    bart,
    bert,
    bert_generation,
    big_bird,
    bigbird_pegasus,
    bit,
    blip,
    blip_2,
    bloom,
    camembert,
    canine,
    chameleon,
    clap,
    clip,
    clipseg,
    clvp,
    codegen,
    colpali,
    convbert,
    convnext,
    convnextv2,
    ctrl,
    dac,
    data2vec,
    dbrx,
    deit,
    depth_anything,
    dinov2,
    distilbert,
    dpr,
    dpt,
    electra,
    emu3,
    encodec,
    ernie,
    falcon,
    fastspeech2_conformer,
    flaubert,
    fsmt,
    funnel,
    fuyu,
    gemma,
    gemma2,
    gemma3,
    git,
    glm,
    glpn,
    got_ocr2,
    gpt2,
    gpt_neo,
    granite,
    granitemoe,
    granitemoeshared,
    hiera,
    hubert,
    idefics,
    idefics2,
    idefics3,
    ijepa,
    imagegpt,
    instructblip,
    instructblipvideo,
    jetmoe,
    kosmos2,
    levit,
    lilt,
    llama,
    llava,
    llava_next,
    llava_next_video,
    llava_onevision,
    luke,
    m2m_100,
    mamba,
    mamba2,
    megatron_bert,
    mgp_str,
    mimi,
    minicpm4,
    mistral,
    mistral3,
    mixtral,
    mllama,
    mobilebert,
    modernbert,
    moonshine,
    moshi,
    mpnet,
    mpt,
    mvp,
    nystromformer,
    olmo,
    olmo2,
    opt,
    owlv2,
    owlvit,
    paligemma,
    pegasus,
    pegasus_x,
    persimmon,
    phi,
    phi3,
    pix2struct,
    pixtral,
    poolformer,
    pop2piano,
    prophetnet,
    qwen2,
    qwen2_5_vl,
    qwen2_audio,
    qwen2_vl,
    resnet,
    roberta,
    rwkv,
    sam,
    seamless_m4t_v2,
    segformer,
    sew,
    sew_d,
    siglip,
    siglip2,
    smolvlm,
    speech_encoder_decoder,
    speech_to_text,
    speecht5,
<<<<<<< HEAD
    splinter,
    squeezebert,
=======
>>>>>>> 0c3c7751 (Add stablelm model)
    stablelm,
    starcoder2,
    swin2sr,
    switch_transformers,
    t5,
    tapas,
    trocr,
    tvp,
    umt5,
    unispeech,
    unispeech_sat,
    univnet,
    video_llava,
    vilt,
    vipllava,
    vision_encoder_decoder,
    visual_bert,
    vit,
    vit_msn,
    vitdet,
    vitmatte,
    vitpose,
    vitpose_backbone,
    vits,
    wav2vec2,
    x_clip,
    xlm_roberta,
    yolos,
    zamba,
    zamba2,
    zoedepth,
)

if version.parse(transformers.__version__) >= version.parse("4.51.0"):
    from . import qwen3

if version.parse(transformers.__version__) >= version.parse("4.51.3"):
    from . import glm4

if version.parse(transformers.__version__) >= version.parse("4.53.0"):
    from . import glm4v, minimax, qwen2_5_omni, vjepa2
